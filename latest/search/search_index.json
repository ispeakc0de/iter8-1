{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Iter8 \u00b6","title":"Home"},{"location":"#iter8","text":"","title":"Iter8"},{"location":"news/","text":"News and Announcements \u00b6 Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News"},{"location":"news/#news-and-announcements","text":"Iter8 at KubeCon + CloudNativeCon Europe, May 6, 2021 Iter8 at Knative meetup, Mar 24, 2021 Kubeflow blog article by Animesh Singh and Dan Sun: Operationalize, Scale and Infuse Trust in AI Models using KFServing , Mar 8, 2021 Medium blog article by Michael Kalantar: Automated Canary Release of Microservices on Kubernetes using Tekton and iter8 , Oct 26, 2020 Medium blog article by Kusuma Chalasani: Better Performance with kruize and iter8 for your microservices application , Oct 12, 2020 Medium blog article by Srinivasan Parthasarathy: Automated Canary Release of TensorFlow Models on Kubernetes , Oct 5, 2020 Medium blog article by Sushma Ravichandran: Iter8: Take a look at the magic under the hood , Oct 1, 2020 Medium blog article by Fabio Oliveira: Iter8: Achieving Agility with Control , Aug 17 th , 2020","title":"News and Announcements"},{"location":"roadmap/","text":"Roadmap \u00b6 Enhanced experiments A/B/n, and Pareto testing strategies with single and multiple reward metrics Early termination of experiments Analytics extensibility for traffic shifting and version assessment algorithms Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Enhancing Kubernetes and OpenShift integration Support for OpenShift runtimes Enhanced Knative metrics in tutorials using OpenTelemetry collector Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Enhanced experiments A/B/n, and Pareto testing strategies with single and multiple reward metrics Early termination of experiments Analytics extensibility for traffic shifting and version assessment algorithms Experiments with support and confidence Metrics Support for more metric providers like MySQL, PostgreSQL, CouchDB, MongoDB, and Google Analytics. Enhanced MLOps experiments Customized experiments/metrics for serving frameworks like TorchServe and TFServing GitOps Integration with ArgoCD, Flux and other GitOps operators Enhancing Kubernetes and OpenShift integration Support for OpenShift runtimes Enhanced Knative metrics in tutorials using OpenTelemetry collector Git triggered workflows and CI/CD Integration with GitHub Actions and other pipeline providers Improved installation Iter8 Helm chart, Iter8 Operator","title":"Roadmap"},{"location":"concepts/buildingblocks/","text":"Building Blocks \u00b6 We introduce the building blocks of an Iter8 experiment below. Applications and Versions \u00b6 Iter8 defines an application broadly as an entity that can be: instantiated (run) on Kubernetes, can be versioned, and for which metrics can be collected. Examples A stateless K8s application whose versions correspond to deployments . A stateful K8s application whose versions correspond to statefulsets . A Knative application whose versions correspond to revisions . A KFServing inference service, whose versions correspond to model revisions . A distributed application whose versions correspond to Helm releases . Objectives (SLOs) \u00b6 Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. Examples The 99 th -percentile tail latency of the application should be under 50 msec. The precision of the ML model version should be over 92%. The (average) number of GPU cores consumed by a model should be under 5.0 Reward \u00b6 Reward typically corresponds to a business metric which you wish to optimize during an A/B testing experiment. In Iter8 experiments, reward is specified as a metric along with a preferred direction, which could be high or low . Examples User-engagement Conversion rate Click-through rate Revenue Precision, recall, or accuracy (for ML models) Number of GPU cores consumed by an ML model All but the last example above have a preferred direction high ; the last example is that of a reward with preferred direction low . Baseline and candidate versions \u00b6 Every Iter8 experiment involves a baseline version and may also involve zero, one or more candidate versions. Experiments often involve two versions, baseline and a candidate, with the baseline version corresponding to the stable version of your app, and the candidate version corresponds to a canary. Testing strategy \u00b6 Testing strategy determines how the winning version (winner) in an experiment is identified. SLO validation \u00b6 SLO validation experiments may involve a single version or two versions. SLO validation experiment with baseline version and no candidate (conformance testing): If baseline satisfies the objectives , it is the winner. Otherwise, there is no winner. SLO validation experiment with baseline and candidate versions: If candidate satisfies the objectives , it is the winner. Else, if baseline satisfies the objectives , it is the winner. Else, there is no winner. A/B testing \u00b6 A/B testing experiments involve a baseline version, a candidate version, and a reward metric. The version which performs best in terms of the reward metric is the winner. A/B/n testing \u00b6 A/B/n testing experiments involve a baseline version, two or more candidate versions, and a reward metric. The version which performs best in terms of the reward metric is the winner. Hybrid (A/B + SLOs) testing \u00b6 Hybrid (A/B + SLOs) testing experiments combine A/B or A/B/n testing on the one hand with SLO validation on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. If no version satisfies objectives, then there is no winner. Rollout strategy \u00b6 Rollout strategy defines how traffic is split between versions during the experiment. Iter8 makes it easy for you to take total advantage of all the traffic engineering features available in your K8s environment (i.e., supported by the ingress or service mesh technology available in your K8s cluster). A few common deployment strategies used in Iter8 experiments are described below. In the following description, v1 and v2 refer to the current and new versions of the application respectively. Simple rollout & rollout \u00b6 This pattern is modeled after the rolling update of a Kubernetes deployment . After v2 is deployed, it replaces v1 . If v2 is the winner of the experiment, it is retained. Else, v2 is rolled back and v1 is retained. All traffic flows to v2 during the experiment. BlueGreen \u00b6 After v2 is deployed, both v1 and v2 are available. All traffic is routed to v2 . If v2 is the winner of the experiment, all traffic continues to flow to v2 . Else, all traffic is routed back to v1 . Dark launch \u00b6 After v2 is deployed, it is hidden from end-users. v2 is not used to serve end-user requests but can still be experimented with. Built-in load/metrics \u00b6 During the experiment, Iter8 generates load for v2 and/or collects built-in metrics. Traffic mirroring (shadowing) \u00b6 Mirrored traffic is a replica of the real user requests 1 that is routed to v2 , and used to collect metrics for v2 . Canary \u00b6 Canary deployment involves exposing v2 to a small fraction of end-user requests during the experiment before exposing it to a larger fraction of requests or all the requests. Fixed-%-split \u00b6 A fixed % of end-user requests is sent to v2 and the rest is sent to v1 . Fixed-%-split with user segmentation \u00b6 Only a specific segment of the users participate in the experiment. A fixed % of requests from the participating segment is sent to v2 . Rest is sent to v1 . All requests from end-users in the non-participating segment is sent to v1 . Progressive traffic shift \u00b6 Traffic is incrementally shifted to the winner over multiple iterations. Progressive traffic shift with user segmentation \u00b6 Only a specific segment of the users participate in the experiment. Within this segment, traffic is incrementally shifted to the winner over multiple iterations. All requests from end-users in the non-participating segment is sent to v1 . Session affinity \u00b6 Session affinity, sometimes referred to as sticky sessions, routes all requests coming from an end-user to the same version consistently throughout the experiment. User grouping for affinity can be configured based on a number of different attributes of the request including request headers, cookies, query parameters, geo location, user agent (browser version, screen size, operating system) and language. Version promotion \u00b6 Iter8 can automatically promote the winning version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Experiment building blocks"},{"location":"concepts/buildingblocks/#building-blocks","text":"We introduce the building blocks of an Iter8 experiment below.","title":"Building Blocks"},{"location":"concepts/buildingblocks/#applications-and-versions","text":"Iter8 defines an application broadly as an entity that can be: instantiated (run) on Kubernetes, can be versioned, and for which metrics can be collected. Examples A stateless K8s application whose versions correspond to deployments . A stateful K8s application whose versions correspond to statefulsets . A Knative application whose versions correspond to revisions . A KFServing inference service, whose versions correspond to model revisions . A distributed application whose versions correspond to Helm releases .","title":"Applications and Versions"},{"location":"concepts/buildingblocks/#objectives-slos","text":"Objectives correspond to service-level objectives or SLOs. In Iter8 experiments, objectives are specified as metrics along with acceptable limits on their values. Iter8 will report how versions are performing with respect to these metrics and whether or not they satisfy the objectives. Examples The 99 th -percentile tail latency of the application should be under 50 msec. The precision of the ML model version should be over 92%. The (average) number of GPU cores consumed by a model should be under 5.0","title":"Objectives (SLOs)"},{"location":"concepts/buildingblocks/#reward","text":"Reward typically corresponds to a business metric which you wish to optimize during an A/B testing experiment. In Iter8 experiments, reward is specified as a metric along with a preferred direction, which could be high or low . Examples User-engagement Conversion rate Click-through rate Revenue Precision, recall, or accuracy (for ML models) Number of GPU cores consumed by an ML model All but the last example above have a preferred direction high ; the last example is that of a reward with preferred direction low .","title":"Reward"},{"location":"concepts/buildingblocks/#baseline-and-candidate-versions","text":"Every Iter8 experiment involves a baseline version and may also involve zero, one or more candidate versions. Experiments often involve two versions, baseline and a candidate, with the baseline version corresponding to the stable version of your app, and the candidate version corresponds to a canary.","title":"Baseline and candidate versions"},{"location":"concepts/buildingblocks/#testing-strategy","text":"Testing strategy determines how the winning version (winner) in an experiment is identified.","title":"Testing strategy"},{"location":"concepts/buildingblocks/#slo-validation","text":"SLO validation experiments may involve a single version or two versions. SLO validation experiment with baseline version and no candidate (conformance testing): If baseline satisfies the objectives , it is the winner. Otherwise, there is no winner. SLO validation experiment with baseline and candidate versions: If candidate satisfies the objectives , it is the winner. Else, if baseline satisfies the objectives , it is the winner. Else, there is no winner.","title":"SLO validation"},{"location":"concepts/buildingblocks/#ab-testing","text":"A/B testing experiments involve a baseline version, a candidate version, and a reward metric. The version which performs best in terms of the reward metric is the winner.","title":"A/B testing"},{"location":"concepts/buildingblocks/#abn-testing","text":"A/B/n testing experiments involve a baseline version, two or more candidate versions, and a reward metric. The version which performs best in terms of the reward metric is the winner.","title":"A/B/n testing"},{"location":"concepts/buildingblocks/#hybrid-ab-slos-testing","text":"Hybrid (A/B + SLOs) testing experiments combine A/B or A/B/n testing on the one hand with SLO validation on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. If no version satisfies objectives, then there is no winner.","title":"Hybrid (A/B + SLOs) testing"},{"location":"concepts/buildingblocks/#rollout-strategy","text":"Rollout strategy defines how traffic is split between versions during the experiment. Iter8 makes it easy for you to take total advantage of all the traffic engineering features available in your K8s environment (i.e., supported by the ingress or service mesh technology available in your K8s cluster). A few common deployment strategies used in Iter8 experiments are described below. In the following description, v1 and v2 refer to the current and new versions of the application respectively.","title":"Rollout strategy"},{"location":"concepts/buildingblocks/#simple-rollout-rollout","text":"This pattern is modeled after the rolling update of a Kubernetes deployment . After v2 is deployed, it replaces v1 . If v2 is the winner of the experiment, it is retained. Else, v2 is rolled back and v1 is retained. All traffic flows to v2 during the experiment.","title":"Simple rollout &amp; rollout"},{"location":"concepts/buildingblocks/#bluegreen","text":"After v2 is deployed, both v1 and v2 are available. All traffic is routed to v2 . If v2 is the winner of the experiment, all traffic continues to flow to v2 . Else, all traffic is routed back to v1 .","title":"BlueGreen"},{"location":"concepts/buildingblocks/#dark-launch","text":"After v2 is deployed, it is hidden from end-users. v2 is not used to serve end-user requests but can still be experimented with.","title":"Dark launch"},{"location":"concepts/buildingblocks/#built-in-loadmetrics","text":"During the experiment, Iter8 generates load for v2 and/or collects built-in metrics.","title":"Built-in load/metrics"},{"location":"concepts/buildingblocks/#traffic-mirroring-shadowing","text":"Mirrored traffic is a replica of the real user requests 1 that is routed to v2 , and used to collect metrics for v2 .","title":"Traffic mirroring (shadowing)"},{"location":"concepts/buildingblocks/#canary","text":"Canary deployment involves exposing v2 to a small fraction of end-user requests during the experiment before exposing it to a larger fraction of requests or all the requests.","title":"Canary"},{"location":"concepts/buildingblocks/#fixed-split","text":"A fixed % of end-user requests is sent to v2 and the rest is sent to v1 .","title":"Fixed-%-split"},{"location":"concepts/buildingblocks/#fixed-split-with-user-segmentation","text":"Only a specific segment of the users participate in the experiment. A fixed % of requests from the participating segment is sent to v2 . Rest is sent to v1 . All requests from end-users in the non-participating segment is sent to v1 .","title":"Fixed-%-split with user segmentation"},{"location":"concepts/buildingblocks/#progressive-traffic-shift","text":"Traffic is incrementally shifted to the winner over multiple iterations.","title":"Progressive traffic shift"},{"location":"concepts/buildingblocks/#progressive-traffic-shift-with-user-segmentation","text":"Only a specific segment of the users participate in the experiment. Within this segment, traffic is incrementally shifted to the winner over multiple iterations. All requests from end-users in the non-participating segment is sent to v1 .","title":"Progressive traffic shift with user segmentation"},{"location":"concepts/buildingblocks/#session-affinity","text":"Session affinity, sometimes referred to as sticky sessions, routes all requests coming from an end-user to the same version consistently throughout the experiment. User grouping for affinity can be configured based on a number of different attributes of the request including request headers, cookies, query parameters, geo location, user agent (browser version, screen size, operating system) and language.","title":"Session affinity"},{"location":"concepts/buildingblocks/#version-promotion","text":"Iter8 can automatically promote the winning version at the end of an experiment. It is possible to mirror only a certain percentage of the requests instead of all requests. \u21a9","title":"Version promotion"},{"location":"concepts/whatisiter8/","text":"What is Iter8? \u00b6 Iter8 is the release engineering platform for Kubernetes applications and ML models. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience. Use Iter8 for SLO validation, A/B testing and progressive rollouts of K8s apps/ML models. What is an Iter8 experiment? \u00b6 Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B(/n) testing and progressive rollouts as shown below. What is Helmex? \u00b6 Helmex, short for Helm-based experiments, is an Iter8 experimentation pattern that uses Helm. In Helmex, the Helm values file used for managing the application releases satisfy the Helmex schema . How does Iter8 work? \u00b6 Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-iter8","text":"Iter8 is the release engineering platform for Kubernetes applications and ML models. Iter8 is designed for DevOps and MLOps teams interested in maximizing release velocity and business value with their apps/ML models while protecting end-user experience. Use Iter8 for SLO validation, A/B testing and progressive rollouts of K8s apps/ML models.","title":"What is Iter8?"},{"location":"concepts/whatisiter8/#what-is-an-iter8-experiment","text":"Iter8 defines a Kubernetes resource called Experiment that automates SLO validation, A/B(/n) testing and progressive rollouts as shown below.","title":"What is an Iter8 experiment?"},{"location":"concepts/whatisiter8/#what-is-helmex","text":"Helmex, short for Helm-based experiments, is an Iter8 experimentation pattern that uses Helm. In Helmex, the Helm values file used for managing the application releases satisfy the Helmex schema .","title":"What is Helmex?"},{"location":"concepts/whatisiter8/#how-does-iter8-work","text":"Iter8 consists of a Go-based Kubernetes controller that orchestrates (reconciles) experiments in conjunction with a Python-based analytics service , and a Go-based task runner .","title":"How does Iter8 work?"},{"location":"contributing/analytics/","text":"Extending Iter8's Analytics Functions \u00b6 Iter8's analytics functions are implemented in the iter8-analytics repo . Python virtual environment \u00b6 Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate Running iter8-analytics locally \u00b6 Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs. Running unit tests for iter8-analytics locally \u00b6 1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Analytics"},{"location":"contributing/analytics/#extending-iter8s-analytics-functions","text":"Iter8's analytics functions are implemented in the iter8-analytics repo .","title":"Extending Iter8's Analytics Functions"},{"location":"contributing/analytics/#python-virtual-environment","text":"Use a Python 3+ virtual environment to locally develop iter8-analytics . You can create and activate a virtual environment as follows. git clone git@github.com:iter8-tools/iter8-analytics.git cd iter8-analytics python3 -m venv .venv source .venv/bin/activate","title":"Python virtual environment"},{"location":"contributing/analytics/#running-iter8-analytics-locally","text":"Create and activate a Python 3+ virtual environment as described above. The following instructions have been verified in a Python 3.9 virtual environment. Run them from the root folder of your iter8-analytics local repo. 1. pip install -r requirements.txt 2. pip install -e . 3. cd iter8_analytics 4. python fastapi_app.py Navigate to http://localhost:8080/docs on your browser. You can interact with the iter8-analytics service and read its API documentation here. The iter8-analytics APIs are intended to work with metric databases, and use Kubernetes secrets for obtaining the required authentication information for querying the metric DBs.","title":"Running iter8-analytics locally"},{"location":"contributing/analytics/#running-unit-tests-for-iter8-analytics-locally","text":"1. pip install -r requirements.txt 2. pip install -r test-requirements.txt 3. pip install -e . 4. coverage run --source=iter8_analytics --omit=\"*/__init__.py\" -m pytest You can see the coverage report by opening htmlcov/index.html in your browser.","title":"Running unit tests for iter8-analytics locally"},{"location":"contributing/newk8sstack/","text":"Add a K8s Stack / Service Mesh / Ingress \u00b6 Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules. Step 1: Fork Iter8 \u00b6 Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo. Step 2: Edit kustomization.yaml \u00b6 cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon - iter8-linkerd # -iter8-<your stack> # add your stack here Step 3: Create subfolder \u00b6 mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml Step 4: Create RBAC rules \u00b6 cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples. Step 5: Update RBAC rules \u00b6 Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation. Step 6: Submit PR \u00b6 Sign your commit and submit your pull request to the Iter8 repo.","title":"New K8s stack"},{"location":"contributing/newk8sstack/#add-a-k8s-stack-service-mesh-ingress","text":"Performing Iter8 experiments requires RBAC rules, which are contained in this Kustomize folder and are installed as part of the Iter8 installation. Enable Iter8 experiments over a new K8s stack by extending these RBAC rules.","title":"Add a K8s Stack / Service Mesh / Ingress"},{"location":"contributing/newk8sstack/#step-1-fork-iter8","text":"Fork the Iter8 GitHub repo . Locally clone your forked repo. For the rest of this document, $ITER8 will refer to the root of your local Iter8 repo.","title":"Step 1: Fork Iter8"},{"location":"contributing/newk8sstack/#step-2-edit-kustomizationyaml","text":"cd $ITER8 /install/core/rbac/stacks Edit kustomization.yaml to add your K8s stack. At the time of writing, it contains the following stacks: resources : - iter8-knative - iter8-istio - iter8-kfserving - iter8-seldon - iter8-linkerd # -iter8-<your stack> # add your stack here","title":"Step 2: Edit kustomization.yaml"},{"location":"contributing/newk8sstack/#step-3-create-subfolder","text":"mkdir iter8-<your stack> cp iter8-kfserving/kustomization.yaml iter8-<your stack>/kustomization.yaml","title":"Step 3: Create subfolder"},{"location":"contributing/newk8sstack/#step-4-create-rbac-rules","text":"cd iter8-<your stack> Foo & Istio virtual service example Suppose Iter8 experiments on your stack involves manipulation of two types of resources: The foo resource belonging to the API group bar.my.org . The Istio virtual service resource. Note: Foo and bar are merely placeholders. It can be replaced by any standard K8s resource type like deployment or service , or a custom resource type, as required. Create RBAC rules that will enable Iter8 to manipulate foo resources and Istio virtual service resources during experiments. You can do so by creating roles.yaml and rolebindings.yaml files as follows. roles.yaml # This cluster role enables manipulation of foo resources apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : foo-for-<your stack> rules : - apiGroups : - bar.my.org resources : - foo verbs : - get - list - patch - update - create - delete - watch --- # This cluster role enables manipulation of Istio virtual services apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : vs-for-<your stack> rules : - apiGroups : - networking.istio.io resources : - virtualservices verbs : - get - list - patch - update - create - delete - watch rolebindings.yaml # This cluster role binding enables Iter8 controller and task runner to manipulate # foo resources in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : foo-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : foo-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers --- # This role binding enables Iter8 controller and handler to manipulate # Istio virtual services in any namespace apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : vs-for-<your stack> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : vs-for-<your stack> subjects : - kind : ServiceAccount name : controller - kind : ServiceAccount name : handlers You can also refer to the Istio , KFServing , Knative , and Seldon examples.","title":"Step 4: Create RBAC rules"},{"location":"contributing/newk8sstack/#step-5-update-rbac-rules","text":"Update the RBAC rules that are applied to the Kubernetes cluster as part of the Iter8 installation.","title":"Step 5: Update RBAC rules"},{"location":"contributing/newk8sstack/#step-6-submit-pr","text":"Sign your commit and submit your pull request to the Iter8 repo.","title":"Step 6: Submit PR"},{"location":"contributing/overview/","text":"Overview \u00b6 Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know! Ways to Contribute \u00b6 We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together. Come to Iter8 Community Meetings! \u00b6 Everyone is welcome to join the Iter8 community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Iter8 community meetings are on the 1 st and 3 rd Wednesdays of each month from 11 AM - 12 PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you would like to present a demo or discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel . Find an Issue \u00b6 Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine. Ask for Help \u00b6 The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings Pull Request Lifecycle \u00b6 Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools . Sign Your Commits \u00b6 Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Overview"},{"location":"contributing/overview/#overview","text":"Welcome! We are delighted that you want to contribute to Iter8! \ud83d\udc96 As you get started, you are in the best position to give us feedback on areas of our project that we need help with including: Problems found during setup of Iter8 Gaps in our quick start guide or other tutorials and documentation Bugs in our test and automation scripts If anything doesn't make sense, or doesn't work when you run it, please open a bug report and let us know!","title":"Overview"},{"location":"contributing/overview/#ways-to-contribute","text":"We welcome many different types of contributions including: Iter8 documentation/tutorials New features New K8s stack/service mesh/ingress Iter8 code samples for OpenShift Analytics Tasks Builds, CI Bug fixes Web design for https://iter8.tools Communications/social media/blog posts Reviewing pull requests Not everything happens through a GitHub pull request. Please come to our community meetings or contact us and let's discuss how we can work together.","title":"Ways to Contribute"},{"location":"contributing/overview/#come-to-iter8-community-meetings","text":"Everyone is welcome to join the Iter8 community meetings. You never need an invite to attend. In fact, we want you to join us, even if you don\u2019t have anything you feel like you want to contribute. Just being there is enough! Iter8 community meetings are on the 1 st and 3 rd Wednesdays of each month from 11 AM - 12 PM EST/EDT. You can find the video conference link here . You can find the agenda here as well as the meeting notes here . If you would like to present a demo or discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel .","title":"Come to Iter8 Community Meetings!"},{"location":"contributing/overview/#find-an-issue","text":"Iter8 issues are managed centrally here . We have good first issues for new contributors and help wanted issues suitable for any contributor. Issued labeled good first issue have extra information to help you make your first contribution. Issues labeled help wanted are issues suitable for someone who isn't a core maintainer and is good to move onto after your first pull request. Sometimes there won\u2019t be any issues with these labels. That\u2019s ok! There is likely still something for you to work on. If you want to contribute but you don\u2019t know where to start or can't find a suitable issue, you can reach out to us over the Iter8 Slack workspace for help finding something to work on. Once you see an issue that you'd like to work on, please post a comment saying that you want to work on it. Something like \"I want to work on this\" is fine.","title":"Find an Issue"},{"location":"contributing/overview/#ask-for-help","text":"The best ways to reach us with a question when contributing is to ask on: The original GitHub issue #development channel in the Iter8 Slack workspace Bring your questions to our community meetings","title":"Ask for Help"},{"location":"contributing/overview/#pull-request-lifecycle","text":"Your PR is associated with one (and infrequently, with more than one) GitHub issue . You can start the submission of your PR as soon as this issue has been created. Follow the standard GitHub fork and pull request process when creating and submitting your PR. The associated GitHub issue might need to go through design discussions and may not be ready for development. Your PR might require new tests; these new or existing tests may not yet be running successfully. At this stage, keep your PR as a draft , to signal that it is not yet ready for review. Once design discussions are complete and tests pass, convert the draft PR into a regular PR to signal that it is ready for review. Additionally, post a message in the #development Slack channel of the Iter8 Slack workspace with a link to your PR. This will expedite the review. You can expect an initial review within 1-2 days of submitting a PR, and follow up reviews (if any) to happen over 2-5 days. Use the #development Slack channel of Iter8 Slack workspace to ping/bump when the pull request is ready for further review or if it appears stalled. Iter8 releases happen frequently. Once your PR is merged, you can expect your contribution to show up live in a short amount of time at https://iter8.tools .","title":"Pull Request Lifecycle"},{"location":"contributing/overview/#sign-your-commits","text":"Licensing is important to open source projects. It provides some assurances that the software will continue to be available based under the terms that the author(s) desired. We require that contributors sign off on commits submitted to our project's repositories. The Developer Certificate of Origin (DCO) is a way to certify that you wrote and have the right to contribute the code you are submitting to the project. Read GitHub's documentation on signing your commits . You sign-off by adding the following to your commit messages. Your sign-off must match the git user and email associated with the commit. This is my commit message Signed-off-by: Your Name <your.name@example.com> Git has a -s command line option to do this automatically: git commit -s -m 'This is my commit message' If you forgot to do this and have not yet pushed your changes to the remote repository, you can amend your commit with the sign-off by running git commit --amend -s","title":"Sign Your Commits"},{"location":"contributing/tasks/","text":"Writing Iter8 Tasks \u00b6 Iter8 tasks are implemented in the handler Go repo . Clone this repo. Go version \u00b6 Ensure you have Go version 1.16+ . Running handler locally \u00b6 # cd <root of the locally cloned handler repo> go build ./handler Testing handler with coverage \u00b6 make test # uncomment the line below to show coverage percentage # make coverage # uncomment the line below to view coverage in a browser # make show-coverage # uncomment the line below to sort functions in descending order of coverage # go tool cover -func coverage.out | sort -nr -k 3 Implementing a new task \u00b6 This section is coming soon.","title":"Tasks"},{"location":"contributing/tasks/#writing-iter8-tasks","text":"Iter8 tasks are implemented in the handler Go repo . Clone this repo.","title":"Writing Iter8 Tasks"},{"location":"contributing/tasks/#go-version","text":"Ensure you have Go version 1.16+ .","title":"Go version"},{"location":"contributing/tasks/#running-handler-locally","text":"# cd <root of the locally cloned handler repo> go build ./handler","title":"Running handler locally"},{"location":"contributing/tasks/#testing-handler-with-coverage","text":"make test # uncomment the line below to show coverage percentage # make coverage # uncomment the line below to view coverage in a browser # make show-coverage # uncomment the line below to sort functions in descending order of coverage # go tool cover -func coverage.out | sort -nr -k 3","title":"Testing handler with coverage"},{"location":"contributing/tasks/#implementing-a-new-task","text":"This section is coming soon.","title":"Implementing a new task"},{"location":"contributing/tutorials/","text":"Contribute Tutorials \u00b6 MkDocs \u00b6 Iter8 documentation uses Mkdocs . The section on linking to pages and images is especially useful for Iter8 tutorial authors. Test your tutorial \u00b6 All Iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests. Locally serve Iter8 docs \u00b6 Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs. Locally view live changes to Iter8 docs \u00b6 The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Tutorials"},{"location":"contributing/tutorials/#contribute-tutorials","text":"","title":"Contribute Tutorials"},{"location":"contributing/tutorials/#mkdocs","text":"Iter8 documentation uses Mkdocs . The section on linking to pages and images is especially useful for Iter8 tutorial authors.","title":"MkDocs"},{"location":"contributing/tutorials/#test-your-tutorial","text":"All Iter8 tutorials include e2e tests, either as part of GitHub Actions workflows or as a standalone test script like this one if they require more resources than what is available in GitHub Actions workflows. When contributing a tutorial, please include relevant e2e tests.","title":"Test your tutorial"},{"location":"contributing/tutorials/#locally-serve-iter8-docs","text":"Pre-requisite: Python 3+. Use a Python 3 virtual environment to locally serve Iter8 docs. Run the following commands from the top-level directory of the Iter8 repo. cd mkdocs python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt mkdocs serve -s Browse http://localhost:8000 to view your local Iter8 docs.","title":"Locally serve Iter8 docs"},{"location":"contributing/tutorials/#locally-view-live-changes-to-iter8-docs","text":"The overall structure of the documentation, as reflected in the nav tabs of https://iter8.tools , is located in the iter8/mkdocs/mkdocs.yml file. The markdown files for Iter8 docs are located under the iter8/mkdocs/docs folder. You will see live updates to http://localhost:8000 as you update the above files.","title":"Locally view live changes to Iter8 docs"},{"location":"getting-started/first-experiment/","text":"Your First Experiment \u00b6 Scenario: Validate service-level objectives (SLOs) for a K8s app Problem : You have a K8s app. You want to verify that it satisfies latency and error rate SLOs. Solution : In this tutorial, you will launch a K8s app along with an Iter8 experiment. Iter8 will validate that the new satisfies latency and error-based objectives (SLOs) using built-in metrics . Setup K8s cluster and local environment Setup K8s cluster Install Iter8 in K8s cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd ) 1. Create app \u00b6 The hello world app consists of a K8s deployment and service. Deploy the app as follows. kubectl apply -n default -f $ITER8 /samples/deployments/app/deploy.yaml kubectl apply -n default -f $ITER8 /samples/deployments/app/service.yaml Verify app \u00b6 Verify that the app is running using these instructions # do this in a separate terminal kubectl port-forward -n default svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv 2. Create Iter8 experiment \u00b6 Deploy an Iter8 experiment for SLO validation of the app as follows. helm upgrade -n default my-exp $ITER8 /samples/first-exp \\ --set URL = 'http://hello.default.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --install The above command creates an Iter8 experiment that generates requests, collects latency and error rate metrics for the app, and verifies that the app satisfies mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View Iter8 experiment View the Iter8 experiment as follows. helm get manifest -n default my-exp 3. Observe experiment \u00b6 Assert that the experiment completed and found a winning version. Wait 20 seconds before trying the following command. If the assertions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Describe the results of the Iter8 experiment. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+ 4. Cleanup \u00b6 # remove experiment helm uninstall -n default my-exp # remove app kubectl delete -n default -f $ITER8 /samples/deployments/app/service.yaml kubectl delete -n default -f $ITER8 /samples/deployments/app/deploy.yaml Next Steps Use with your app Run the above experiment with your app by setting the URL value in the Helm command to the URL of your app. You can also customize the mean latency, error rate, and tail latency limits. This experiment can be run in any K8s environment such as a dev, test, staging, or production cluster.","title":"Your first experiment"},{"location":"getting-started/first-experiment/#your-first-experiment","text":"Scenario: Validate service-level objectives (SLOs) for a K8s app Problem : You have a K8s app. You want to verify that it satisfies latency and error rate SLOs. Solution : In this tutorial, you will launch a K8s app along with an Iter8 experiment. Iter8 will validate that the new satisfies latency and error-based objectives (SLOs) using built-in metrics . Setup K8s cluster and local environment Setup K8s cluster Install Iter8 in K8s cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd )","title":"Your First Experiment"},{"location":"getting-started/first-experiment/#1-create-app","text":"The hello world app consists of a K8s deployment and service. Deploy the app as follows. kubectl apply -n default -f $ITER8 /samples/deployments/app/deploy.yaml kubectl apply -n default -f $ITER8 /samples/deployments/app/service.yaml","title":"1. Create app"},{"location":"getting-started/first-experiment/#verify-app","text":"Verify that the app is running using these instructions # do this in a separate terminal kubectl port-forward -n default svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"Verify app"},{"location":"getting-started/first-experiment/#2-create-iter8-experiment","text":"Deploy an Iter8 experiment for SLO validation of the app as follows. helm upgrade -n default my-exp $ITER8 /samples/first-exp \\ --set URL = 'http://hello.default.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --install The above command creates an Iter8 experiment that generates requests, collects latency and error rate metrics for the app, and verifies that the app satisfies mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View Iter8 experiment View the Iter8 experiment as follows. helm get manifest -n default my-exp","title":"2. Create Iter8 experiment"},{"location":"getting-started/first-experiment/#3-observe-experiment","text":"Assert that the experiment completed and found a winning version. Wait 20 seconds before trying the following command. If the assertions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Describe the results of the Iter8 experiment. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+","title":"3. Observe experiment"},{"location":"getting-started/first-experiment/#4-cleanup","text":"# remove experiment helm uninstall -n default my-exp # remove app kubectl delete -n default -f $ITER8 /samples/deployments/app/service.yaml kubectl delete -n default -f $ITER8 /samples/deployments/app/deploy.yaml Next Steps Use with your app Run the above experiment with your app by setting the URL value in the Helm command to the URL of your app. You can also customize the mean latency, error rate, and tail latency limits. This experiment can be run in any K8s environment such as a dev, test, staging, or production cluster.","title":"4. Cleanup"},{"location":"getting-started/help/","text":"Get Help \u00b6 Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our community meetings! Everyone is welcome to join our community meetings. Our community meetings are on the 1 st and 3 rd Wednesdays of each month from 11 AM to 12 PM EST/EDT. Video conference link for community meetings Agenda Meeting notes If you would like to present a demo or discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel .","title":"Get help"},{"location":"getting-started/help/#get-help","text":"Read Iter8 docs . Join the Iter8 Slack workspace . File an issue or start a discussion on the Iter8 GitHub repo . Attend our community meetings! Everyone is welcome to join our community meetings. Our community meetings are on the 1 st and 3 rd Wednesdays of each month from 11 AM to 12 PM EST/EDT. Video conference link for community meetings Agenda Meeting notes If you would like to present a demo or discuss a topic, please list it on the agenda along with a time estimate. Community meetings are recorded and publicly available on our YouTube channel .","title":"Get Help"},{"location":"getting-started/install/","text":"Install Iter8 \u00b6 Install Iter8 in your Kubernetes cluster as follows. This step requires Kustomize v3+ . export TAG = master kustomize build \"https://github.com/iter8-tools/iter8/install/core/?ref= ${ TAG } \" | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build \"https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref= ${ TAG } \" | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system To pin Iter8 to a specific version during install, export the appropriate Iter8 tag. For example, to install version v0.7.21 of Iter8, use export TAG=v0.7.21 instead of master . Get iter8ctl \u00b6 Get iter8ctl CLI on your local machine as follows. This step requires Go 1.16+ . GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.6","title":"Install Iter8"},{"location":"getting-started/install/#install-iter8","text":"Install Iter8 in your Kubernetes cluster as follows. This step requires Kustomize v3+ . export TAG = master kustomize build \"https://github.com/iter8-tools/iter8/install/core/?ref= ${ TAG } \" | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build \"https://github.com/iter8-tools/iter8/install/builtin-metrics/?ref= ${ TAG } \" | kubectl apply -f - kubectl wait --for = condition = Ready pods --all -n iter8-system To pin Iter8 to a specific version during install, export the appropriate Iter8 tag. For example, to install version v0.7.21 of Iter8, use export TAG=v0.7.21 instead of master .","title":"Install Iter8"},{"location":"getting-started/install/#get-iter8ctl","text":"Get iter8ctl CLI on your local machine as follows. This step requires Go 1.16+ . GO111MODULE = on GOBIN = /usr/local/bin go get github.com/iter8-tools/iter8ctl@v0.1.6","title":"Get iter8ctl"},{"location":"getting-started/setup-for-tutorials/","text":"Setup for Tutorials \u00b6 Setup common components that are required in Iter8 tutorials. Local Kubernetes cluster \u00b6 Use a managed K8s cluster or a local K8s cluster for running Iter8 tutorials. You can setup the latter using Kind or Minikube as follows. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Minikube minikube start Setting CPU and memory resources for your local K8s cluster Iter8 tutorials in certain K8s environments, especially those involving Istio, require additional CPU and memory resources. You can set this as follows. Kind A Kind cluster inherits its CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as follows. Minikube Set CPU and memory resources while starting Minikube as follows. minikube start --cpus 8 --memory 12288 Iter8 GitHub repo \u00b6 Clone the Iter8 GitHub repo and set the ITER8 environment variable as follows: git clone https://github.com/iter8-tools/iter8.git export ITER8 = ./iter8 Iter8 Helm repo \u00b6 Iter8 Helm repo contains charts that support Iter8's Helmex tutorials. Get the repo as follows: helm repo add iter8 https://iter8-tools.github.io/iter8/ --force-update","title":"Setup for tutorials"},{"location":"getting-started/setup-for-tutorials/#setup-for-tutorials","text":"Setup common components that are required in Iter8 tutorials.","title":"Setup for Tutorials"},{"location":"getting-started/setup-for-tutorials/#local-kubernetes-cluster","text":"Use a managed K8s cluster or a local K8s cluster for running Iter8 tutorials. You can setup the latter using Kind or Minikube as follows. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Minikube minikube start Setting CPU and memory resources for your local K8s cluster Iter8 tutorials in certain K8s environments, especially those involving Istio, require additional CPU and memory resources. You can set this as follows. Kind A Kind cluster inherits its CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as follows. Minikube Set CPU and memory resources while starting Minikube as follows. minikube start --cpus 8 --memory 12288","title":"Local Kubernetes cluster"},{"location":"getting-started/setup-for-tutorials/#iter8-github-repo","text":"Clone the Iter8 GitHub repo and set the ITER8 environment variable as follows: git clone https://github.com/iter8-tools/iter8.git export ITER8 = ./iter8","title":"Iter8 GitHub repo"},{"location":"getting-started/setup-for-tutorials/#iter8-helm-repo","text":"Iter8 Helm repo contains charts that support Iter8's Helmex tutorials. Get the repo as follows: helm repo add iter8 https://iter8-tools.github.io/iter8/ --force-update","title":"Iter8 Helm repo"},{"location":"metrics/builtin/","text":"Built-in Metrics \u00b6 Built-in latency/error metrics Iter8 ships with a set of nine built-in metrics that measure your application's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly. As part of metrics collection, Iter8 will also generate HTTP requests to the application end-point. List of built-in metrics \u00b6 The following are the set of built-in Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency Collecting built-in metrics \u00b6 Use the metrics/collect task in an experiment to collect built-in metrics for your app/ML model versions. Example \u00b6 For an example of an experiment that uses built-in metrics, look inside the Knative experiment in this tutorial .","title":"Builtin metrics"},{"location":"metrics/builtin/#built-in-metrics","text":"Built-in latency/error metrics Iter8 ships with a set of nine built-in metrics that measure your application's performance in terms of latency and errors. You can collect and use these metrics in experiments without the need to configure any external databases. This feature enables you to get started with Iter8 experiments, especially, SLO validation experiments, quickly. As part of metrics collection, Iter8 will also generate HTTP requests to the application end-point.","title":"Built-in Metrics"},{"location":"metrics/builtin/#list-of-built-in-metrics","text":"The following are the set of built-in Iter8 metrics. Namespace Name Type Description iter8-system request-count Counter Number of requests iter8-system error-count Gauge Number of responses with HTTP status code 4xx or 5xx iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx iter8-system mean-latency Gauge Mean response latency iter8-system latency-50 th -percentile Gauge 50 th percentile (median) response latency iter8-system latency-75 th -percentile Gauge 75 th percentile response latency iter8-system latency-90 th -percentile Gauge 90 th percentile response latency iter8-system latency-95 th -percentile Gauge 95 th percentile response latency iter8-system latency-99 th -percentile Gauge 99 th percentile response latency","title":"List of built-in metrics"},{"location":"metrics/builtin/#collecting-built-in-metrics","text":"Use the metrics/collect task in an experiment to collect built-in metrics for your app/ML model versions.","title":"Collecting built-in metrics"},{"location":"metrics/builtin/#example","text":"For an example of an experiment that uses built-in metrics, look inside the Knative experiment in this tutorial .","title":"Example"},{"location":"metrics/custom/","text":"Custom Metrics \u00b6 Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments. Metrics with/without auth \u00b6 Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service. Placeholder substitution \u00b6 Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well. JSON response \u00b6 Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } } Processing the JSON response \u00b6 Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression . Error handling \u00b6 Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Custom metrics"},{"location":"metrics/custom/#custom-metrics","text":"Custom Iter8 metrics enable you to use data from any database for evaluating app/ML model versions within Iter8 experiments. This document describes how you can define custom Iter8 metrics and (optionally) supply authentication information that may be required by the metrics provider. Metric providers differ in the following aspects. HTTP request authentication method: no authentication, basic auth, API keys, or bearer token HTTP request method: GET or POST Format of HTTP parameters and/or JSON body used while querying them Format of the JSON response returned by the provider The logic used by Iter8 to extract the metric value from the JSON response The examples in this document focus on Prometheus, NewRelic, Sysdig, and Elastic. However, the principles illustrated here will enable you to use metrics from any provider in experiments.","title":"Custom Metrics"},{"location":"metrics/custom/#metrics-withwithout-auth","text":"Note: Metrics are defined by you, the Iter8 end-user . Prometheus Prometheus does not support any authentication mechanism out-of-the-box . However, Prometheus can be setup in conjunction with a reverse proxy, which in turn can support HTTP request authentication, as described here . No Authentication The following is an example of an Iter8 metric with Prometheus as the provider. This example assumes that Prometheus can be queried by Iter8 without any authentication. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://myprometheusservice.com/api/v1 Basic auth Suppose Prometheus is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can enable Iter8 to query this Prometheus instance as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic promcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : description : A Prometheus example provider : prometheus params : - name : query value : >- sum(increase(revision_app_request_latencies_count{service_name='${name}',${userfilter}}[${elapsedTime}s])) or on() vector(0) type : Counter authType : Basic secret : myns/promcredentials jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : https://my.secure.prometheus.service.com/api/v1 Brief explanation of the request-count metric Prometheus enables metric queries using HTTP GET requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of request-count , and defaulted to GET . Iter8 will query Prometheus during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named query as required by Prometheus . The value of this parameter is derived by substituting the placeholders in the value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Prometheus. The urlTemplate field provides the URL of the prometheus service. New Relic New Relic uses API Keys to authenticate requests as documented here . The API key may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. API key embedded in metric The following is an example of an Iter8 metric with Prometheus as the provider. In this example, t0p-secret-api-key is the New Relic API key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter headerTemplates : - name : X-Query-Key value : t0p-secret-api-key jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id API key embedded in secret Suppose your New Relic API key is t0p-secret-api-key ; you wish to store this API key in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the API key. kubectl create secret generic nrcredentials -n myns --from-literal = mykey = t0p-secret-api-key The above secret contains a data field named mykey whose value is the API key. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics = myns Define metric: When defining the metric, ensure that the authType field is set to APIKey and the appropriate secret is referenced. In the headerTemplates field, include X-Query-Key as the name of a header field (as required by New Relic ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${mykey} at query time, by looking up the referenced secret named nrcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : name-count spec : description : A New Relic example provider : newrelic params : - name : nrql value : >- SELECT count(appName) FROM PageView WHERE revisionName='${revision}' SINCE ${elapsedTime} seconds ago type : Counter authType : APIKey secret : myns/nrcredentials headerTemplates : - name : X-Query-Key value : ${mykey} jqExpression : \".results[0].count | tonumber\" urlTemplate : https://insights-api.newrelic.com/v1/accounts/my_account_id Brief explanation of the name-count metric New Relic enables metric queries using both HTTP GET or POST requests. GET is the default value for the method field of an Iter8 metric. This field is optional; it is omitted in the definition of name-count , and defaulted to GET . Iter8 will query New Relic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a single query parameter named nrql as required by New Relic . The value of this parameter is derived by substituting the placeholders in its value string. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by New Relic. The urlTemplate field provides the URL of the New Relic service. Sysdig Sysdig data API accepts HTTP POST requests and uses a bearer token for authentication as documented here . The bearer token may be directly embedded within the Iter8 metric, or supplied as part of a Kubernetes secret. Bearer token embedded in metric The following is an example of an Iter8 metric with Sysdig as the provider. In this example, 87654321-1234-1234-1234-123456789012 is the Sysdig bearer token (also referred to as access key by Sysdig). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer 87654321-1234-1234-1234-123456789012 jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Bearer token embedded in secret Suppose your Sysdig token is 87654321-1234-1234-1234-123456789012 ; you wish to store this token in a Kubernetes secret, and reference this secret in an Iter8 metric. You can do so as follows. Create secret: Create a Kubernetes secret containing the token. kubectl create secret generic sdcredentials -n myns --from-literal = token = 87654321 -1234-1234-1234-123456789012 The above secret contains a data field named token whose value is the Sysdig token. The data field name (which can be any string of your choice) will be used in Step 3 below as a placeholder. Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Bearer and the appropriate secret is referenced. In the headerTemplates field, include Authorize header field (as required by Sysdig ). The value for this header field is a templated string. Iter8 will substitute the placeholder ${token} at query time, by looking up the referenced secret named sdcredentials in the myns namespace. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : cpu-utilization spec : description : A Sysdig example provider : sysdig body : >- { \"last\": ${elapsedTime}, \"sampling\": 600, \"filter\": \"kubernetes.app.revision.name = '${revision}'\", \"metrics\": [ { \"id\": \"cpu.cores.used\", \"aggregations\": { \"time\": \"avg\", \"group\": \"sum\" } } ], \"dataSourceType\": \"container\", \"paging\": { \"from\": 0, \"to\": 99 } } method : POST authType : Bearer secret : myns/sdcredentials type : Gauge headerTemplates : - name : Accept value : application/json - name : Authorization value : Bearer ${token} jqExpression : \".data[0].d[0] | tonumber\" urlTemplate : https://secure.sysdig.com/api/data Brief explanation of the cpu-utilization metric Sysdig enables metric queries using both POST requests; hence, the method field of the Iter8 metric is set to POST. Iter8 will query Sysdig during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Sysdig . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Sysdig. The urlTemplate field provides the URL of the Sysdig service. Elastic Elasticsearch REST API accepts HTTP GET or POST requests and uses basic authentication as documented here . Suppose Elasticsearch is set up to enforce basic auth with the following credentials: username : produser password : t0p-secret You can then enable Iter8 to query the Elasticsearch service as follows. Create secret: Create a Kubernetes secret that contains the authentication information. In particular, this secret needs to have the username and password fields in the data section with correct values. kubectl create secret generic elasticcredentials -n myns --from-literal = username = produser --from-literal = password = t0p-secret Create RBAC rule: Provide the required permissions for Iter8 to read this secret. The service account iter8-analytics in the iter8-system namespace will have permissions to read secrets in the myns namespace. kubectl create rolebinding iter8-cred --clusterrole = iter8-secret-reader-analytics --serviceaccount = iter8-system:iter8-analytics -n myns Define metric: When defining the metric, ensure that the authType field is set to Basic and the appropriate secret is referenced. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : average-sales spec : description : An elastic example provider : elastic body : >- { \"aggs\": { \"range\": { \"date_range\": { \"field\": \"date\", \"ranges\": [ { \"from\": \"now-${elapsedTime}s/s\" } ] } }, \"items_to_sell\": { \"filter\": { \"term\": { \"version\": \"${revision}\" } }, \"aggs\": { \"avg_sales\": { \"avg\": { \"field\": \"sale_price\" } } } } } } method : POST authType : Basic secret : myns/elasticcredentials type : Gauge headerTemplates : - name : Content-Type value : application/json jqExpression : \".aggregations.items_to_sell.avg_sales.value | tonumber\" urlTemplate : https://secure.elastic.com/my/sales Brief explanation of the average sales metric Elastic enables metric queries using GET or POST requests. In the elastic example, The method field of the Iter8 metric is set to POST. Iter8 will query Elastic during each iteration of the experiment. In each iteration, Iter8 will use n HTTP queries to fetch metric values for each version, where n is the number of versions in the experiment 2 . The HTTP query used by Iter8 contains a JSON body as required by Elastic . This JSON body is derived by substituting the placeholders in body template. The jqExpression enables Iter8 to extract the metric value from the JSON response returned by Elastic. The urlTemplate field provides the URL of the Elastic service.","title":"Metrics with/without auth"},{"location":"metrics/custom/#placeholder-substitution","text":"Note: This step is automated by Iter8 . Iter8 will substitute placeholders in the metric query based on the time elapsed since the start of the experiment, and information associated with each version in the experiment. Suppose the metrics defined above are referenced within an experiment as follows. Further, suppose this experiment has started, Iter8 is about to do an iteration of this experiment, and the time elapsed since the start of the experiment is 600 seconds. Look inside sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : sample-exp spec : target : default/sample-app strategy : testingPattern : Canary criteria : # This experiment assumes that metrics have been created in the `myns` namespace requestCount : myns/request-count objectives : - metric : myns/name-count lowerLimit : 50 - metric : myns/cpu-utilization upperLimit : 90 - metric : myns/average-sales lowerLimit : \"250.0\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : baseline : name : current variables : - name : revision value : sample-app-v1 - name : userfilter value : 'usergroup!~\"wakanda\"' candidates : - name : candidate variables : - name : revision value : sample-app-v2 - name : userfilter value : 'usergroup=~\"wakanda\"' For the sample experiment above, Iter8 will use two HTTP(S) queries to fetch metric values, one for the baseline version, and another for the candidate version. Prometheus Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named query whose value equals: sum(increase(revision_app_request_latencies_count{service_name='current',usergroup!~\"wakanda\"}[600s])) or on() vector(0) New Relic Consider the baseline version. Iter8 will send an HTTP(S) request with a single parameter named nrql whose value equals: SELECT count(appName) FROM PageView WHERE revisionName='sample-app-v1' SINCE 600 seconds ago Sysdig Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"last\" : 600 , \"sampling\" : 600 , \"filter\" : \"kubernetes.app.revision.name = 'sample-app-v1'\" , \"metrics\" : [ { \"id\" : \"cpu.cores.used\" , \"aggregations\" : { \"time\" : \"avg\" , \"group\" : \"sum\" } } ], \"dataSourceType\" : \"container\" , \"paging\" : { \"from\" : 0 , \"to\" : 99 } } Elastic Consider the baseline version. Iter8 will send an HTTP(S) request with the following JSON body: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"aggs\" : { \"range\" : { \"date_range\" : { \"field\" : \"date\" , \"ranges\" : [ { \"from\" : \"now-600s/s\" } ] } }, \"items_to_sell\" : { \"filter\" : { \"term\" : { \"version\" : \"sample-app-v1\" } }, \"aggs\" : { \"avg_sales\" : { \"avg\" : { \"field\" : \"sale_price\" } } } } } } The placeholder $elapsedTime has been substituted with 600, which is the time elapsed since the start of the experiment. The other placeholders have been substituted based on the versionInfo field of the baseline version in the experiment. Iter8 builds and sends an HTTP request in a similar manner for the candidate version as well.","title":"Placeholder substitution"},{"location":"metrics/custom/#json-response","text":"Note: This step is handled by the metrics provider . The metrics provider is expected to respond to Iter8's HTTP request with a JSON object. The format of this JSON object is defined by the provider. Prometheus The format of the Prometheus JSON response is defined here . A sample Prometheus response is as follows. 1 2 3 4 5 6 7 8 9 10 11 { \"status\" : \"success\" , \"data\" : { \"resultType\" : \"vector\" , \"result\" : [ { \"value\" : [ 1556823494.744 , \"21.7639\" ] } ] } } New Relic The format of the New Relic JSON response is discussed here . A sample New Relic response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"results\" : [ { \"count\" : 80275388 } ], \"metadata\" : { \"eventTypes\" : [ \"PageView\" ], \"eventType\" : \"PageView\" , \"openEnded\" : true , \"beginTime\" : \"2014-08-03T19:00:00Z\" , \"endTime\" : \"2017-01-18T23:18:41Z\" , \"beginTimeMillis=\" : 1407092400000 , \"endTimeMillis\" : 1484781521198 , \"rawSince\" : \"'2014-08-04 00:00:00+0500'\" , \"rawUntil\" : \"`now`\" , \"rawCompareWith\" : \"\" , \"clippedTimeWindows\" : { \"Browser\" : { \"beginTimeMillis\" : 1483571921198 , \"endTimeMillis\" : 1484781521198 , \"retentionMillis\" : 1209600000 } }, \"messages\" : [], \"contents\" : [ { \"function\" : \"count\" , \"attribute\" : \"appName\" , \"simple\" : true } ] } } Sysdig The format of the Sysdig JSON response is discussed here . A sample Sysdig response is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 { \"data\" : [ { \"t\" : 1582756200 , \"d\" : [ 6.481 ] } ], \"start\" : 1582755600 , \"end\" : 1582756200 } Elastic The format of the Elastic JSON response is discussed here . A sample Elastic response is as follows. 1 2 3 4 5 6 7 8 { \"aggregations\" : { \"items_to_sell\" : { \"doc_count\" : 3 , \"avg_sales\" : { \"value\" : 128.33333333333334 } } } }","title":"JSON response"},{"location":"metrics/custom/#processing-the-json-response","text":"Note: This step is automated by Iter8 . Iter8 uses jq to extract the metric value from the JSON response of the provider. The jqExpression used by Iter8 is supplied as part of the metric definition. When the jqExpression is applied to the JSON response, it is expected to yield a number. Prometheus Consider the jqExpression defined in the sample Prometheus metric . Let us apply it to the sample JSON response from Prometheus . echo '{ \"status\": \"success\", \"data\": { \"resultType\": \"vector\", \"result\": [ { \"value\": [1556823494.744, \"21.7639\"] } ] } }' | jq \".data.result[0].value[1] | tonumber\" Executing the above command results yields 21.7639 , a number, as required by Iter8. New Relic Consider the jqExpression defined in the sample New Relic metric . Let us apply it to the sample JSON response from New Relic . echo '{ \"results\": [ { \"count\": 80275388 } ], \"metadata\": { \"eventTypes\": [ \"PageView\" ], \"eventType\": \"PageView\", \"openEnded\": true, \"beginTime\": \"2014-08-03T19:00:00Z\", \"endTime\": \"2017-01-18T23:18:41Z\", \"beginTimeMillis=\": 1407092400000, \"endTimeMillis\": 1484781521198, \"rawSince\": \"' 2014 -08-04 00 :00:00+0500 '\", \"rawUntil\": \"`now`\", \"rawCompareWith\": \"\", \"clippedTimeWindows\": { \"Browser\": { \"beginTimeMillis\": 1483571921198, \"endTimeMillis\": 1484781521198, \"retentionMillis\": 1209600000 } }, \"messages\": [], \"contents\": [ { \"function\": \"count\", \"attribute\": \"appName\", \"simple\": true } ] } }' | jq \".results[0].count | tonumber\" Executing the above command results yields 80275388 , a number, as required by Iter8. Sysdig Consider the jqExpression defined in the sample Sysdig metric . Let us apply it to the sample JSON response from Sysdig . echo '{ \"data\": [ { \"t\": 1582756200, \"d\": [ 6.481 ] } ], \"start\": 1582755600, \"end\": 1582756200 }' | jq \".data[0].d[0] | tonumber\" Executing the above command results yields 6.481 , a number, as required by Iter8. Elastic Consider the jqExpression defined in the sample Elastic metric . Let us apply it to the sample JSON response from Elastic . echo '{ \"aggregations\": { \"items_to_sell\": { \"doc_count\": 3, \"avg_sales\": { \"value\": 128.33333333333334 } } } }' | jq \".aggregations.items_to_sell.avg_sales.value | tonumber\" Executing the above command results yields 128.33333333333334 , a number, as required by Iter8. Note: The shell command above is for illustration only. Iter8 uses Python bindings for jq to evaluate the jqExpression .","title":"Processing the JSON response"},{"location":"metrics/custom/#error-handling","text":"Note: This step is automated by Iter8 . Errors may occur during Iter8's metric queries due to a number of reasons (for example, due to an invalid jqExpression supplied within the metric). If Iter8 encounters errors during its attempt to retrieve metric values, Iter8 will mark the respective metric as unavailable. Iter8 can be used with any provider that can receive an HTTP request and respond with a JSON object containing the metrics information. Documentation requests and contributions (PRs) are welcome for providers not listed here. \u21a9 In a conformance experiment, n = 1 . In canary and A/B experiments, n = 2 . In A/B/n experiments, n > 2 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Error handling"},{"location":"metrics/mock/","text":"Metrics mock \u00b6 Mocking the value of a metric Iter8 enables you to mock the values of a metric. This is useful for learning purposes and quickly trying out sample Iter8 experiments without having to set up metric databases. Examples \u00b6 1 2 3 4 5 6 7 8 9 10 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement spec : mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" 1 2 3 4 5 6 7 8 9 10 11 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : type : Counter mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" Explanation \u00b6 When the mock field is present within a metric spec, Iter8 will mock the values for this metric. The name field refers to the name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. You can mock both Counter and Gauge metrics. The semantics of level field are as follows: If the metric is a counter, level is x , and time elapsed since the start of the experiment is y seconds, then xy is the metric value. Note that the (mocked) metric value will keep increasing over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the (mocked) metric will be x but its observed value may increase or decrease over time.","title":"Mock metrics"},{"location":"metrics/mock/#metrics-mock","text":"Mocking the value of a metric Iter8 enables you to mock the values of a metric. This is useful for learning purposes and quickly trying out sample Iter8 experiments without having to set up metric databases.","title":"Metrics mock"},{"location":"metrics/mock/#examples","text":"1 2 3 4 5 6 7 8 9 10 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement spec : mock : - name : default level : \"20.0\" - name : canary level : \"15.0\" 1 2 3 4 5 6 7 8 9 10 11 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : type : Counter mock : - name : default level : \"20.0\" - name : canary level : \"15.0\"","title":"Examples"},{"location":"metrics/mock/#explanation","text":"When the mock field is present within a metric spec, Iter8 will mock the values for this metric. The name field refers to the name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. You can mock both Counter and Gauge metrics. The semantics of level field are as follows: If the metric is a counter, level is x , and time elapsed since the start of the experiment is y seconds, then xy is the metric value. Note that the (mocked) metric value will keep increasing over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the (mocked) metric will be x but its observed value may increase or decrease over time.","title":"Explanation"},{"location":"metrics/using-metrics/","text":"Using Metrics in Experiments \u00b6 Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined built-in metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice. List metrics \u00b6 Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 built-in metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 built-in metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 built-in metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 built-in metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 built-in metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 built-in metric ) iter8-system request-count Counter Number of requests ( Iter8 built-in metric ) Referencing metrics within experiments \u00b6 Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" Observing metric values \u00b6 During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Using metrics"},{"location":"metrics/using-metrics/#using-metrics-in-experiments","text":"Iter8 metric resources Iter8 defines a custom Kubernetes resource (CRD) called Metric that makes it easy to define and use metrics in experiments. Iter8 installation includes a set of pre-defined built-in metrics that pertain to app/ML model latency/errors. You can also define custom metrics that enable you to utilize data from Prometheus, New Relic, Sysdig, Elastic or any other database of your choice.","title":"Using Metrics in Experiments"},{"location":"metrics/using-metrics/#list-metrics","text":"Find the set Iter8 metrics available in your cluster using kubectl get . kubectl get metrics.iter8.tools --all-namespaces NAMESPACE NAME TYPE DESCRIPTION iter8-kfserving user-engagement Gauge Average duration of a session iter8-system error-count Counter Number of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system error-rate Gauge Fraction of responses with HTTP status code 4xx or 5xx ( Iter8 built-in metric ) iter8-system latency-50th-percentile Gauge 50th percentile ( median ) latency ( Iter8 built-in metric ) iter8-system latency-75th-percentile Gauge 75th percentile latency ( Iter8 built-in metric ) iter8-system latency-90th-percentile Gauge 90th percentile latency ( Iter8 built-in metric ) iter8-system latency-95th-percentile Gauge 95th percentile latency ( Iter8 built-in metric ) iter8-system latency-99th-percentile Gauge 99th percentile latency ( Iter8 built-in metric ) iter8-system mean-latency Gauge Mean latency ( Iter8 built-in metric ) iter8-system request-count Counter Number of requests ( Iter8 built-in metric )","title":"List metrics"},{"location":"metrics/using-metrics/#referencing-metrics-within-experiments","text":"Use metrics in experiments by referencing them in the criteria section of the experiment manifest. Reference metrics using the namespace/name or name format . Sample experiment illustrating the use of metrics kind : Experiment ... spec : ... criteria : requestCount : iter8-knative/request-count # mean latency of version should be under 50 milliseconds # 95th percentile latency should be under 100 milliseconds # error rate should be under 1% objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\"","title":"Referencing metrics within experiments"},{"location":"metrics/using-metrics/#observing-metric-values","text":"During an experiment, Iter8 reports the metric values observed for each version. Use iter8ctl to observe these metric values in realtime. See here for an example.","title":"Observing metric values"},{"location":"reference/experiment/","text":"Experiment Resource \u00b6 Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml - if : not CandidateWon() run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent Version This document describes version v2alpha2 of Iter8's experiment API. Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No Status \u00b6 Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No Experiment field types \u00b6 Strategy \u00b6 Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing strategies, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No TaskSpec \u00b6 Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes condition string Execute the task only if the specified condition is satisfied. Two built-in conditions are supported namely, WinnerFound() and CandidateWon() . They can be combined with predicates like not : not WinnerFound() and not CandidateWon() are also valid conditions. 1 No with map[string] apiextensionsv1.JSON Inputs to the task. No Criteria \u00b6 Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure. Objective \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No Reward \u00b6 Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes Duration \u00b6 The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions . VersionInfo \u00b6 spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more. VersionDetail \u00b6 Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No MetricInfo \u00b6 Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No ExperimentCondition \u00b6 Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No Analysis \u00b6 Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables built-in metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No AggregatedBuiltinHists \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 built-in metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for built-in metics collection. Reserved for Iter8 internal use. No VersionAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No AggregatedMetricsAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes AggregatedMetricsData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No AggregatedMetricsVersionData \u00b6 Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No WinnerAssessmentAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No WinnerAssessmentData \u00b6 Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No WeightAnalysis \u00b6 Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No WeightData \u00b6 Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes Common field types \u00b6 NamedValue \u00b6 Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes This feature uses the expr Go expression library . The Experiment Go struct is used as the environment for expression evaluation. Hence, .Status.Analysis != nil is also a valid condition. \u21a9","title":"Experiment resource"},{"location":"reference/experiment/#experiment-resource","text":"Experiment resource Iter8's Experiment resource type enables application developers and service operators to automate A/B, A/B/n, Canary and Conformance experiments for Kubernetes apps/ML models. The controls provided by the experiment resource type encompass testing, deployment, traffic engineering, and version promotion functions . Sample experiment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml - if : not CandidateWon() run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml criteria : requestCount : iter8-knative/request-count rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-knative/mean-latency upperLimit : 50 - metric : iter8-knative/95th-percentile-tail-latency upperLimit : 100 - metric : iter8-knative/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent Version This document describes version v2alpha2 of Iter8's experiment API.","title":"Experiment Resource"},{"location":"reference/experiment/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/experiment/#spec","text":"Field name Field type Description Required target string Identifies the app under experimentation and determines which experiments can run concurrently. Experiments that have the same target value will not be scheduled concurrently but will be run sequentially in the order of their creation timestamps. Experiments whose target values differ from each other can be scheduled by Iter8 concurrently. Yes strategy Strategy The experimentation strategy which specifies how app versions are tested, how traffic is shifted during experiment, and what tasks are executed at the start and end of the experiment. Yes criteria Criteria Criteria used for evaluating versions. This section includes (business) rewards, service-level objectives (SLOs) and indicators (SLIs). No duration Duration Duration of the experiment. No versionInfo VersionInfo Versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates. No","title":"Spec"},{"location":"reference/experiment/#status","text":"Field name Field type Description Required conditions [] ExperimentCondition A set of conditions that express progress of an experiment. No initTime metav1.Time The time the experiment is created. No startTime metav1.Time The time when the first iteration of experiment begins No lastUpdateTime metav1.Time The time when the status was most recently updated. No stage string Indicator of the progress of an experiment. The stage is Waiting before an experiment executes its start action, Initializing while running the start action, Running while the experiment has begun its first iteration and is progressing, Finishing while any finish action is running and Completed when the experiment terminates. No completedIterations int32 Number of completed iterations of the experiment. This is undefined until the experiment reaches the Running stage. No currentWeightDistribution [] WeightData The latest observed split of traffic between versions. Expressed as percentage. Iter8 ensures that this field is current until the final iteration of the experiment. Iter8 will cease to update this field once a finish action is invoked. No analysis Analysis Result of latest query to the Iter8 analytics service. No versionRecommendedForPromotion string The version recommended for promotion. This field is initially populated by Iter8 as the baseline version and continuously updated during the course of the experiment to match the winner. The value of this field is typically used by finish actions to promote a version at the end of an experiment. No metrics [] MetricInfo A list of metrics referenced in the criteria section of this experiment. No message string Human readable message. No","title":"Status"},{"location":"reference/experiment/#experiment-field-types","text":"","title":"Experiment field types"},{"location":"reference/experiment/#strategy","text":"Field name Field type Description Required testingPattern string Determines the logic used to evaluate the app versions and determine the winner of the experiment. Iter8 supports two testing strategies, namely, Canary and Conformance . Yes deploymentPattern string Determines if and how traffic is shifted during an experiment. This field is relevant only for experiments using the Canary testing pattern. Iter8 supports two deployment patterns, namely, Progressive and FixedSplit . No actions map[ActionType][] TaskSpec An action is a sequence of tasks that can be executed by Iter8. ActionType is a string enum with three valid values: start , loop , and finish . The start action, if specified, is executed at the start of the experiment. The loop action, if specified, is executed during every loop of the experiment, after all the iterations within the loop have completed. The finish action, if specified, is executed at the end of the experiment after all the loops have completed. The actions field is used to specify all three types of actions. No","title":"Strategy"},{"location":"reference/experiment/#taskspec","text":"Specification of a task that will be executed as part of experiment actions. Tasks are documented here . Field name Field type Description Required task string Name of the task. Task names express both the library and the task within the library in the format 'library/task' . Yes condition string Execute the task only if the specified condition is satisfied. Two built-in conditions are supported namely, WinnerFound() and CandidateWon() . They can be combined with predicates like not : not WinnerFound() and not CandidateWon() are also valid conditions. 1 No with map[string] apiextensionsv1.JSON Inputs to the task. No","title":"TaskSpec"},{"location":"reference/experiment/#criteria","text":"Field name Field type Description Required requestCount string Reference to the metric used to count the number of requests sent to app versions. No rewards Reward [] A list of metrics along with their preferred directions. Currently, this list needs to be of size one. This field can only be used in experiments with A/B and A/B/n testing patterns. No objectives Objective [] A list of metrics along with acceptable upper limits, lower limits, or both upper and lower limits for them. Iter8 will verify if app versions satisfy these objectives. No indicators string[] A list of metric references. Iter8 will collect and report the values of these metrics in addition to those referenced in the objectives section. No Note: References to metric resource objects within experiment criteria should be in the namespace/name format or in the name format. If the name format is used (i.e., if only the name of the metric is specified), then Iter8 searches for the metric in the namespace of the experiment resource. If Iter8 cannot find the metric, then the reference is considered invalid and the experiment will terminate in a failure.","title":"Criteria"},{"location":"reference/experiment/#objective","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes upperLimit Quantity Upper limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be below the limit. No lowerLimit Quantity Lower limit on the metric value. If specified, for a version to satisfy this objective, its metric value needs to be above the limit. No","title":"Objective"},{"location":"reference/experiment/#reward","text":"Field name Field type Description Required metric string Reference to a metric resource. Also see note on metric references . Yes preferredDirection string Indicates if higher values or lower values of this metric are preferable. High and Low are the two permissible values for this string. Yes","title":"Reward"},{"location":"reference/experiment/#duration","text":"The duration of the experiment. Field name Field type Description Required maxLoops int32 Maximum number of loops in the experiment. In case of a failure, the experiment may be terminated earlier. Default value = 1. No iterationsPerLoop int32 Number of iterations per experiment loop . In case of a failure, the experiment may be terminated earlier. Default value = 15. No intervalSeconds int32 Duration of a single iteration of the experiment in seconds. Default value = 20 seconds. No Note : Suppose an experiment has maxLoops = x , iterationsPerLoop = y , and intervalSeconds = z . Assuming the experiment does not terminate early due to failures, it would take a minimum of x*y*z seconds to complete. The actual duration may be more due to additional time incurred in acquiring the target , and executing the start , loop and finish actions .","title":"Duration"},{"location":"reference/experiment/#versioninfo","text":"spec.versionInfo describes the app versions involved in the experiment. Every experiment involves a baseline version, and may involve zero or more candidates . Field name Field type Description Required baseline VersionDetail Details of the current or baseline version. Yes candidates [] VersionDetail Details of the candidate version or versions, if any. No Number of versions Conformance experiments involve only a single version (baseline). Hence, in these experiments, the candidates field must be omitted. A/B and Canary experiments involve two versions, a baseline and a candidate. Hence, the candidates field must be a list of length one in these experiments. A/B/n experiments involve three or more versions. Hence, in these experiments, the candidates field must be of length two or more.","title":"VersionInfo"},{"location":"reference/experiment/#versiondetail","text":"Field name Field type Description Required name string Name of the version. Yes variables [] NamedValue Variables are name-value pairs associated with a version. Metrics and tasks within experiment specs can contain strings with placeholders. Iter8 uses variables to substitute placeholders in these strings. No weightObjRef corev1.ObjectReference Reference to a Kubernetes resource and a field-path within the resource. Iter8 uses weightObjRef to get or set weight (traffic percentage) for the version. No","title":"VersionDetail"},{"location":"reference/experiment/#metricinfo","text":"Field name Field type Description Required name string Identifies an Iter8 metric using the namespace/name or name format . Yes metric [] Metric Iter8 metric object referenced by name. No","title":"MetricInfo"},{"location":"reference/experiment/#experimentcondition","text":"Conditions express aspects of the progress of an experiment. The Completed condition indicates whether or not an experiment has completed. The Failed condition indicates whether or not an experiment completed successfully or in failure. The TargetAcquired condition indicates that an experiment has acquired the target and is now scheduled to run. At any point in time, for any given target, Iter8 ensures that at most one experiment has the conditions TargetAcquired set to True and Completed set to False . Field name Field type Description Required type string Type of condition. Valid types are TargetAcquired , Completed and Failed . Yes status corev1.ConditionStatus status of condition, one of True , False , or Unknown . Yes lastTransitionTime metav1.Time The last time any field in the condition was changed. No reason string A reason for the change in value. No message string Human readable decription. No","title":"ExperimentCondition"},{"location":"reference/experiment/#analysis","text":"Field name Field type Description Required aggregatedBuiltinHists AggregatedBuiltinHists This field is used to store intermediate results from the metrics/collect task that enables built-in metrics . Reserved for Iter8 internal use. No aggregatedMetrics AggregatedMetricsAnalysis Most recently observed metric values for all metrics referenced in the experiment criteria. No winnerAssessment WinnerAssessmentAnalysis Information about the winner of the experiment. No versionAssessments VersionAssessmentAnalysis For each version, a summary analysis identifying whether or not the version is satisfying the experiment criteria. No weights WeightsAnalysis Recommended weight distribution to be applied before the next iteration of the experiment. No","title":"Analysis"},{"location":"reference/experiment/#aggregatedbuiltinhists","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 built-in metrics collect task is the only valid value for this field. Reserved for Iter8 internal use. Yes timestamp metav1.Time The time when this field was last updated. Reserved for Iter8 internal use. Yes message string Human readable message. Reserved for Iter8 internal use. No data apiextensionsv1.JSON Aggregated histogram data for storing intermediate results for built-in metics collection. Reserved for Iter8 internal use. No","title":"AggregatedBuiltinHists"},{"location":"reference/experiment/#versionassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string][]bool map of version name to a list of boolean values, one for each objective specified in the experiment criteria, indicating whether not the objective is satisified. No","title":"VersionAssessmentAnalysis"},{"location":"reference/experiment/#aggregatedmetricsanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data map[string] AggregatedMetricsData Map from metric name to most recent data (from all versions) for the metric. Yes","title":"AggregatedMetricsAnalysis"},{"location":"reference/experiment/#aggregatedmetricsdata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric accross all versions. Yes min Quantity The minimum value observed for this metric accross all versions. Yes data map[string] AggregatedMetricsVersionData A map from version name to the most recent aggregated metrics data for that version. No","title":"AggregatedMetricsData"},{"location":"reference/experiment/#aggregatedmetricsversiondata","text":"Field name Field type Description Required max Quantity The maximum value observed for this metric for this version over all observations. No min Quantity The minimum value observed for this metric for this version over all observations. No value Quantity The value. No sampleSize int32 The number of requests observed by this version. No","title":"AggregatedMetricsVersionData"},{"location":"reference/experiment/#winnerassessmentanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data WinnerAssessmentData Details on whether or not a winner has been identified and which version if so. No","title":"WinnerAssessmentAnalysis"},{"location":"reference/experiment/#winnerassessmentdata","text":"Field name Field type Description Required winnerFound bool Whether or not a winner has been identified. Yes winner string The name of the identified winner, if one has been found. No","title":"WinnerAssessmentData"},{"location":"reference/experiment/#weightanalysis","text":"Field name Field type Description Required provenance string Source of the data. Currently, Iter8 analytics service URL is the only value for this field. Yes timestamp metav1.Time The time when the analysis took place. Yes message string Human readable message. No data [] WeightData List of version name/value pairs representing a recommended weight for each version No","title":"WeightAnalysis"},{"location":"reference/experiment/#weightdata","text":"Field name Field type Description Required name string Version name Yes value int32 Percentage of traffic being sent to the version. Yes","title":"WeightData"},{"location":"reference/experiment/#common-field-types","text":"","title":"Common field types"},{"location":"reference/experiment/#namedvalue","text":"Field name Field type Description Required name string Name of a variable. Yes value string Value of a variable. Yes This feature uses the expr Go expression library . The Experiment Go struct is used as the environment for expression evaluation. Hence, .Status.Analysis != nil is also a valid condition. \u21a9","title":"NamedValue"},{"location":"reference/helmex-schema/","text":"Helmex Schema \u00b6 The Helmex schema for the Helm values.yaml file is described below. It is intended for applications that are templated using Helm and use Iter8 experiments during releases. In addition to the below requirements, an application may impose additional application-specific schema requirements on values.yaml . Top-level fields \u00b6 Field name Field type Description Required baseline Version Information about the baseline version of the application. The baseline version typically corresponds to the stable version of the application. Yes candidate Version Information about the candidate version. Iter8 experiment resource will be created only if this field is present. If this field is modified, any existing experiment for the application will be replaced by a new experiment. No Version \u00b6 Field name Field type Description Required dynamic Dynamic Information associated with a specific version. For example, each time the baseline version of the application changes, the baseline.dynamic field in the Helm values file should change. Yes Dynamic \u00b6 Field name Field type Description Required id string Alpha-numeric string that uniquely identifies a version. This optional field is strongly recommended for every version. No. Example \u00b6 The following Helm values file is an instance of the Helmex schema. # values meant for both baseline and candidate versions of the application; common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello # values meant for baseline version of the application only; # baseline version is required by Helmex schema baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline # required field for baseline version dynamic : # unique alpha-numeric version ID is strongly recommended id : \"mn82l82\" tag : \"1.0\" # values meant for candidate version of the application only; # optional section; Iter8 experiment will be deployed if this section is present candidate : name : hello-candidate selectorLabels : app.kubernetes.io/track : candidate # required field for candidate version # if candidate is promoted, the dynamic field from candidate will be copied over to baseline, and candidate will be set to null dynamic : # unique alpha-numeric version ID is strongly recommended id : \"8s72oa\" tag : \"2.0\" # this section is used in the creation of the Iter8 experiment # the specific experiment section below is used in the context of an SLO validation experiment experiment : # The SLO validation experiment will collect Iter8's built-in latency and error metrics. # There will be 8.0 * 5 = 40 queries sent during metrics collection. # time is the duration over which queries are sent during metrics collection. time : 5s # QPS is number of queries per second sent during metrics collection. QPS : 8.0 # (msec) acceptable limit for mean latency of the application limitMeanLatency : 500.0 # (msec) acceptable error rate for the application (1%) limitErrorRate : 0.01 # (msec) acceptable limit for 95th percentile latency of the application limit95thPercentileLatency : 1000.0","title":"Helmex schema"},{"location":"reference/helmex-schema/#helmex-schema","text":"The Helmex schema for the Helm values.yaml file is described below. It is intended for applications that are templated using Helm and use Iter8 experiments during releases. In addition to the below requirements, an application may impose additional application-specific schema requirements on values.yaml .","title":"Helmex Schema"},{"location":"reference/helmex-schema/#top-level-fields","text":"Field name Field type Description Required baseline Version Information about the baseline version of the application. The baseline version typically corresponds to the stable version of the application. Yes candidate Version Information about the candidate version. Iter8 experiment resource will be created only if this field is present. If this field is modified, any existing experiment for the application will be replaced by a new experiment. No","title":"Top-level fields"},{"location":"reference/helmex-schema/#version","text":"Field name Field type Description Required dynamic Dynamic Information associated with a specific version. For example, each time the baseline version of the application changes, the baseline.dynamic field in the Helm values file should change. Yes","title":"Version"},{"location":"reference/helmex-schema/#dynamic","text":"Field name Field type Description Required id string Alpha-numeric string that uniquely identifies a version. This optional field is strongly recommended for every version. No.","title":"Dynamic"},{"location":"reference/helmex-schema/#example","text":"The following Helm values file is an instance of the Helmex schema. # values meant for both baseline and candidate versions of the application; common : application : hello repo : \"gcr.io/google-samples/hello-app\" serviceType : ClusterIP servicePortInfo : port : 8080 regularLabels : app.kubernetes.io/managed-by : Iter8 selectorLabels : app.kubernetes.io/name : hello # values meant for baseline version of the application only; # baseline version is required by Helmex schema baseline : name : hello selectorLabels : app.kubernetes.io/track : baseline # required field for baseline version dynamic : # unique alpha-numeric version ID is strongly recommended id : \"mn82l82\" tag : \"1.0\" # values meant for candidate version of the application only; # optional section; Iter8 experiment will be deployed if this section is present candidate : name : hello-candidate selectorLabels : app.kubernetes.io/track : candidate # required field for candidate version # if candidate is promoted, the dynamic field from candidate will be copied over to baseline, and candidate will be set to null dynamic : # unique alpha-numeric version ID is strongly recommended id : \"8s72oa\" tag : \"2.0\" # this section is used in the creation of the Iter8 experiment # the specific experiment section below is used in the context of an SLO validation experiment experiment : # The SLO validation experiment will collect Iter8's built-in latency and error metrics. # There will be 8.0 * 5 = 40 queries sent during metrics collection. # time is the duration over which queries are sent during metrics collection. time : 5s # QPS is number of queries per second sent during metrics collection. QPS : 8.0 # (msec) acceptable limit for mean latency of the application limitMeanLatency : 500.0 # (msec) acceptable error rate for the application (1%) limitErrorRate : 0.01 # (msec) acceptable limit for 95th percentile latency of the application limit95thPercentileLatency : 1000.0","title":"Example"},{"location":"reference/metrics/","text":"Metric Resource \u00b6 Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here . Iter8 provides a set of built-in metrics that are documented here . Creation of custom metrics is documented here . It is also possible to mock metric values; mock metrics are documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metadata \u00b6 Standard Kubernetes meta.v1/ObjectMeta resource. Spec \u00b6 Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 built-in metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No mock [] NamedLevel Information on how to mock this metric. The presence of this fields indicates that this metric will be mocked; the absence of this field indicates that this is a real metric. No Named Level \u00b6 Field name Field type Description Required name string Name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. Yes level string Level of a version. The semantics of level are as follows. If the metric is a counter, if level is x , and time elapsed since the start of the experiment is y seconds, then xy is the mocked metric value. This will keep increasing the metric value over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the mocked metric is x ; the actual value may increase or decrease over time. Yes","title":"Metric resource"},{"location":"reference/metrics/#metric-resource","text":"Metric resource Iter8 defines the Metric resource type, which encapsulates the REST query that is used to retrieve a metric value from the metrics provider. Metric resources are referenced in experiments. Version This document describes version v2alpha2 of Iter8's metric API. Metrics usage is documented here . Iter8 provides a set of built-in metrics that are documented here . Creation of custom metrics is documented here . It is also possible to mock metric values; mock metrics are documented here . Sample metric 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count spec : params : - name : query value : | sum(increase(revision_app_request_latencies_count{revision_name='$revision'}[$elapsedTime])) or on() vector(0) description : Number of requests type : counter provider : prometheus jqExpression : \".data.result[0].value[1] | tonumber\" urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"Metric Resource"},{"location":"reference/metrics/#metadata","text":"Standard Kubernetes meta.v1/ObjectMeta resource.","title":"Metadata"},{"location":"reference/metrics/#spec","text":"Field name Field type Description Required description string Human readable description. This field is meant for informational purposes. No units string Units of measurement. This field is meant for informational purposes. No provider string Type of the metrics provider (example, prometheus , newrelic , sysdig , elastic , ...). The keyword iter8 is reserved for Iter8 built-in metrics. No params [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP query parameters used by Iter8 when querying the metrics provider. Each name represents a parameter name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. No body string String used to construct the JSON body of the HTTP request. Body may be templated, in which Iter8 will attempt to substitute placeholders in the template at query time using version information. No type string Metric type. Valid values are Counter and Gauge . Default value = Gauge . A Counter metric is one whose value never decreases over time. A Gauge metric is one whose value may increase or decrease over time. No method string HTTP method (verb) used in the HTTP request. Valid values are GET and POST . Default value = GET . No authType string Identifies the type of authentication used in the HTTP request. Valid values are Basic , Bearer and APIKey which correspond to HTTP authentication with these respective methods. No sampleSize string Reference to a metric that represents the number of data points over which the value of this metric is computed. This field applies only to Gauge metrics. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace of the referring metric. No secret string Reference to a secret that contains information used for authenticating with the metrics provider. In particular, Iter8 uses data in this secret to substitute placeholders in the HTTP headers and URL while querying the provider. References can be expressed in the form 'name' or 'namespace/name'. If just name is used, the implied namespace is the namespace where Iter8 is installed (which is iter8-system by default). No headerTemplates [] NamedValue List of name/value pairs corresponding to the name and value of the HTTP request headers used by Iter8 when querying the metrics provider. Each name represents a header field name; the corresponding value is a string template with placeholders; the placeholders will be dynamically substituted by Iter8 with values at query time. Placeholder substitution is attempted only if authType and secret fields are present. No jqExpression string The jq expression used by Iter8 to extract the metric value from the JSON response returned by the provider. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No urlTemplate string Template for the metric provider's URL. Typically, urlTemplate is expected to be the actual URL without any placeholders. However, urlTemplate may be templated, in which case, Iter8 will attempt to substitute placeholders in the urlTemplate at query time using the secret referenced in the metric. Placeholder substitution will not be attempted if secret is not specified. This field may be omitted if the metric is mocked . No mock [] NamedLevel Information on how to mock this metric. The presence of this fields indicates that this metric will be mocked; the absence of this field indicates that this is a real metric. No","title":"Spec"},{"location":"reference/metrics/#named-level","text":"Field name Field type Description Required name string Name of the version. Version names should be unique. Version name should match the name of a version in the experiment's versionInfo section. If not, any value generated for the non-matching name will be ignored. Yes level string Level of a version. The semantics of level are as follows. If the metric is a counter, if level is x , and time elapsed since the start of the experiment is y seconds, then xy is the mocked metric value. This will keep increasing the metric value over time. If the metric is gauge, if level is x , the metric value is a random value with mean x . The expected value of the mocked metric is x ; the actual value may increase or decrease over time. Yes","title":"Named Level"},{"location":"reference/tasks/common-readiness/","text":"common/readiness \u00b6 The common/readiness task can be used to verify that Kubernetes resources (for example, deployments, Knative services, or Istio virtual services) that are required for the experiment are available and ready. This task is intended to be included in the start action of an experiment. Example \u00b6 The following is an experiment snippet with a common/readiness task. ... spec : strategy : actions : start : - task : common/readiness with : # verify that the following deployments exist objRefs : - kind : Deployment name : hello namespace : default # verify that the deployment is available waitFor : condition=available - kind : Deployment name : hello-candidate namespace : default # verify that the deployment is available waitFor : condition=available ... # `common/readiness` task will also inspect the versionInfo section. # If resources are specified as part of weightObjRef fields within versionInfo, # the task will verify the existence of these resources as well. versionInfo : baseline : name : stable weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[1].weight Inputs \u00b6 Field name Field type Description Required initialDelaySeconds int Verification will be attempted only after this initial delay. Default value is 5 . No numRetries int If the task cannot verify the existence/conditions of the resources after the first attempt, it will retry with further attempts. Total number of attempts = 1 + numRetries. Default value for numRetries is 12 . No intervalSeconds int Time interval between each attempt. Default value is 5 . No objRefs [] ObjRef A list of Kubernetes object references along with any associated conditions which need to be verified. No ObjRef \u00b6 Field name Field type Description Required kind string Kind of the Kubernetes resource. Specified in the TYPE[.VERSION][.GROUP] format used by the kubectl get command Yes namespace string Namespace of the Kubernetes resource. Default value is the namespace of the experiment resource. No name string Name of the Kubernetes resource. Yes waitFor string Any value that is accepted by the --for flag of the kubectl wait command . No Result \u00b6 The task will succeed if all the specified resources are verified to exist (along with any associated conditions) within the specified number of attempts. Otherwise, the task will fail, resulting in the failure of the experiment.","title":"common/readiness"},{"location":"reference/tasks/common-readiness/#commonreadiness","text":"The common/readiness task can be used to verify that Kubernetes resources (for example, deployments, Knative services, or Istio virtual services) that are required for the experiment are available and ready. This task is intended to be included in the start action of an experiment.","title":"common/readiness"},{"location":"reference/tasks/common-readiness/#example","text":"The following is an experiment snippet with a common/readiness task. ... spec : strategy : actions : start : - task : common/readiness with : # verify that the following deployments exist objRefs : - kind : Deployment name : hello namespace : default # verify that the deployment is available waitFor : condition=available - kind : Deployment name : hello-candidate namespace : default # verify that the deployment is available waitFor : condition=available ... # `common/readiness` task will also inspect the versionInfo section. # If resources are specified as part of weightObjRef fields within versionInfo, # the task will verify the existence of these resources as well. versionInfo : baseline : name : stable weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[0].weight candidates : - name : candidate weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : default name : hello-routing fieldPath : .spec.http[0].route[1].weight","title":"Example"},{"location":"reference/tasks/common-readiness/#inputs","text":"Field name Field type Description Required initialDelaySeconds int Verification will be attempted only after this initial delay. Default value is 5 . No numRetries int If the task cannot verify the existence/conditions of the resources after the first attempt, it will retry with further attempts. Total number of attempts = 1 + numRetries. Default value for numRetries is 12 . No intervalSeconds int Time interval between each attempt. Default value is 5 . No objRefs [] ObjRef A list of Kubernetes object references along with any associated conditions which need to be verified. No","title":"Inputs"},{"location":"reference/tasks/common-readiness/#objref","text":"Field name Field type Description Required kind string Kind of the Kubernetes resource. Specified in the TYPE[.VERSION][.GROUP] format used by the kubectl get command Yes namespace string Namespace of the Kubernetes resource. Default value is the namespace of the experiment resource. No name string Name of the Kubernetes resource. Yes waitFor string Any value that is accepted by the --for flag of the kubectl wait command . No","title":"ObjRef"},{"location":"reference/tasks/common-readiness/#result","text":"The task will succeed if all the specified resources are verified to exist (along with any associated conditions) within the specified number of attempts. Otherwise, the task will fail, resulting in the failure of the experiment.","title":"Result"},{"location":"reference/tasks/metrics-collect/","text":"metrics/collect \u00b6 The metrics/collect task enables collection of built-in metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency and error metrics. Example \u00b6 The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects built-in latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # If the version name matches the name of a version in the experiment's `versionInfo` section, # then the version is considered *real*. # If the version name does not match the name of a version in the experiment's `versionInfo` section, # then the version is considered *pseudo*. # Built-in metrics collected for real versions can be used within the experiment's `criteria` section. # Pseudo versions are useful if the intent is only to generate load (`GET` and `POST` requests). # Built-in metrics collected for pseudo versions cannot be used with the experiment's `criteria` section. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000 Inputs \u00b6 Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No Version \u00b6 Field name Field type Description Required name string Name of the version. Version names must be unique. If the version name matches the name of a version in the experiment's versionInfo section, then the version is considered real . If the version name does not match the name of a version in the experiment's versionInfo section, then the version is considered pseudo . Built-in metrics collected for real versions can be used within the experiment's criteria section. Pseudo versions are useful if the intent is only to generate load ( GET and POST requests). Built-in metrics collected for pseudo versions cannot be used with the experiment's criteria section. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes Result \u00b6 This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Built-in metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail. Start vs loop actions \u00b6 If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated. Load generation without metrics collection \u00b6 You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"metrics/collect"},{"location":"reference/tasks/metrics-collect/#metricscollect","text":"The metrics/collect task enables collection of built-in metrics . It generates a stream of HTTP requests to one or more app/ML model versions, and collects latency and error metrics.","title":"metrics/collect"},{"location":"reference/tasks/metrics-collect/#example","text":"The following start action contains a metrics/collect task which is executed at the start of the experiment. The task sends a certain number of HTTP requests to each version specified in the task, and collects built-in latency/error metrics for them. start : - task : metrics/collect with : versions : # Version names must be unique. # If the version name matches the name of a version in the experiment's `versionInfo` section, # then the version is considered *real*. # If the version name does not match the name of a version in the experiment's `versionInfo` section, # then the version is considered *pseudo*. # Built-in metrics collected for real versions can be used within the experiment's `criteria` section. # Pseudo versions are useful if the intent is only to generate load (`GET` and `POST` requests). # Built-in metrics collected for pseudo versions cannot be used with the experiment's `criteria` section. - name : iter8-app # URL is where this version receives HTTP requests url : http://iter8-app.default.svc:8000 - name : iter8-app-candidate url : http://iter8-app-candidate.default.svc:8000","title":"Example"},{"location":"reference/tasks/metrics-collect/#inputs","text":"Field name Field type Description Required time string Duration of the metrics/collect task run. Specified in the Go duration string format . Default value is 5s . No payloadURL string URL of JSON-encoded data. If this field is specified, the metrics collector will send HTTP POST requests to versions, and the POST requests will contain this JSON data as payload. No versions [] Version A non-empty list of versions. Yes loadOnly bool If set to true, this task will send requests without collecting metrics. Default value is false . No","title":"Inputs"},{"location":"reference/tasks/metrics-collect/#version","text":"Field name Field type Description Required name string Name of the version. Version names must be unique. If the version name matches the name of a version in the experiment's versionInfo section, then the version is considered real . If the version name does not match the name of a version in the experiment's versionInfo section, then the version is considered pseudo . Built-in metrics collected for real versions can be used within the experiment's criteria section. Pseudo versions are useful if the intent is only to generate load ( GET and POST requests). Built-in metrics collected for pseudo versions cannot be used with the experiment's criteria section. Yes qps float How many queries per second will be sent to this version. Default is 8.0. No headers map[string]string HTTP headers to be used in requests sent to this version. No url string HTTP URL of this version. Yes","title":"Version"},{"location":"reference/tasks/metrics-collect/#result","text":"This task will run for the specified duration ( time ), send requests to each version ( versions ) at the specified rate ( qps ), and will collect built-in metrics for each version. Built-in metric values are stored in the metrics field of the experiment status in the same manner as custom metric values. The task may result in an error, for instance, if one or more required fields are missing or if URLs are mis-specified. In this case, the experiment to which it belongs will fail.","title":"Result"},{"location":"reference/tasks/metrics-collect/#start-vs-loop-actions","text":"If this task is embedded in start actions, it will run once at the beginning of the experiment. If this task is embedded in loop actions, it will run in each loop of the experiment. The results from each run will be aggregated.","title":"Start vs loop actions"},{"location":"reference/tasks/metrics-collect/#load-generation-without-metrics-collection","text":"You can use this task to send HTTP GET and POST requests to app/ML model versions without collecting metrics by setting the loadOnly input to true .","title":"Load generation without metrics collection"},{"location":"reference/tasks/notification-http/","text":"notification/http \u00b6 The notification/http task can be used to send an HTTP request either as a form of notification or to trigger an action. For example, this task can be used to trigger a GitHub Action or a Tekton pipeline. Example \u00b6 The following is an experiment snippet with a notification/http task that triggers a GitHub action that takes two inputs. Here the inputs are set to the name and namespace of the experiment. ... spec : strategy : actions : finish : - task : notification/http with : url : https://api.github.com/repos/ORG/REPO/actions/workflows/ACTION.yaml/dispatches body : | { \"ref\":\"master\", \"inputs\":{ \"name\": \"{{.this.metadata.name}}\", \"home\":\"{{.this.metadata.namespace}}\" } } secret : default/github-token authType : Bearer headers : - name : Accept value : application/vnd.github.v3+json ... Inputs \u00b6 Field name Field type Description Required url string URL to which request is to be made. May contain placeholders that will be subsituted at runtime. Yes method string HTTP request method to use; either POST or GET . Default value is POST . No authtype string Type of authorization to use. Valid values are Basic , Bearer and APIKey . If not set, no authorization is used. No secret string Name of a secret (in form of namespace/name ). Values are used to dynamically substitute placeholders. No headers [] NamedValue A list of name-value pairs that are converted into headers as part of the request. Values may contain placeholders that will be subsituted at runtime. No body string The body of the request to be sent. May contain placeholders that will be subsituted at runtime. No ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No The url , headers and body may all contain placeholders that will be substituted at runtime. A placeholder prefixed by .secret will use value from the secret. A placeholder prefixed by .this will use a value from the experiment. If not set, the default body will be of the following form: { \"summary\" : { \"winnerFound\" : true , \"winner\" : \"candidate\" , \"versionRecommededForPromotion\" : \"candidate\" , \"lastRecommendedWeights\" : [ { \"name\" : \"candiate\" \"weight\" : 95 }, { \"name\" : \"baseline\" \"weight\" : 5 } ] }, \"experiment\" : <JSON represe ntat io n o f t he experime nt objec t > } In the default body, the winner will be set only if winnerFound is true . The versionRecommededForPromotion field will be omitted in start actions but will be included thereafter. The weights are the last weights recommended by the analytics engine. Note that this may not match the current weights. No authoriziation is provided if authtype is not set. If it is set, the behavior is as follows: Basic : the task expects fields named username and password in the secret . These will be used to add an appropriate Authorization header to the request. Bearer : the expects a field named token in the secret . This will be used to construct a suitable Authorization header to the request. APIKey : the task expects the header field to explicitly specify any needed authorization headers. Placeholders can be used to explicitly refer to any required values from the secret . By default, a Content-type header with value application/json is included in the request. This can be replaced by specifying a different value. For example to set it by text/plain by: ... headers : - name : Content-type value : text/plain ... Result \u00b6 The task will create and send an HTTP request.","title":"notification/http"},{"location":"reference/tasks/notification-http/#notificationhttp","text":"The notification/http task can be used to send an HTTP request either as a form of notification or to trigger an action. For example, this task can be used to trigger a GitHub Action or a Tekton pipeline.","title":"notification/http"},{"location":"reference/tasks/notification-http/#example","text":"The following is an experiment snippet with a notification/http task that triggers a GitHub action that takes two inputs. Here the inputs are set to the name and namespace of the experiment. ... spec : strategy : actions : finish : - task : notification/http with : url : https://api.github.com/repos/ORG/REPO/actions/workflows/ACTION.yaml/dispatches body : | { \"ref\":\"master\", \"inputs\":{ \"name\": \"{{.this.metadata.name}}\", \"home\":\"{{.this.metadata.namespace}}\" } } secret : default/github-token authType : Bearer headers : - name : Accept value : application/vnd.github.v3+json ...","title":"Example"},{"location":"reference/tasks/notification-http/#inputs","text":"Field name Field type Description Required url string URL to which request is to be made. May contain placeholders that will be subsituted at runtime. Yes method string HTTP request method to use; either POST or GET . Default value is POST . No authtype string Type of authorization to use. Valid values are Basic , Bearer and APIKey . If not set, no authorization is used. No secret string Name of a secret (in form of namespace/name ). Values are used to dynamically substitute placeholders. No headers [] NamedValue A list of name-value pairs that are converted into headers as part of the request. Values may contain placeholders that will be subsituted at runtime. No body string The body of the request to be sent. May contain placeholders that will be subsituted at runtime. No ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No The url , headers and body may all contain placeholders that will be substituted at runtime. A placeholder prefixed by .secret will use value from the secret. A placeholder prefixed by .this will use a value from the experiment. If not set, the default body will be of the following form: { \"summary\" : { \"winnerFound\" : true , \"winner\" : \"candidate\" , \"versionRecommededForPromotion\" : \"candidate\" , \"lastRecommendedWeights\" : [ { \"name\" : \"candiate\" \"weight\" : 95 }, { \"name\" : \"baseline\" \"weight\" : 5 } ] }, \"experiment\" : <JSON represe ntat io n o f t he experime nt objec t > } In the default body, the winner will be set only if winnerFound is true . The versionRecommededForPromotion field will be omitted in start actions but will be included thereafter. The weights are the last weights recommended by the analytics engine. Note that this may not match the current weights. No authoriziation is provided if authtype is not set. If it is set, the behavior is as follows: Basic : the task expects fields named username and password in the secret . These will be used to add an appropriate Authorization header to the request. Bearer : the expects a field named token in the secret . This will be used to construct a suitable Authorization header to the request. APIKey : the task expects the header field to explicitly specify any needed authorization headers. Placeholders can be used to explicitly refer to any required values from the secret . By default, a Content-type header with value application/json is included in the request. This can be replaced by specifying a different value. For example to set it by text/plain by: ... headers : - name : Content-type value : text/plain ...","title":"Inputs"},{"location":"reference/tasks/notification-http/#result","text":"The task will create and send an HTTP request.","title":"Result"},{"location":"reference/tasks/notification-slack/","text":"notification/slack \u00b6 The notification/slack task posts a Slack message about current state of the experiment. Example \u00b6 The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token Inputs \u00b6 Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No Result \u00b6 A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task. Requirements \u00b6 Slack API token \u00b6 An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token> Permission to read secret with Slack token \u00b6 The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system Slack channel ID \u00b6 A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"notification/slack"},{"location":"reference/tasks/notification-slack/#notificationslack","text":"The notification/slack task posts a Slack message about current state of the experiment.","title":"notification/slack"},{"location":"reference/tasks/notification-slack/#example","text":"The following task notifies a Slack channel with id C0138103183 and using the token contained in the secret slack-token in the ns namespace. task : notification/slack with : channel : C0138103183 secret : ns/slack-token","title":"Example"},{"location":"reference/tasks/notification-slack/#inputs","text":"Field name Field type Description Required channel string Name of the Slack channel to which messages should be posted. Yes secret string Identifies a secret containing a token to be used for authentication. Expressed as namespace/name . If namespace is not specified, the namespace of the experiment is used. Yes ignoreFailure bool A flag indicating whether or not to ignore failures. If failures are not ignored, they cause the experiment to fail. Default is true . No","title":"Inputs"},{"location":"reference/tasks/notification-slack/#result","text":"A Slack message describing the experiment will be posted to the specified channel. Below is a sample Slack notification from this task.","title":"Result"},{"location":"reference/tasks/notification-slack/#requirements","text":"","title":"Requirements"},{"location":"reference/tasks/notification-slack/#slack-api-token","text":"An API token allowing posting messages to the desired Slack channel is needed. To obtain a suitable token, see Sending messages using Incoming Webhooks . Once you have the token, store it in a Kubernetes secret. For example, to create the secret slack-secret in the default namespace: kubectl create secret generic slack-secret --from-literal = token = <slack token>","title":"Slack API token"},{"location":"reference/tasks/notification-slack/#permission-to-read-secret-with-slack-token","text":"The Iter8 task runner needs permission to read the identified secret. For example the following RBAC changes will allow the task runner read the secret from the default namespace: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml Inspect role and rolebinding 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # This role enables reading of secrets apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : iter8-secret-reader rules : - apiGroups : - \"\" resources : - secrets verbs : [ \"get\" , \"list\" ] --- # This role binding enables Iter8 handler to read secrets in the default namespace. # To change the namespace apply to the target namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : iter8-secret-reader-handler roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : iter8-secret-reader subjects : - kind : ServiceAccount name : iter8-handlers namespace : iter8-system","title":"Permission to read secret with Slack token"},{"location":"reference/tasks/notification-slack/#slack-channel-id","text":"A Slack channel is identified by an id. To find the id, open the Slack channel in a web browser. The channel id is the portion of the URL of the form: CXXXXXXXX","title":"Slack channel ID"},{"location":"reference/tasks/overview/","text":"Tasks \u00b6 Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. The following tasks are available for use in Iter8 experiments. run : Execute a bash script. common/readiness : Check if K8s resources needed for an experiment are available and/or ready. metrics/collect : Generate GET/POST requests for the application versions and collect latency and error metrics. This task supports Iter8's built-in metrics feature. notification/http : Send a HTTP request to a specified target. notification/slack : Send a slack notification with a summary of the experiment. Actions \u00b6 Tasks specified within start, finish, or loop actions with be executed by Iter8 at the start of an experiment, end of an experiment, or periodically during each loop of the experiment respectively. See spec.strategy.actions for details about experiment actions.","title":"Task overview"},{"location":"reference/tasks/overview/#tasks","text":"Tasks are an extension mechanism for enhancing the behavior of Iter8 experiments and can be specified within the spec.strategy.actions field of the experiment. The following tasks are available for use in Iter8 experiments. run : Execute a bash script. common/readiness : Check if K8s resources needed for an experiment are available and/or ready. metrics/collect : Generate GET/POST requests for the application versions and collect latency and error metrics. This task supports Iter8's built-in metrics feature. notification/http : Send a HTTP request to a specified target. notification/slack : Send a slack notification with a summary of the experiment.","title":"Tasks"},{"location":"reference/tasks/overview/#actions","text":"Tasks specified within start, finish, or loop actions with be executed by Iter8 at the start of an experiment, end of an experiment, or periodically during each loop of the experiment respectively. See spec.strategy.actions for details about experiment actions.","title":"Actions"},{"location":"reference/tasks/run/","text":"run \u00b6 The run task executes a bash script. Basic Example \u00b6 The following (partially-specified) experiment executes the one line script kubectl apply using a YAML manifest at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : - run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/mysample/manifest.yaml Conditional Execution \u00b6 The run task can be executed conditionally , where the condition is specified using the if clause. Supported conditions include WinnerFound() , CandidateWon() and their negations using the not keyword. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" Secret \u00b6 The run task can be used with a Kubernetes secret provided to it as an input. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - run : \"git clone https://username:{{ .Secret 'token' }}@github.com/username/repo.git\" with : # reference to a K8s secret resource in the namespace/name format. If no namespace is specified, the namespace of the secret is assumed to be that of the experiment. secret : myns/mysecret In the above example, a token value is extracted from the given Kubernetes secret, and inserted into the (templated) git clone script. This task requires the secret resource mysecret to be available in the myns namespace, and requires the secret to contain token as a key in its data section. Scratch folder \u00b6 The SCRATCH_DIR environment variable points to a scratch folder. This intended for creating and manipulating files as part of the run script. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - run : | cd $SCRATCH_DIR echo \"hello\" > world.txt Available commands \u00b6 The Dockerfile used to build the task runner image is here . In addition to the standard linux commands ( sed , awk , ...) available in its base image, the task runner also includes the commands kubectl , kustomize , helm , yq , git , curl , and gh . kind : Experiment ... spec : ... strategy : ... actions : finish : # a few things you can do within the run script - run : | kustomize build hello/world/folder > manifest.yaml kubectl apply -f manifest.yaml helm upgrade my-app helm/chart --install yq -i a=b manifest.yaml git clone https://github.com/iter8-tools/iter8.git gh pr create curl https://iter8.tools -O $SCARCH_DIR/i.html Inputs \u00b6 Field name Field type Description Required secret string Reference to a K8s secret in the namespace/name format. If no namespace is specified, then the namespace is assumed to be that of the experiment. No Result \u00b6 The script will be executed. If this script exits with a non-zero error code, the run task and therefore the experiment to which it belongs will fail.","title":"run"},{"location":"reference/tasks/run/#run","text":"The run task executes a bash script.","title":"run"},{"location":"reference/tasks/run/#basic-example","text":"The following (partially-specified) experiment executes the one line script kubectl apply using a YAML manifest at the end of the experiment. kind : Experiment ... spec : ... strategy : ... actions : finish : - run : kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/mysample/manifest.yaml","title":"Basic Example"},{"location":"reference/tasks/run/#conditional-execution","text":"The run task can be executed conditionally , where the condition is specified using the if clause. Supported conditions include WinnerFound() , CandidateWon() and their negations using the not keyword. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\"","title":"Conditional Execution"},{"location":"reference/tasks/run/#secret","text":"The run task can be used with a Kubernetes secret provided to it as an input. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - run : \"git clone https://username:{{ .Secret 'token' }}@github.com/username/repo.git\" with : # reference to a K8s secret resource in the namespace/name format. If no namespace is specified, the namespace of the secret is assumed to be that of the experiment. secret : myns/mysecret In the above example, a token value is extracted from the given Kubernetes secret, and inserted into the (templated) git clone script. This task requires the secret resource mysecret to be available in the myns namespace, and requires the secret to contain token as a key in its data section.","title":"Secret"},{"location":"reference/tasks/run/#scratch-folder","text":"The SCRATCH_DIR environment variable points to a scratch folder. This intended for creating and manipulating files as part of the run script. kind : Experiment ... spec : ... strategy : ... actions : finish : # run the following sequence of tasks at the end of the experiment - run : | cd $SCRATCH_DIR echo \"hello\" > world.txt","title":"Scratch folder"},{"location":"reference/tasks/run/#available-commands","text":"The Dockerfile used to build the task runner image is here . In addition to the standard linux commands ( sed , awk , ...) available in its base image, the task runner also includes the commands kubectl , kustomize , helm , yq , git , curl , and gh . kind : Experiment ... spec : ... strategy : ... actions : finish : # a few things you can do within the run script - run : | kustomize build hello/world/folder > manifest.yaml kubectl apply -f manifest.yaml helm upgrade my-app helm/chart --install yq -i a=b manifest.yaml git clone https://github.com/iter8-tools/iter8.git gh pr create curl https://iter8.tools -O $SCARCH_DIR/i.html","title":"Available commands"},{"location":"reference/tasks/run/#inputs","text":"Field name Field type Description Required secret string Reference to a K8s secret in the namespace/name format. If no namespace is specified, then the namespace is assumed to be that of the experiment. No","title":"Inputs"},{"location":"reference/tasks/run/#result","text":"The script will be executed. If this script exits with a non-zero error code, the run task and therefore the experiment to which it belongs will fail.","title":"Result"},{"location":"tutorials/chaos/chaos/","text":"Chaos Testing with SLO Validation \u00b6 Scenario: Inject Chaos into Kubernetes cluster and verify if application can satisfy SLOs Problem: You have a Kubernetes app. You want to stress test it by injecting chaos, and verify that it can satisfy service-level objectives (SLOs). This helps you guarantee that your application is resilient, and works well even under periods of stress (like intermittent pod failures). Solution: You will launch a Kubernetes application along with a composite experiment, consisting of Litmus Chaos experiment resource and an Iter8 experiment resource. The chaos experiment will delete pods of the application periodically, while the Iter8 experiment will send requests to the application and verify if it is able to satisfy SLOs. Setup Kubernetes cluster and local environment If you completed the Iter8 getting-started tutorial (highly recommended), you may skip to step number 6. Setup Kubernetes cluster Install Iter8 in Kubernetes cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd ) Install Litmus in Kubernetes cluster . kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.13.8.yaml Verify that the Litmus is install correctly as described here . 1. Create app \u00b6 The hello app consists of a Kubernetes deployment and service. Deploy the app as follows. kubectl apply -n default -f $ITER8 /samples/deployments/app/deploy.yaml kubectl apply -n default -f $ITER8 /samples/deployments/app/service.yaml Use these instructions to verify that your app is running. 2. Create composite experiment \u00b6 helm upgrade -n default my-exp $ITER8 /samples/chaos \\ --set appns = 'default' \\ --set applabel = 'app.kubernetes.io/name=hello' \\ --set URL = 'http://hello.default.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --install 3. Observe Experiment \u00b6 View the chaos resources (chaos engine resource, pod deletion experiment, and RBAC resources), and Iter8 experiment resource. helm get manifest my-exp Verify that the phase of the chaos experiment is Completed . kubectl get chaosresults engine-hello-pod-delete -n default -ojsonpath = '{.status.experimentStatus.phase}' Verify that the chaos experiment returned a Pass verdict. The Pass verdict states that the application is still running after the chaos. kubectl get chaosresults engine-hello-pod-delete -n default -ojsonpath = '{.status.experimentStatus.verdict}' Ensure that the Iter8 experiment completed. iter8ctl assert -c completed Due to chaos injection, and the fact that the number of replicas of the app in the deployment manifest is set to 1, the SLOs are not expected to be satisfied during this experiment. Verify this is the case. # this assertion is expected to fail iter8ctl assert -c winnerFound Describe the Iter8 experiment. This will print the metrics collected during the experiment along with SLOs. iter8ctl describe 4. Scale app and retry \u00b6 Scale up the app so that replica count is increased to 2. The scaled app is now more resilient. Performing the same experiment as above will now result in SLOs being satisfied and a winner being found. kubectl scale --replicas = 2 -n default -f $ITER8 /samples/deployments/app/deploy.yaml Retry steps 2 and 3 above. You should now find that SLOs are satisfied and a winner is found at the end of the experiment. 5. Cleanup \u00b6 # remove chaos + Iter8 experiments helm uninstall -n default my-exp # remove app kubectl delete -n default -f $ITER8 /samples/deployments/app/service.yaml kubectl delete -n default -f $ITER8 /samples/deployments/app/deploy.yaml Next Steps Use with your own app, and try other types of Chaos You can easily replace the hello app used in this tutorial with your own application. a) Modify Step 1 to use your service and deployment. b) Modify Step 2 by supplying the correct namespace and label for your app, and also the correct URL where the app receives requests. Litmus makes it possible to inject over 51 types of Chaos . Modify the composite Helm chart to use any of these other types of chaos experiment. Iter8 makes it possible to promote the winning version in a number of different ways. For example, you may have a stable version running in production, a candidate version deployed in a staging environment, perform this experiment, ensure that the candidate is successful, and promote it as the latest stable version in a GitOps-y manner as described here .","title":"Chaos Testing"},{"location":"tutorials/chaos/chaos/#chaos-testing-with-slo-validation","text":"Scenario: Inject Chaos into Kubernetes cluster and verify if application can satisfy SLOs Problem: You have a Kubernetes app. You want to stress test it by injecting chaos, and verify that it can satisfy service-level objectives (SLOs). This helps you guarantee that your application is resilient, and works well even under periods of stress (like intermittent pod failures). Solution: You will launch a Kubernetes application along with a composite experiment, consisting of Litmus Chaos experiment resource and an Iter8 experiment resource. The chaos experiment will delete pods of the application periodically, while the Iter8 experiment will send requests to the application and verify if it is able to satisfy SLOs. Setup Kubernetes cluster and local environment If you completed the Iter8 getting-started tutorial (highly recommended), you may skip to step number 6. Setup Kubernetes cluster Install Iter8 in Kubernetes cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd ) Install Litmus in Kubernetes cluster . kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.13.8.yaml Verify that the Litmus is install correctly as described here .","title":"Chaos Testing with SLO Validation"},{"location":"tutorials/chaos/chaos/#1-create-app","text":"The hello app consists of a Kubernetes deployment and service. Deploy the app as follows. kubectl apply -n default -f $ITER8 /samples/deployments/app/deploy.yaml kubectl apply -n default -f $ITER8 /samples/deployments/app/service.yaml Use these instructions to verify that your app is running.","title":"1. Create app"},{"location":"tutorials/chaos/chaos/#2-create-composite-experiment","text":"helm upgrade -n default my-exp $ITER8 /samples/chaos \\ --set appns = 'default' \\ --set applabel = 'app.kubernetes.io/name=hello' \\ --set URL = 'http://hello.default.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --install","title":"2. Create composite experiment"},{"location":"tutorials/chaos/chaos/#3-observe-experiment","text":"View the chaos resources (chaos engine resource, pod deletion experiment, and RBAC resources), and Iter8 experiment resource. helm get manifest my-exp Verify that the phase of the chaos experiment is Completed . kubectl get chaosresults engine-hello-pod-delete -n default -ojsonpath = '{.status.experimentStatus.phase}' Verify that the chaos experiment returned a Pass verdict. The Pass verdict states that the application is still running after the chaos. kubectl get chaosresults engine-hello-pod-delete -n default -ojsonpath = '{.status.experimentStatus.verdict}' Ensure that the Iter8 experiment completed. iter8ctl assert -c completed Due to chaos injection, and the fact that the number of replicas of the app in the deployment manifest is set to 1, the SLOs are not expected to be satisfied during this experiment. Verify this is the case. # this assertion is expected to fail iter8ctl assert -c winnerFound Describe the Iter8 experiment. This will print the metrics collected during the experiment along with SLOs. iter8ctl describe","title":"3. Observe Experiment"},{"location":"tutorials/chaos/chaos/#4-scale-app-and-retry","text":"Scale up the app so that replica count is increased to 2. The scaled app is now more resilient. Performing the same experiment as above will now result in SLOs being satisfied and a winner being found. kubectl scale --replicas = 2 -n default -f $ITER8 /samples/deployments/app/deploy.yaml Retry steps 2 and 3 above. You should now find that SLOs are satisfied and a winner is found at the end of the experiment.","title":"4. Scale app and retry"},{"location":"tutorials/chaos/chaos/#5-cleanup","text":"# remove chaos + Iter8 experiments helm uninstall -n default my-exp # remove app kubectl delete -n default -f $ITER8 /samples/deployments/app/service.yaml kubectl delete -n default -f $ITER8 /samples/deployments/app/deploy.yaml Next Steps Use with your own app, and try other types of Chaos You can easily replace the hello app used in this tutorial with your own application. a) Modify Step 1 to use your service and deployment. b) Modify Step 2 by supplying the correct namespace and label for your app, and also the correct URL where the app receives requests. Litmus makes it possible to inject over 51 types of Chaos . Modify the composite Helm chart to use any of these other types of chaos experiment. Iter8 makes it possible to promote the winning version in a number of different ways. For example, you may have a stable version running in production, a candidate version deployed in a staging environment, perform this experiment, ensure that the candidate is successful, and promote it as the latest stable version in a GitOps-y manner as described here .","title":"5. Cleanup"},{"location":"tutorials/deployments/slo-validation-gitops/","text":"SLO Validation with GitOps \u00b6 Scenario: Validate SLOs and promote a new version of a K8s app Problem: You have a new version of a K8s app. You want to verify that it satisfies latency and error rate SLOs, and promote it to production as the stable version of your app in a GitOps-y manner. Solution: In this tutorial, you will dark launch the new version of your K8s app along with an Iter8 experiment. Iter8 will validate that the new satisfies latency and error-based objectives (SLOs) using built-in metrics and promote the new version by raising a pull-request in a GitHub repo . Setup Kubernetes cluster and local environment If you completed the Iter8 getting-started tutorial (highly recommended), you may skip the remaining steps of setup. Setup K8s cluster Install Iter8 in K8s cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd ) 1. Create stable version \u00b6 Create version 1.0 of the hello world app as follows. # USERNAME is exported as part of setup steps. kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/deploy.yaml kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/service.yaml 2. Create candidate version \u00b6 Create version 2.0 of the hello world app in the staging environment as follows. For the purpose of this tutorial, the production environment is the default namespace, and the staging environment is the staging namespace. kubectl create ns staging # create version 2.0 of hello world app in the staging namespace kubectl set image --local -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/deploy.yaml hello = 'gcr.io/google-samples/hello-app:2.0' -o yaml | kubectl apply -n staging -f - kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/service.yaml -n staging Adapt these instructions to verify that stable and candidate versions of your app are running. 3. Enable Iter8 GitOps \u00b6 3.1) Create a personal access token on GitHub . In Step 8 of this process, grant repo and read:org permissions to this token. This will ensure that the token can be used by Iter8 to update your app manifest in GitHub. 3.2) Create K8s secret # replace $GHTOKEN with GitHub token created above kubectl create secret generic -n staging ghtoken --from-literal = token = $GHTOKEN 3.3) Provide RBAC permission kubectl create role -n staging ghtoken-reader \\ --verb = get \\ --resource = secrets \\ --resource-name = ghtoken kubectl create rolebinding -n staging ghtoken-reader-binding \\ --role = ghtoken-reader \\ --serviceaccount = iter8-system:iter8-handlers 4. Create Iter8 experiment \u00b6 Deploy an Iter8 experiment for SLO validation and GitOps-y promotion of the app as follows. helm upgrade -n staging my-exp $ITER8 /samples/slo-gitops \\ --set URL = 'http://hello.staging.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --set username = $USERNAME \\ --set newImage = 'gcr.io/google-samples/hello-app:2.0' \\ --install The above command creates an Iter8 experiment that generates requests, collects latency and error rate metrics for the candidate version of the app, and verifies that the candidate satisfies mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. In the above command, the USERNAME environment variable was defined during setup. After the Iter8 experiment validates SLOs for the candidate, it uses the GitHub token (also provided during setup) to promote the candidate to production using a GitHub pull-request. 5. View and observe experiment \u00b6 View the Iter8 experiment as described here . Observe the experiment by following these steps . Ensure correct namespace ( staging ) is used. 6. Review Iter8's PR \u00b6 Once the experiment completes, you can visit your fork at https://github.com/$USERNAME/iter8/pulls to review the pull-request created by Iter8. 7. Cleanup \u00b6 # remove Iter8 experiment and candidate version of the app kubectl delete ns staging # remove stable version of the app kubectl delete deploy/hello kubectl delete svc/hello Next Steps Use with your git repo/app The Helm chart used in this tutorial is located at $ITER8/samples/slo-gitops. Within this folder, the file templates/experiment.yaml contains the Iter8 experiment template. The following lines in this template are responsible for cloning the git repo, modifying it locally, and pushing the changes. # clone repo using token git clone https:// $USERNAME : $TOKEN @github.com/ $USERNAME /iter8.git # commit changes locally and push to a branch cd iter8 git checkout -b iter8-gitops yq e -i '.spec.template.spec.containers[0].image = \"{{ required \".Values.newImage is required!\" .Values.newImage }}\"' samples/deployments/app/deploy.yaml git commit --allow-empty -a -m \"promote candidate version to production\" git push -f origin iter8-gitops Change these lines in the template so that the correct repo is cloned and your app is modified correctly. Ensure that correct GitHub access token is supplied to Iter8. You may need to supplement or replace the Helm values username and newImage used in this tutorial with other Helm values as needed by your repo/app.","title":"SLO Validation with GitOps"},{"location":"tutorials/deployments/slo-validation-gitops/#slo-validation-with-gitops","text":"Scenario: Validate SLOs and promote a new version of a K8s app Problem: You have a new version of a K8s app. You want to verify that it satisfies latency and error rate SLOs, and promote it to production as the stable version of your app in a GitOps-y manner. Solution: In this tutorial, you will dark launch the new version of your K8s app along with an Iter8 experiment. Iter8 will validate that the new satisfies latency and error-based objectives (SLOs) using built-in metrics and promote the new version by raising a pull-request in a GitHub repo . Setup Kubernetes cluster and local environment If you completed the Iter8 getting-started tutorial (highly recommended), you may skip the remaining steps of setup. Setup K8s cluster Install Iter8 in K8s cluster Get Helm 3.4+ . Get iter8ctl Fork the Iter8 GitHub repo . Clone your fork, and set the ITER8 environment variable as follows. export USERNAME = <your GitHub username> git clone git@github.com: $USERNAME /iter8.git cd iter8 export ITER8 = $( pwd )","title":"SLO Validation with GitOps"},{"location":"tutorials/deployments/slo-validation-gitops/#1-create-stable-version","text":"Create version 1.0 of the hello world app as follows. # USERNAME is exported as part of setup steps. kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/deploy.yaml kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/service.yaml","title":"1. Create stable version"},{"location":"tutorials/deployments/slo-validation-gitops/#2-create-candidate-version","text":"Create version 2.0 of the hello world app in the staging environment as follows. For the purpose of this tutorial, the production environment is the default namespace, and the staging environment is the staging namespace. kubectl create ns staging # create version 2.0 of hello world app in the staging namespace kubectl set image --local -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/deploy.yaml hello = 'gcr.io/google-samples/hello-app:2.0' -o yaml | kubectl apply -n staging -f - kubectl apply -f https://raw.githubusercontent.com/ $USERNAME /iter8/master/samples/deployments/app/service.yaml -n staging Adapt these instructions to verify that stable and candidate versions of your app are running.","title":"2. Create candidate version"},{"location":"tutorials/deployments/slo-validation-gitops/#3-enable-iter8-gitops","text":"3.1) Create a personal access token on GitHub . In Step 8 of this process, grant repo and read:org permissions to this token. This will ensure that the token can be used by Iter8 to update your app manifest in GitHub. 3.2) Create K8s secret # replace $GHTOKEN with GitHub token created above kubectl create secret generic -n staging ghtoken --from-literal = token = $GHTOKEN 3.3) Provide RBAC permission kubectl create role -n staging ghtoken-reader \\ --verb = get \\ --resource = secrets \\ --resource-name = ghtoken kubectl create rolebinding -n staging ghtoken-reader-binding \\ --role = ghtoken-reader \\ --serviceaccount = iter8-system:iter8-handlers","title":"3. Enable Iter8 GitOps"},{"location":"tutorials/deployments/slo-validation-gitops/#4-create-iter8-experiment","text":"Deploy an Iter8 experiment for SLO validation and GitOps-y promotion of the app as follows. helm upgrade -n staging my-exp $ITER8 /samples/slo-gitops \\ --set URL = 'http://hello.staging.svc.cluster.local:8080' \\ --set limitMeanLatency = 50 .0 \\ --set limitErrorRate = 0 .0 \\ --set limit95thPercentileLatency = 100 .0 \\ --set username = $USERNAME \\ --set newImage = 'gcr.io/google-samples/hello-app:2.0' \\ --install The above command creates an Iter8 experiment that generates requests, collects latency and error rate metrics for the candidate version of the app, and verifies that the candidate satisfies mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. In the above command, the USERNAME environment variable was defined during setup. After the Iter8 experiment validates SLOs for the candidate, it uses the GitHub token (also provided during setup) to promote the candidate to production using a GitHub pull-request.","title":"4. Create Iter8 experiment"},{"location":"tutorials/deployments/slo-validation-gitops/#5-view-and-observe-experiment","text":"View the Iter8 experiment as described here . Observe the experiment by following these steps . Ensure correct namespace ( staging ) is used.","title":"5. View and observe experiment"},{"location":"tutorials/deployments/slo-validation-gitops/#6-review-iter8s-pr","text":"Once the experiment completes, you can visit your fork at https://github.com/$USERNAME/iter8/pulls to review the pull-request created by Iter8.","title":"6. Review Iter8's PR"},{"location":"tutorials/deployments/slo-validation-gitops/#7-cleanup","text":"# remove Iter8 experiment and candidate version of the app kubectl delete ns staging # remove stable version of the app kubectl delete deploy/hello kubectl delete svc/hello Next Steps Use with your git repo/app The Helm chart used in this tutorial is located at $ITER8/samples/slo-gitops. Within this folder, the file templates/experiment.yaml contains the Iter8 experiment template. The following lines in this template are responsible for cloning the git repo, modifying it locally, and pushing the changes. # clone repo using token git clone https:// $USERNAME : $TOKEN @github.com/ $USERNAME /iter8.git # commit changes locally and push to a branch cd iter8 git checkout -b iter8-gitops yq e -i '.spec.template.spec.containers[0].image = \"{{ required \".Values.newImage is required!\" .Values.newImage }}\"' samples/deployments/app/deploy.yaml git commit --allow-empty -a -m \"promote candidate version to production\" git push -f origin iter8-gitops Change these lines in the template so that the correct repo is cloned and your app is modified correctly. Ensure that correct GitHub access token is supplied to Iter8. You may need to supplement or replace the Helm values username and newImage used in this tutorial with other Helm values as needed by your repo/app.","title":"7. Cleanup"},{"location":"tutorials/istio/platform-setup/","text":"Platform Setup for Istio \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Istio, Iter8 and Telemetry \u00b6 Setup Istio, Iter8, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/istio/platform-setup/#platform-setup-for-istio","text":"","title":"Platform Setup for Istio"},{"location":"tutorials/istio/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/istio/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/istio/platform-setup/#3-install-istio-iter8-and-telemetry","text":"Setup Istio, Iter8, and Prometheus add-on within your cluster. $ITER8 /samples/istio/quickstart/platformsetup.sh","title":"3. Install Istio, Iter8 and Telemetry"},{"location":"tutorials/istio/quick-start/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create application versions \u00b6 Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\" 2. Generate requests \u00b6 Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. For the purpose of this tutorial, you will mock the user-engagement metric. The latency and error metrics will be provided by Prometheus. kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : mock : - name : productpage-v1 level : 15.0 - name : productpage-v2 level : 20.0 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Iter8 defines a custom K8s resource called Experiment that automates a variety of release engineering and experimentation strategies for K8s applications and ML models. Launch the Hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"Quick start"},{"location":"tutorials/istio/quick-start/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/istio/quick-start/#1-create-application-versions","text":"Deploy the bookinfo microservice application including two versions of the productpage microservice. kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Look inside productpage-v2.yaml (v1 is similar) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v2 labels : app : productpage version : v2 spec : replicas : 1 selector : matchLabels : app : productpage version : v2 template : metadata : annotations : sidecar.istio.io/inject : \"true\" labels : app : productpage version : v2 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v2\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"green\" - name : reward_min value : \"10\" - name : reward_max value : \"20\" - name : port value : \"9080\"","title":"1. Create application versions"},{"location":"tutorials/istio/quick-start/#2-generate-requests","text":"Generate requests to your app using Fortio as follows. # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/istio/quick-start/#3-define-metrics","text":"Iter8 introduces a Kubernetes CRD called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. For the purpose of this tutorial, you will mock the user-engagement metric. The latency and error metrics will be provided by Prometheus. kubectl apply -f $ITER8 /samples/istio/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 apiVersion : v1 kind : Namespace metadata : labels : creator : iter8 stack : istio name : iter8-istio --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-istio spec : mock : - name : productpage-v1 level : 15.0 - name : productpage-v2 level : 20.0 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count namespace : iter8-istio spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate namespace : iter8-istio spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_requests_total{response_code=~'5..',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le500ms-latency-percentile namespace : iter8-istio spec : description : Less than 500 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_bucket{le='500',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_request_duration_milliseconds_bucket{le='+Inf',reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-istio/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency namespace : iter8-istio spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(istio_request_duration_milliseconds_sum{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : iter8-istio spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(istio_requests_total{reporter='source',destination_workload='$name',destination_workload_namespace='$namespace'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/istio/quick-start/#4-launch-experiment","text":"Iter8 defines a custom K8s resource called Experiment that automates a variety of release engineering and experimentation strategies for K8s applications and ML models. Launch the Hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/istio/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"4. Launch experiment"},{"location":"tutorials/istio/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/setup-for-tutorials/","text":"Setup For Istio Tutorials \u00b6 Clone iter8 repository \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) Install Istio \u00b6 For production installation of Istio, refer to the official Istio instructions . For exercising Iter8 tutorials, install Istio as follows. If not already cloned, clone the iter8 repositiory . $ITER8 /samples/istio/quickstart/istio-setup.sh Install Optional Prometheus Add-On \u00b6 The Iter8 Prometheus add-on is suitable only for tutorials. To install Prometheus for production, see the official Prometheus documentation . To install the add-on: export TAG = v0.7.21 kustomize build https://github.com/iter8-tools/iter8/install/prometheus-add-on/prometheus-operator/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/prometheus-add-on/prometheus/?ref = ${ TAG } | kubectl apply -f - kubectl apply -f ${ ITER8 } /samples/istio/quickstart/service-monitor.yaml Install Argo CD \u00b6 If not already cloned, clone the iter8 repositiory . $ITER8 /samples/istio/gitops/argocd-setup.sh The output from the install script will provide instructions on how to access the Argo CD UI to setup your Argo CD app. Take those steps now. After logging in, you should see Argo CD showing no application is currently installed. Create GitHub Token \u00b6 Login to GitHub . From the upper right corner of the page, go to Settings > Developer settings > Personal access token > Generate new token. Make sure the token is granted access for repo and workflow . Save the generated token to a Kubernetes secret as follows: kubectl create secret generic iter8-token --from-literal = token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Finally, give Iter8 permission to read the secret: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml","title":"Setup For Istio Tutorials"},{"location":"tutorials/istio/setup-for-tutorials/#setup-for-istio-tutorials","text":"","title":"Setup For Istio Tutorials"},{"location":"tutorials/istio/setup-for-tutorials/#clone-iter8-repository","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"Clone iter8 repository"},{"location":"tutorials/istio/setup-for-tutorials/#install-istio","text":"For production installation of Istio, refer to the official Istio instructions . For exercising Iter8 tutorials, install Istio as follows. If not already cloned, clone the iter8 repositiory . $ITER8 /samples/istio/quickstart/istio-setup.sh","title":"Install Istio"},{"location":"tutorials/istio/setup-for-tutorials/#install-optional-prometheus-add-on","text":"The Iter8 Prometheus add-on is suitable only for tutorials. To install Prometheus for production, see the official Prometheus documentation . To install the add-on: export TAG = v0.7.21 kustomize build https://github.com/iter8-tools/iter8/install/prometheus-add-on/prometheus-operator/?ref = ${ TAG } | kubectl apply -f - kubectl wait crd -l creator = iter8 --for condition = established --timeout = 120s kustomize build https://github.com/iter8-tools/iter8/install/prometheus-add-on/prometheus/?ref = ${ TAG } | kubectl apply -f - kubectl apply -f ${ ITER8 } /samples/istio/quickstart/service-monitor.yaml","title":"Install Optional Prometheus Add-On"},{"location":"tutorials/istio/setup-for-tutorials/#install-argo-cd","text":"If not already cloned, clone the iter8 repositiory . $ITER8 /samples/istio/gitops/argocd-setup.sh The output from the install script will provide instructions on how to access the Argo CD UI to setup your Argo CD app. Take those steps now. After logging in, you should see Argo CD showing no application is currently installed.","title":"Install Argo CD"},{"location":"tutorials/istio/setup-for-tutorials/#create-github-token","text":"Login to GitHub . From the upper right corner of the page, go to Settings > Developer settings > Personal access token > Generate new token. Make sure the token is granted access for repo and workflow . Save the generated token to a Kubernetes secret as follows: kubectl create secret generic iter8-token --from-literal = token = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Finally, give Iter8 permission to read the secret: kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/tasks/rbac/read-secrets.yaml","title":"Create GitHub Token"},{"location":"tutorials/istio/gitops/argocd/","text":"GitOps with Argo CD \u00b6 Scenario: Iter8 Experiment + Gitops GitOps is an approach increasingly adopted to simplify cluster management tasks. In this approach, the desired state of one or more clusters is kept in a Git environment repository. A CD tool, such as Argo CD, continuously monitors this repository for changes and synchronizes any detected changes to the target clusters. In the wider context, commits to the code repository trigger a CI pipeline to, for example, lint, build, test, and push newly built images to an image repository. It then writes configuration changes to the environment repository. The CD pipeline or tool then detects these changes and synchronizes them to the target clusters. Iter8 can be used in the context of GitOps so that new versions of an application can be progressively rolled out, or even rolled back when problems are detected. To do this, configuration to create an experiment is implemented in the CI pipeline. This tutorial assumes a basic understanding of Iter8. See, for example, the Istio quick start tutorial . Setup the environment repository In this tutorial, a fork of the iter8 repository is used as the environment repository. To make changes to it, you will need your own fork of the repository. For the purpose of this tutorial, we assume that your fork is at: https://github.com/[YOUR_ORG]/iter8 Clone the forked repository and modify it to replace the generic MY_ORG with YOUR_ORG . Clone the forked repository : git clone git@github.com:kalantar/iter8.git cd iter8 export ITER8 = $( pwd ) Modify the clone : MacOS export YOUR_ORG = fill-in find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i '' \"s/MY_ORG/ $YOUR_ORG /\" git commit -a -m \"update references\" git push origin head Linux export YOUR_ORG = fill-in find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i \"s/MY_ORG/ $YOUR_ORG /\" git commit -a -m \"update references\" git push origin head Setup a K8s cluster with Iter8, Istio and Argo CD Setup K8s cluster Install Iter8 in K8s cluster Get iter8ctl Install Istio in K8s cluster Install Prometheus add-on Install Argo CD in k8s cluster Setup a GitHub token and give Iter8 permission to use it. 1. Create Baseline Version \u00b6 Install the bookinfo application by creating an Argo CD application as follows: kubectl apply -f $ITER8 /samples/istio/gitops/argocd-app.yaml The GitOps application defined by argocd-app.yaml identifies a part of your repository, https://github.com/[YOUR_ORG]/iter8 , to be the environment repository. Argo CD will immediately begin to synchronize the configuration (bookinfo application config) found there. You can monitor the progress of the syncrhonization through the Argo CD UI. When the state is both Healthy and Synced , it is ready; this might take a few minutes. 2. Create Candidate Version \u00b6 When changes are merged into a code repository, a CI pipeline to, for example, lint, build, test, and push newly built images to an image repository runs. It then writes configuration changes to the environment repository indicating changes are needed to the deployed application. In this tutorial, we simulate the execution of a CI pipeline, by executing a simplified GitHub workflow: https://github.com/[YOUR_ORG]/iter8/actions/workflows/gitops-ci.yaml Navigate to the workflow and click the button \"Run workflow\" More about GitHub workflow gitops-ci.yaml The \"Simulate CI pipeline\" creates the configuration changes that a typical CI pipeline would create when changes are made. In particular, it: Creates a new application configiration (by modifying a color property in a random way). Creates an Iter8 experiment to evaluate the new version. Configures load generation. These changes are pushed to the environment repository triggering Argo CD to deploy them. You can use the Argo CD UI to monitor progress of the deployment. By default, Argo CD is configured to run every three minutes. If you don't want to wait, manually refresh the application so that the changes are immediately synced to the cluster. 3. Observe experiment \u00b6 The experiment should run for a few minutes once it starts, and you can track its progress with this command: kubectl get experiments.iter8.tools --watch More detail is available using the Iter8 CLI: iter8ctl describe 4. Promote winner \u00b6 Once the experiment finishes, review the pull requests on the environment repository, https://github.com/[YOUR_ORG]/iter8/pulls . Iter8 will have created a new pull request titled Deploy version recommended by Iter8 . Changes to productpage.yaml capture the recommended version changes. Other changes remove artifacts specific to the experiment including the candidate version, the experiment and any load generation. Merge the pull request to complete the experiment. Argo CD will detect the changes and synchronize the cluster to the new desired state. How Iter8 creates a pull request Iter8 creates a pull request at the end of the experiment by running a GitHub workflow. Iter8 uses a [notification/http] task(../../../../reference/tasks/notification-http) to execute a GitHub workflow at the end of the experiment. This workflow creates the needed pull request. 5. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/gitops/ kubectl delete ns istio-system kubectl delete ns iter8-system kubectl delete ns argocd Additional details \u00b6 Environment repository \u00b6 It is generally considered good practice to use different Git repositories for code and for environment configuration. In cases where the same repository is being used for both, one needs to be careful when configuring CI/CD pipeline tools so code changes can be differentiated from configuration changes, so that one doesn't inadvertently create infinite loops. The environment repository can be organized in many different ways. With tools such as Helm and Kustomize becoming widely used, it becomes even simpler for CI pipeline tools to update an environment repository to roll out new app versions. In this tutorial, we consciously decided to use the simplest directory structure (i.e., all YAML files within a single directory without subdirectories) without the use of any higher level templating tools. Adapting the basic directory structure to Helm/Kustomize is straight forward. When organizing the directory structure, one needs to keep in mind that the CI pipeline tool will be creating new resources in the environment repository to start an Iter8 experiment. And when the experiment finishes, Iter8 (specifically, Iter8 tasks) will delete the added resources and update the baseline version in the environment repository. In other words, the invariant here is the directory structure, which should stay the same before and after an experiment. GitOps support for multiple environments \u00b6 Some users might use GitOps to manage multiple environments, e.g., dev, staging, prod, so changes can always propagate from environment to environment, minimizing the chance of defects from reaching the prod environment. In this case, the Iter8 GitOps task would need to be modified so that environment repository changes are done at the correct places. For example, if different environments are managed by different environment repositories, the task would need to make multiple git commits, one for each of the respositories. This could be done all within a single task, or across multiple tasks. GitOps Guarantees \u00b6 Unlike other progressive delivery tools, Iter8 adheres to GitOps' guarantees by ensuring the actual state is always in sync with the desired state. App versions that fail promotion criteria will never get promoted, even if the cluster has to be recreated from scratch. This important GitOps property is often not guaranteed by other tools! Caveats \u00b6 Both CI pipeline tools and Iter8 need to write to the environment repository. If not coordinated, race conditions can occur. In this tutorial, we assume repo changes are done via pull requests, which is a common practice, so the chance of having a race condition is minimized, if not eliminated. However, other means to coordinate writes to the environment repository by different entities can be done so Iter8 can operate in fully automated pipelines. When a new app version becomes available while an experiment is still running, Iter8 will preempt the existing experiment with the new one. We currently don't support test-every-commit behavior by queuing new experiments, but this could be supported in the future if it turrned out to be more common than we are currently expecting. Iter8 task could fail, just like everything else. Iter8 tasks are currently fail-stop without retries. Please take this into account when writing Iter8 tasks and error handling code.","title":"Argo CD + Istio"},{"location":"tutorials/istio/gitops/argocd/#gitops-with-argo-cd","text":"Scenario: Iter8 Experiment + Gitops GitOps is an approach increasingly adopted to simplify cluster management tasks. In this approach, the desired state of one or more clusters is kept in a Git environment repository. A CD tool, such as Argo CD, continuously monitors this repository for changes and synchronizes any detected changes to the target clusters. In the wider context, commits to the code repository trigger a CI pipeline to, for example, lint, build, test, and push newly built images to an image repository. It then writes configuration changes to the environment repository. The CD pipeline or tool then detects these changes and synchronizes them to the target clusters. Iter8 can be used in the context of GitOps so that new versions of an application can be progressively rolled out, or even rolled back when problems are detected. To do this, configuration to create an experiment is implemented in the CI pipeline. This tutorial assumes a basic understanding of Iter8. See, for example, the Istio quick start tutorial . Setup the environment repository In this tutorial, a fork of the iter8 repository is used as the environment repository. To make changes to it, you will need your own fork of the repository. For the purpose of this tutorial, we assume that your fork is at: https://github.com/[YOUR_ORG]/iter8 Clone the forked repository and modify it to replace the generic MY_ORG with YOUR_ORG . Clone the forked repository : git clone git@github.com:kalantar/iter8.git cd iter8 export ITER8 = $( pwd ) Modify the clone : MacOS export YOUR_ORG = fill-in find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i '' \"s/MY_ORG/ $YOUR_ORG /\" git commit -a -m \"update references\" git push origin head Linux export YOUR_ORG = fill-in find $ITER8 /samples/istio/gitops -name \"*\" -type f | xargs sed -i \"s/MY_ORG/ $YOUR_ORG /\" git commit -a -m \"update references\" git push origin head Setup a K8s cluster with Iter8, Istio and Argo CD Setup K8s cluster Install Iter8 in K8s cluster Get iter8ctl Install Istio in K8s cluster Install Prometheus add-on Install Argo CD in k8s cluster Setup a GitHub token and give Iter8 permission to use it.","title":"GitOps with Argo CD"},{"location":"tutorials/istio/gitops/argocd/#1-create-baseline-version","text":"Install the bookinfo application by creating an Argo CD application as follows: kubectl apply -f $ITER8 /samples/istio/gitops/argocd-app.yaml The GitOps application defined by argocd-app.yaml identifies a part of your repository, https://github.com/[YOUR_ORG]/iter8 , to be the environment repository. Argo CD will immediately begin to synchronize the configuration (bookinfo application config) found there. You can monitor the progress of the syncrhonization through the Argo CD UI. When the state is both Healthy and Synced , it is ready; this might take a few minutes.","title":"1. Create Baseline Version"},{"location":"tutorials/istio/gitops/argocd/#2-create-candidate-version","text":"When changes are merged into a code repository, a CI pipeline to, for example, lint, build, test, and push newly built images to an image repository runs. It then writes configuration changes to the environment repository indicating changes are needed to the deployed application. In this tutorial, we simulate the execution of a CI pipeline, by executing a simplified GitHub workflow: https://github.com/[YOUR_ORG]/iter8/actions/workflows/gitops-ci.yaml Navigate to the workflow and click the button \"Run workflow\" More about GitHub workflow gitops-ci.yaml The \"Simulate CI pipeline\" creates the configuration changes that a typical CI pipeline would create when changes are made. In particular, it: Creates a new application configiration (by modifying a color property in a random way). Creates an Iter8 experiment to evaluate the new version. Configures load generation. These changes are pushed to the environment repository triggering Argo CD to deploy them. You can use the Argo CD UI to monitor progress of the deployment. By default, Argo CD is configured to run every three minutes. If you don't want to wait, manually refresh the application so that the changes are immediately synced to the cluster.","title":"2. Create Candidate Version"},{"location":"tutorials/istio/gitops/argocd/#3-observe-experiment","text":"The experiment should run for a few minutes once it starts, and you can track its progress with this command: kubectl get experiments.iter8.tools --watch More detail is available using the Iter8 CLI: iter8ctl describe","title":"3. Observe experiment"},{"location":"tutorials/istio/gitops/argocd/#4-promote-winner","text":"Once the experiment finishes, review the pull requests on the environment repository, https://github.com/[YOUR_ORG]/iter8/pulls . Iter8 will have created a new pull request titled Deploy version recommended by Iter8 . Changes to productpage.yaml capture the recommended version changes. Other changes remove artifacts specific to the experiment including the candidate version, the experiment and any load generation. Merge the pull request to complete the experiment. Argo CD will detect the changes and synchronize the cluster to the new desired state. How Iter8 creates a pull request Iter8 creates a pull request at the end of the experiment by running a GitHub workflow. Iter8 uses a [notification/http] task(../../../../reference/tasks/notification-http) to execute a GitHub workflow at the end of the experiment. This workflow creates the needed pull request.","title":"4. Promote winner"},{"location":"tutorials/istio/gitops/argocd/#5-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/gitops/ kubectl delete ns istio-system kubectl delete ns iter8-system kubectl delete ns argocd","title":"5. Cleanup"},{"location":"tutorials/istio/gitops/argocd/#additional-details","text":"","title":"Additional details"},{"location":"tutorials/istio/gitops/argocd/#environment-repository","text":"It is generally considered good practice to use different Git repositories for code and for environment configuration. In cases where the same repository is being used for both, one needs to be careful when configuring CI/CD pipeline tools so code changes can be differentiated from configuration changes, so that one doesn't inadvertently create infinite loops. The environment repository can be organized in many different ways. With tools such as Helm and Kustomize becoming widely used, it becomes even simpler for CI pipeline tools to update an environment repository to roll out new app versions. In this tutorial, we consciously decided to use the simplest directory structure (i.e., all YAML files within a single directory without subdirectories) without the use of any higher level templating tools. Adapting the basic directory structure to Helm/Kustomize is straight forward. When organizing the directory structure, one needs to keep in mind that the CI pipeline tool will be creating new resources in the environment repository to start an Iter8 experiment. And when the experiment finishes, Iter8 (specifically, Iter8 tasks) will delete the added resources and update the baseline version in the environment repository. In other words, the invariant here is the directory structure, which should stay the same before and after an experiment.","title":"Environment repository"},{"location":"tutorials/istio/gitops/argocd/#gitops-support-for-multiple-environments","text":"Some users might use GitOps to manage multiple environments, e.g., dev, staging, prod, so changes can always propagate from environment to environment, minimizing the chance of defects from reaching the prod environment. In this case, the Iter8 GitOps task would need to be modified so that environment repository changes are done at the correct places. For example, if different environments are managed by different environment repositories, the task would need to make multiple git commits, one for each of the respositories. This could be done all within a single task, or across multiple tasks.","title":"GitOps support for multiple environments"},{"location":"tutorials/istio/gitops/argocd/#gitops-guarantees","text":"Unlike other progressive delivery tools, Iter8 adheres to GitOps' guarantees by ensuring the actual state is always in sync with the desired state. App versions that fail promotion criteria will never get promoted, even if the cluster has to be recreated from scratch. This important GitOps property is often not guaranteed by other tools!","title":"GitOps Guarantees"},{"location":"tutorials/istio/gitops/argocd/#caveats","text":"Both CI pipeline tools and Iter8 need to write to the environment repository. If not coordinated, race conditions can occur. In this tutorial, we assume repo changes are done via pull requests, which is a common practice, so the chance of having a race condition is minimized, if not eliminated. However, other means to coordinate writes to the environment repository by different entities can be done so Iter8 can operate in fully automated pipelines. When a new app version becomes available while an experiment is still running, Iter8 will preempt the existing experiment with the new one. We currently don't support test-every-commit behavior by queuing new experiments, but this could be supported in the future if it turrned out to be more common than we are currently expecting. Iter8 task could fail, just like everything else. Iter8 tasks are currently fail-stop without retries. Please take this into account when writing Iter8 tasks and error handling code.","title":"Caveats"},{"location":"tutorials/istio/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create versions and fix traffic split \u00b6 kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40 2. Steps 2 and 3 \u00b6 Please follow Steps 2 and 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8","title":"Fixed-%-split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#1-create-versions-and-fix-traffic-split","text":"kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/fixed-split/bookinfo-app.yaml kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/quickstart/productpage-v2.yaml kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : bookinfo spec : gateways : - mesh - bookinfo-gateway hosts : - productpage - \"bookinfo.example.com\" http : - match : - uri : exact : /productpage - uri : prefix : /static - uri : exact : /login - uri : exact : /logout - uri : prefix : /api/v1/products route : - destination : host : productpage port : number : 9080 subset : productpage-v1 weight : 60 - destination : host : productpage port : number : 9080 subset : productpage-v2 weight : 40","title":"1. Create versions and fix traffic split"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#2-steps-2-and-3","text":"Please follow Steps 2 and 3 of the quick start tutorial .","title":"2. Steps 2 and 3"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/istio/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will not shift traffic during iterations deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : iter8-istio/user-engagement preferredDirection : High objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 candidates : - name : productpage-v2 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8","title":"4. Launch experiment"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/rollout-strategies/fixed-split/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete ns bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The hybrid testing (quick start) and the SLO validation tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing quick start tutorial for Istio uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this virtual service and the traffic fields within the virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The hybrid testing (quick start) and the SLO validation tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/istio/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing quick start tutorial for Istio uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this virtual service and the traffic fields within the virtual service corresponding to the versions. versionInfo : baseline : name : productpage-v1 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/istio/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/istio/testing-strategies/conformance/","text":"SLO Validation with a single version \u00b6 Scenario: SLO validation with a single version Iter8 enables you to perform SLO validation with a single version of your application (a.k.a. conformance testing ). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. 1. Create application version \u00b6 Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\" 2. Generate requests \u00b6 Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Please follow step 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8","title":"SLO validation (single version)"},{"location":"tutorials/istio/testing-strategies/conformance/#slo-validation-with-a-single-version","text":"Scenario: SLO validation with a single version Iter8 enables you to perform SLO validation with a single version of your application (a.k.a. conformance testing ). In this tutorial, you will: Perform conformance testing. Specify latency and error-rate based service-level objectives (SLOs). If your version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"SLO Validation with a single version"},{"location":"tutorials/istio/testing-strategies/conformance/#1-create-application-version","text":"Deploy bookinfo app: kubectl apply -n bookinfo-iter8 -f $ITER8 /samples/istio/conformance/bookinfo-app.yaml Look inside productpage-v1 defined in bookinfo-app.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 apiVersion : apps/v1 kind : Deployment metadata : name : productpage-v1 labels : app : productpage version : v1 spec : replicas : 1 selector : matchLabels : app : productpage version : v1 template : metadata : annotations : sidecar.istio.io/inject : \"true\" prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"9080\" labels : app : productpage version : v1 spec : serviceAccountName : bookinfo-productpage containers : - name : productpage image : iter8/productpage:demo imagePullPolicy : IfNotPresent ports : - containerPort : 9080 env : - name : deployment value : \"productpage-v1\" - name : namespace valueFrom : fieldRef : fieldPath : metadata.namespace - name : color value : \"red\" - name : reward_min value : \"0\" - name : reward_max value : \"5\" - name : port value : \"9080\"","title":"1. Create application version"},{"location":"tutorials/istio/testing-strategies/conformance/#2-generate-requests","text":"Generate requests using Fortio as follows. kubectl wait -n bookinfo-iter8 --for = condition = Ready pods --all # URL_VALUE is the URL of the `bookinfo` application URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80/productpage\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/istio/quickstart/fortio.yaml | kubectl apply -f - Look inside fortio.yaml apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"16\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: bookinfo.example.com' , \"$(URL)\" ] env : - name : URL value : URL_VALUE volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/istio/testing-strategies/conformance/#3-define-metrics","text":"Please follow step 3 of the quick start tutorial .","title":"3. Define metrics"},{"location":"tutorials/istio/testing-strategies/conformance/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/istio/conformance/experiment.yaml Look inside experiment.yaml apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : conformance-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Conformance test testingPattern : Conformance criteria : objectives : # used for validating versions - metric : iter8-istio/mean-latency upperLimit : 100 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used by final action if this version is the winner value : bookinfo-iter8","title":"4. Launch experiment"},{"location":"tutorials/istio/testing-strategies/conformance/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/testing-strategies/conformance/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/conformance/fortio.yaml kubectl delete -f $ITER8 /samples/istio/conformance/experiment.yaml kubectl delete ns bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/istio/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) Testing \u00b6 Hybrid (A/B + SLOs) testing is documented as part of the Istio quick start tutorial .","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/istio/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Hybrid (A/B + SLOs) testing is documented as part of the Istio quick start tutorial .","title":"Hybrid (A/B + SLOs) Testing"},{"location":"tutorials/istio/testing-strategies/slovalidation/","text":"SLO Validation \u00b6 Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster. Steps 1 to 3 \u00b6 Follow Steps 1 to 3 of the Iter8 quick start tutorial . 4. Launch experiment \u00b6 Launch the SLO validation experiment. kubectl apply -f $ITER8 /samples/istio/slovalidation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : slovalidation-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/slovalidation/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"SLO validation"},{"location":"tutorials/istio/testing-strategies/slovalidation/#slo-validation","text":"Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Prometheus as the provider for latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Istio in your K8s cluster.","title":"SLO Validation"},{"location":"tutorials/istio/testing-strategies/slovalidation/#steps-1-to-3","text":"Follow Steps 1 to 3 of the Iter8 quick start tutorial .","title":"Steps 1 to 3"},{"location":"tutorials/istio/testing-strategies/slovalidation/#4-launch-experiment","text":"Launch the SLO validation experiment. kubectl apply -f $ITER8 /samples/istio/slovalidation/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : slovalidation-exp spec : # target identifies the service under experimentation using its fully qualified name target : bookinfo-iter8/productpage strategy : # this experiment will perform a Canary test testingPattern : Canary # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n bookinfo-iter8 apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/istio/quickstart/vs-for-v1.yaml criteria : objectives : # metrics used to validate versions - metric : iter8-istio/mean-latency upperLimit : 300 - metric : iter8-istio/error-rate upperLimit : \"0.01\" requestCount : iter8-istio/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : productpage-v1 variables : - name : namespace # used in metric queries value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[0].weight candidates : - name : productpage-v2 variables : - name : namespace # used in metric queries value : bookinfo-iter8 weightObjRef : apiVersion : networking.istio.io/v1beta1 kind : VirtualService namespace : bookinfo-iter8 name : bookinfo fieldPath : .spec.http[0].route[1].weight","title":"4. Launch experiment"},{"location":"tutorials/istio/testing-strategies/slovalidation/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/istio/testing-strategies/slovalidation/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/istio/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/istio/slovalidation/experiment.yaml kubectl delete namespace bookinfo-iter8","title":"6. Cleanup"},{"location":"tutorials/kfserving/platform-setup/","text":"Platform Setup for KFServing \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install KFServing, Iter8 and Telemetry \u00b6 Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/kfserving/platform-setup/#platform-setup-for-kfserving","text":"","title":"Platform Setup for KFServing"},{"location":"tutorials/kfserving/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/kfserving/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/kfserving/platform-setup/#3-install-kfserving-iter8-and-telemetry","text":"Setup KFServing, Iter8, a mock New Relic service, and Prometheus add-on within your cluster. $ITER8 /samples/kfserving/quickstart/platformsetup.sh","title":"3. Install KFServing, Iter8 and Telemetry"},{"location":"tutorials/kfserving/quick-start/","text":"A/B Testing \u00b6 Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0 2. Generate requests \u00b6 Generate requests for your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json watch --interval 0 .2 -x curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock the user-engagement metric as follows. kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize in the A/B test. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"Quick start"},{"location":"tutorials/kfserving/quick-start/#ab-testing","text":"Scenario: A/B testing and progressive traffic shift for KFServing models A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"A/B Testing"},{"location":"tutorials/kfserving/quick-start/#1-create-ml-model-versions","text":"Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-baseline spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : serving.kubeflow.org/v1beta1 kind : InferenceService metadata : name : flowers namespace : ns-candidate spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 100 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 0","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/quick-start/#2-generate-requests","text":"Generate requests for your model as follows. Port forward Istio ingress in terminal one INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Send requests in terminal two curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json watch --interval 0 .2 -x curl -v -H \"Host: example.com\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json","title":"2. Generate requests"},{"location":"tutorials/kfserving/quick-start/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock the user-engagement metric as follows. kubectl apply -f $ITER8 /samples/kfserving/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked metric used in this tutorial with any custom metric you wish to optimize in the A/B test. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/kfserving/quick-start/#4-launch-experiment","text":"Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/kfserving/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate","title":"4. Launch experiment"},{"location":"tutorials/kfserving/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40 2. Steps 2 and 3 \u00b6 Please follow Steps 2 and 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"Fixed-%-split"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#1-create-ml-model-versions","text":"kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Virtual service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - route : - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 weight : 60 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 weight : 40","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#2-steps-2-and-3","text":"Please follow Steps 2 and 3 of the quick start tutorial .","title":"2. Steps 2 and 3"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate","title":"4. Launch experiment"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/rollout-strategies/fixed-split/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/fixed-split/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/fixed-split/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The A/B testing (quick start) and hybrid (A/B + SLOs) testing tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The A/B testing quick start tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The A/B testing (quick start) and hybrid (A/B + SLOs) testing tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The A/B testing quick start tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/kfserving/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/","text":"Session Affinity \u00b6 Scenario: Canary rollout with session affinity Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use an experiment involving two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The experiment is shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Istio virtual service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1 2. Generate requests \u00b6 Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done 3. Define metrics \u00b6 Please follow Step 3 of the quick start tutorial . 4. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"Session affinity"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#session-affinity","text":"Scenario: Canary rollout with session affinity Session affinity ensures that the version to which a particular user's request is routed remains consistent throughout the duration of the experiment. In this tutorial, you will use an experiment involving two user groups, 1 and 2. Reqeusts from user group 1 will have a userhash header value prefixed with 111 and will be routed to the baseline version. Requests from user group 2 will have a userhash header value prefixed with 101 and will be routed to the candidate version. The experiment is shown below. Platform setup Follow these steps to install Iter8, KFServing and Prometheus in your K8s cluster.","title":"Session Affinity"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#1-create-ml-model-versions","text":"Deploy two KFServing inference services corresponding to two versions of a TensorFlow classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl wait --for = condition = Ready isvc/flowers -n ns-baseline kubectl wait --for = condition = Ready isvc/flowers -n ns-candidate Istio virtual service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - knative-serving/knative-ingress-gateway hosts : - example.com http : - match : - headers : userhash : # user hash is a 10-digit random binary string prefix : \"101\" # in expectation, 1/8th of user hashes will match this prefix route : # matching users will always go to v2 - destination : host : flowers-predictor-default.ns-candidate.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-candidate response : set : version : flowers-v2 - route : # non-matching users will always go to v1 - destination : host : flowers-predictor-default.ns-baseline.svc.cluster.local headers : request : set : Host : flowers-predictor-default.ns-baseline response : set : version : flowers-v1","title":"1. Create ML model versions"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#2-generate-requests","text":"Generate requests to your model as follows. Port forward (terminal one) INGRESS_GATEWAY_SERVICE = $( kubectl get svc -n istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward -n istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 Baseline requests (terminal two) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1111100000\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 0 .29 done Candidate requests (terminal three) curl -o /tmp/input.json https://raw.githubusercontent.com/kubeflow/kfserving/master/docs/samples/v1beta1/rollout/input.json while true ; do curl -v -H \"Host: example.com\" -H \"userhash: 1010101010\" localhost:8080/v1/models/flowers:predict -d @/tmp/input.json sleep 2 .0 done","title":"2. Generate requests"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#3-define-metrics","text":"Please follow Step 3 of the quick start tutorial .","title":"3. Define metrics"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#4-launch-experiment","text":"kubectl apply -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : session-affinity-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : FixedSplit actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 variables : - name : ns value : ns-candidate","title":"4. Launch experiment"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/kfserving/rollout-strategies/session-affinity/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/session-affinity/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/session-affinity/routing-rule.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml","title":"6. Cleanup"},{"location":"tutorials/kfserving/testing-strategies/ab/","text":"A/B Testing \u00b6 The quick start tutorial for KFServing demonstrates A/B testing.","title":"A/B testing (quick start)"},{"location":"tutorials/kfserving/testing-strategies/ab/#ab-testing","text":"The quick start tutorial for KFServing demonstrates A/B testing.","title":"A/B Testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of KFServing models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives, for which data will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. 1. Steps 1, 2, and 3 \u00b6 Follow Steps 1, 2, and 3 of the KFServing quick start tutorial . 4. Define metrics \u00b6 kubectl apply -f $ITER8 /samples/kfserving/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query 5. Launch experiment \u00b6 Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/kfserving/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate 6. Observe experiment \u00b6 Follow these steps to observe your experiment. 7. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/kfserving/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of KFServing models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives, for which data will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#1-steps-1-2-and-3","text":"Follow Steps 1, 2, and 3 of the KFServing quick start tutorial .","title":"1. Steps 1, 2, and 3"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#4-define-metrics","text":"kubectl apply -f $ITER8 /samples/kfserving/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 apiVersion : v1 kind : Namespace metadata : name : iter8-kfserving --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-kfserving spec : mock : - name : flowers-v1 level : \"15.0\" - name : flowers-v2 level : \"20.0\" --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-kfserving spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(revision_app_request_latencies_bucket{namespace_name='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-kfserving spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-kfserving spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_count{response_code_class!='2xx',namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-kfserving spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(revision_app_request_latencies_sum{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-kfserving/request-count type : Gauge units : milliseconds urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-kfserving spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(revision_app_request_latencies_count{namespace_name='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus-operated.iter8-system:9090/api/v1/query","title":"4. Define metrics"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#5-launch-experiment","text":"Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. kubectl apply -f $ITER8 /samples/kfserving/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : flowers strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/kfserving/quickstart/promote-v1.yaml\" criteria : rewards : # Business rewards - metric : iter8-kfserving/user-engagement preferredDirection : High # maximize user engagement duration : intervalSeconds : 5 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : flowers-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline candidates : - name : flowers-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate","title":"5. Launch experiment"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#6-observe-experiment","text":"Follow these steps to observe your experiment.","title":"6. Observe experiment"},{"location":"tutorials/kfserving/testing-strategies/hybrid/#7-cleanup","text":"kubectl delete -f $ITER8 /samples/kfserving/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/kfserving/quickstart/candidate.yaml","title":"7. Cleanup"},{"location":"tutorials/knative/setup-for-tutorials/","text":"Setup For Tutorials \u00b6 For production installation of Knative, refer to the official Knative instructions . Iter8 can work with any Knative networking layer. For simplicity, we recommend Kourier as the Knative networking layer for Iter8 tutorials. You can install Knative-serving in your cluster with Kourier networking as follows. git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) $ITER8 /samples/knative/quickstart/platform-setup.sh","title":"Setup for tutorials"},{"location":"tutorials/knative/setup-for-tutorials/#setup-for-tutorials","text":"For production installation of Knative, refer to the official Knative instructions . Iter8 can work with any Knative networking layer. For simplicity, we recommend Kourier as the Knative networking layer for Iter8 tutorials. You can install Knative-serving in your cluster with Kourier networking as follows. git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) $ITER8 /samples/knative/quickstart/platform-setup.sh","title":"Setup For Tutorials"},{"location":"tutorials/knative/slovalidation-helmex/","text":"SLO validation (Helmex) \u00b6 Scenario: Safely rollout new version of a Knative app with SLO validation Dark launch a candidate version of your Knative application, validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. This tutorial illustrates the Helmex pattern . Setup K8s cluster with Knative and local environment Get Helm 3.4+ Setup K8s cluster . Install Knative in K8s cluster . This tutorial can be tried with any Knative networking layer. Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo 1. Create baseline version \u00b6 Deploy the baseline version of the hello world application using Helm. helm install my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate = null View K8s resources created by Helm Use the command below to view the K8s resources created by Helm during the release of my-app . helm get manifest my-app Verify that baseline version is 1.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready Port-forward the ingress service for Knative. With the Kourier networking layer, you can do this as follows. Kourier # do this in a separate terminal kubectl port-forward svc/kourier -n knative-serving 8080 :80 curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv 2. Create candidate version \u00b6 Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate.dynamic.tag = \"2.0\" \\ --set candidate.dynamic.id = \"v2\" \\ --install The above command creates an Iter8 experiment alongside the candidate version of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. Verify that candidate version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready # this command reuses the port-forward from the first step curl localhost:8080 -H \"Host: candidate-hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 3. Observe experiment \u00b6 Describe the results of the Iter8 experiment. Wait ~1 min before trying the following command. If the output is not as expected, try again after a few seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+ 4. Promote winner \u00b6 Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"2.0\" \\ --set baseline.dynamic.id = \"v2\" \\ --set candidate = null \\ --install Verify that baseline version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 5. Cleanup \u00b6 helm uninstall my-app Next Steps Use in production The knslo Helm chart is located in the $ITER8/helm folder. Modify the chart as needed by your application for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split","title":"SLO validation (Helmex)"},{"location":"tutorials/knative/slovalidation-helmex/#slo-validation-helmex","text":"Scenario: Safely rollout new version of a Knative app with SLO validation Dark launch a candidate version of your Knative application, validate that the candidate satisfies latency and error-based objectives (SLOs) , and promote the candidate. This tutorial illustrates the Helmex pattern . Setup K8s cluster with Knative and local environment Get Helm 3.4+ Setup K8s cluster . Install Knative in K8s cluster . This tutorial can be tried with any Knative networking layer. Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo","title":"SLO validation (Helmex)"},{"location":"tutorials/knative/slovalidation-helmex/#1-create-baseline-version","text":"Deploy the baseline version of the hello world application using Helm. helm install my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate = null View K8s resources created by Helm Use the command below to view the K8s resources created by Helm during the release of my-app . helm get manifest my-app Verify that baseline version is 1.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready Port-forward the ingress service for Knative. With the Kourier networking layer, you can do this as follows. Kourier # do this in a separate terminal kubectl port-forward svc/kourier -n knative-serving 8080 :80 curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"1. Create baseline version"},{"location":"tutorials/knative/slovalidation-helmex/#2-create-candidate-version","text":"Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"1.0\" \\ --set baseline.dynamic.id = \"v1\" \\ --set candidate.dynamic.tag = \"2.0\" \\ --set candidate.dynamic.id = \"v2\" \\ --install The above command creates an Iter8 experiment alongside the candidate version of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. Verify that candidate version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready # this command reuses the port-forward from the first step curl localhost:8080 -H \"Host: candidate-hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"2. Create candidate version"},{"location":"tutorials/knative/slovalidation-helmex/#3-observe-experiment","text":"Describe the results of the Iter8 experiment. Wait ~1 min before trying the following command. If the output is not as expected, try again after a few seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: my-experiment Experiment namespace: default Target: my-app Testing pattern: Conformance Deployment pattern: Progressive ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 1 ****** Winner Assessment ****** > If the version being validated ; i.e., the baseline version, satisfies the experiment objectives, it is the winner. > Otherwise, there is no winner. Winning version: my-app ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +--------------------------------------+--------+ | OBJECTIVE | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency < = | true | | 50 .000 | | +--------------------------------------+--------+ | iter8-system/error-rate < = | true | | 0 .000 | | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | true | | < = 100 .000 | | +--------------------------------------+--------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------------+--------+ | METRIC | MY-APP | +--------------------------------------+--------+ | iter8-system/mean-latency | 1 .233 | +--------------------------------------+--------+ | iter8-system/error-rate | 0 .000 | +--------------------------------------+--------+ | iter8-system/latency-95th-percentile | 2 .311 | +--------------------------------------+--------+ | iter8-system/request-count | 40 .000 | +--------------------------------------+--------+ | iter8-system/error-count | 0 .000 | +--------------------------------------+--------+","title":"3. Observe experiment"},{"location":"tutorials/knative/slovalidation-helmex/#4-promote-winner","text":"Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/knslo \\ --set baseline.dynamic.tag = \"2.0\" \\ --set baseline.dynamic.id = \"v2\" \\ --set candidate = null \\ --install Verify that baseline version is 2.0.0 Ensure that the Knative app is ready. kubectl wait ksvc/hello --for = condition = Ready curl localhost:8080 -H \"Host: hello.default.example.com\" # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"4. Promote winner"},{"location":"tutorials/knative/slovalidation-helmex/#5-cleanup","text":"helm uninstall my-app Next Steps Use in production The knslo Helm chart is located in the $ITER8/helm folder. Modify the chart as needed by your application for production usage. Try other Iter8 Knative tutorials SLO validation with progressive traffic shift Hybrid testing Fixed traffic split","title":"5. Cleanup"},{"location":"tutorials/knative/rollout-strategies/fixed-split/","text":"Fixed % Split \u00b6 Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. 1. Create versions and fix traffic split \u00b6 kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40 2. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : Canary deploymentPattern : FixedSplit actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 candidates : - name : sample-app-v2 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"Fixed-%-split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#fixed-split","text":"Scenario: Canary rollout with fixed-%-split Fixed-%-split is a type of canary rollout strategy. It enables you to experiment while sending a fixed percentage of traffic to each version as shown below. Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster.","title":"Fixed % Split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#1-create-versions-and-fix-traffic-split","text":"kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Knative service with traffic split 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 60 - tag : candidate latestRevision : true percent : 40","title":"1. Create versions and fix traffic split"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#2-launch-experiment","text":"kubectl apply -f $ITER8 /samples/knative/fixed-split/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : fixedsplit-exp spec : # target identifies the knative service under experimentation using its fully qualified name target : default/sample-app strategy : testingPattern : Canary deploymentPattern : FixedSplit actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 candidates : - name : sample-app-v2","title":"2. Launch experiment"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/knative/rollout-strategies/fixed-split/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/fixed-split/experiment.yaml kubectl apply -f $ITER8 /samples/knative/fixed-split/experimentalservice.yaml","title":"4. Cleanup"},{"location":"tutorials/knative/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The SLO validation and hybrid testing tutorials demonstrate progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The SLO validation experiment uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The SLO validation and hybrid testing tutorials demonstrate progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/knative/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The SLO validation experiment uses a Knative service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Knative service and the traffic fields within the Knative service corresponding to the versions. versionInfo : baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"Specifying weightObjRef"},{"location":"tutorials/knative/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/knative/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive rollout of Knative services Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives; these metrics will be collected using Iter8's built-in metrics collection feature. Combine hybrid (A/B + SLOs) testing with progressive rollout . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Install Iter8 and Knative in your K8s cluster as described in the platform setup instructions for slo validation quick start experiment . 1. Create app versions \u00b6 Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 2. Define metrics \u00b6 kubectl apply -f $ITER8 /samples/knative/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : https://my-newrelic-service.com mock : - name : sample-app-v1 level : 15.0 - name : sample-app-v2 level : 20.0 Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here . 3. Launch experiment \u00b6 kubectl apply -f $ITER8 /samples/knative/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() - run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" requestCount : iter8-system/request-count duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent 4. Observe experiment \u00b6 Follow these steps to observe your experiment. 5. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/knative/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive rollout of Knative services Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric. Specify latency and error-rate based objectives; these metrics will be collected using Iter8's built-in metrics collection feature. Combine hybrid (A/B + SLOs) testing with progressive rollout . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Install Iter8 and Knative in your K8s cluster as described in the platform setup instructions for slo validation quick start experiment .","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/knative/testing-strategies/hybrid/#1-create-app-versions","text":"Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"1. Create app versions"},{"location":"tutorials/knative/testing-strategies/hybrid/#2-define-metrics","text":"kubectl apply -f $ITER8 /samples/knative/hybrid/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion : v1 kind : Namespace metadata : name : iter8-knative --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-knative spec : params : - name : nrql value : | SELECT average(duration) FROM Sessions WHERE version='$name' SINCE $elapsedTime sec ago description : Average duration of a session type : Gauge headerTemplates : - name : X-Query-Key value : t0p-secret-api-key provider : newrelic jqExpression : \".results[0] | .[] | tonumber\" urlTemplate : https://my-newrelic-service.com mock : - name : sample-app-v1 level : 15.0 - name : sample-app-v2 level : 20.0 Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here .","title":"2. Define metrics"},{"location":"tutorials/knative/testing-strategies/hybrid/#3-launch-experiment","text":"kubectl apply -f $ITER8 /samples/knative/hybrid/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : hybrid-exp spec : target : default/sample-app strategy : testingPattern : A/B deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() - run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : rewards : # Business rewards - metric : iter8-knative/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" requestCount : iter8-system/request-count duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"3. Launch experiment"},{"location":"tutorials/knative/testing-strategies/hybrid/#4-observe-experiment","text":"Follow these steps to observe your experiment.","title":"4. Observe experiment"},{"location":"tutorials/knative/testing-strategies/hybrid/#5-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/hybrid/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"5. Cleanup"},{"location":"tutorials/knative/testing-strategies/slovalidation/","text":"SLO Validation \u00b6 Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Iter8's built-in capabilities for collecting latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster. 1. Create app versions \u00b6 Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0 2. Launch experiment \u00b6 Launch the SLO validation experiment. This experiment will generate requests for your application versions, collect latency and error-rate metrics, and progressively shift traffic and promote the candidate version after verifying that it satisfies SLOs. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"SLO validation"},{"location":"tutorials/knative/testing-strategies/slovalidation/#slo-validation","text":"Scenario: SLO validation with progressive traffic shift This tutorial illustrates an SLO validation experiment with two versions ; the candidate version will be promoted after Iter8 validates that it satisfies service-level objectives (SLOs). You will: Specify latency and error-rate based service-level objectives (SLOs). If the candidate version satisfies SLOs, Iter8 will declare it as the winner. Use Iter8's built-in capabilities for collecting latency and error-rate metrics. Combine SLO validation with progressive traffic shifting . Platform setup Follow these steps to install Iter8 and Knative in your K8s cluster.","title":"SLO Validation"},{"location":"tutorials/knative/testing-strategies/slovalidation/#1-create-app-versions","text":"Deploy two versions of a Knative app. kubectl apply -f $ITER8 /samples/knative/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml kubectl wait --for = condition = Ready ksvc/sample-app Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v1 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue env : - name : T_VERSION value : \"blue\" Look inside experimentalservice.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sample-app namespace : default spec : template : metadata : name : sample-app-v2 spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green env : - name : T_VERSION value : \"green\" traffic : - tag : current revisionName : sample-app-v1 percent : 100 - tag : candidate latestRevision : true percent : 0","title":"1. Create app versions"},{"location":"tutorials/knative/testing-strategies/slovalidation/#2-launch-experiment","text":"Launch the SLO validation experiment. This experiment will generate requests for your application versions, collect latency and error-rate metrics, and progressively shift traffic and promote the candidate version after verifying that it satisfies SLOs. kubectl apply -f $ITER8 /samples/knative/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : default/sample-app strategy : testingPattern : Canary deploymentPattern : Progressive actions : loop : - task : metrics/collect with : versions : - name : sample-app-v1 url : http://sample-app-v1.default.svc.cluster.local - name : sample-app-v2 url : http://sample-app-v2.default.svc.cluster.local finish : # run the following sequence of tasks at the end of the experiment - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/candidate.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/knative/quickstart/baseline.yaml\" criteria : requestCount : iter8-system/request-count objectives : - metric : iter8-system/mean-latency upperLimit : 50 - metric : iter8-system/latency-95th-percentile upperLimit : 100 - metric : iter8-system/error-rate upperLimit : \"0.01\" duration : maxLoops : 3 intervalSeconds : 1 iterationsPerLoop : 1 versionInfo : # information about app versions used in this experiment baseline : name : sample-app-v1 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[0].percent candidates : - name : sample-app-v2 weightObjRef : apiVersion : serving.knative.dev/v1 kind : Service name : sample-app namespace : default fieldPath : .spec.traffic[1].percent","title":"2. Launch experiment"},{"location":"tutorials/knative/testing-strategies/slovalidation/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/knative/testing-strategies/slovalidation/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/knative/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/knative/quickstart/experimentalservice.yaml","title":"4. Cleanup"},{"location":"tutorials/linkerd/abexperiment-helmex/","text":"A/B Experiment \u00b6 Scenario: Safely rollout a Kubernetes deployment with an A/B experiment Launch a candidate version of your application (a K8s service and deployment), compare it against the baseline version, and promote the winner. Setup K8s cluster and local environment Get Helm 3.4+ Setup K8s cluster Get Linkerd Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo 1. Create baseline version \u00b6 Deploy the baseline version of the hello world application using Helm. helm install my-app iter8/linkerd \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate = null Verify that baseline version is 1.0.0 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv 2. Create candidate version \u00b6 Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/linkerd \\ --set baseline.dynamic.tag = 1 .0 \\ --set baseline.weight = 50 \\ --set candidate.dynamic.tag = 2 .0 \\ --set candidate.weight = 50 \\ --install The above command creates an Iter8 experiment alongside the candidate deployment of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 # do this in a separate terminal kubectl port-forward svc/hello-candidate 8081 :8080 curl localhost:8081 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 3. Observe experiment \u00b6 Describe the results of the Iter8 experiment. Wait 20 seconds before trying the following command. If the output is not as expected, try again after a few more seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: hello-experiment-57a46 Experiment namespace: test Target: hello Testing pattern: A/B Deployment pattern: FixedSplit ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 10 ****** Winner Assessment ****** App versions in this experiment: [ hello hello-candidate ] Winning version: hello-candidate Version recommended for promotion: hello-candidate ****** Reward Assessment ****** > Identifies values of reward metrics for each version. The best version is marked with a '*' . +--------------------------------+-------+-----------------+ | REWARD | HELLO | HELLO-CANDIDATE | +--------------------------------+-------+-----------------+ | test/user-engagement ( higher | 5 .204 | 9 .442 * | | better ) | | | +--------------------------------+-------+-----------------+ ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +------------------------------+-------+-----------------+ | OBJECTIVE | HELLO | HELLO-CANDIDATE | +------------------------------+-------+-----------------+ | test/mean-latency < = 300 .000 | true | true | +------------------------------+-------+-----------------+ | test/error-rate < = 0 .010 | true | true | +------------------------------+-------+-----------------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------+--------+-----------------+ | METRIC | HELLO | HELLO-CANDIDATE | +--------------------------------+--------+-----------------+ | test/mean-latency | 1 .000 | 0 .556 | | ( milliseconds ) | | | +--------------------------------+--------+-----------------+ | request-count | 10 .213 | 10 .213 | +--------------------------------+--------+-----------------+ | test/error-rate | 0 .000 | 0 .000 | +--------------------------------+--------+-----------------+ | test/request-count | 10 .213 | 10 .211 | +--------------------------------+--------+-----------------+ | test/user-engagement | 5 .204 | 9 .442 | +--------------------------------+--------+-----------------+ 4. Promote winner \u00b6 Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few more seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/linkerd \\ --install \\ --set baseline.dynamic.tag = 2 .0 \\ --set candidate = null Verify that baseline version is 2.0.0 # kill the port-forward commands from steps 1 and 2 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv 5. Cleanup \u00b6 helm uninstall my-app Next Steps Use in production The Helm chart source for this application is located in $ITER8/helm/linkerd . Modify the chart, including the experiment template, as needed by your application for production usage.","title":"A/B experiment (Helmex)"},{"location":"tutorials/linkerd/abexperiment-helmex/#ab-experiment","text":"Scenario: Safely rollout a Kubernetes deployment with an A/B experiment Launch a candidate version of your application (a K8s service and deployment), compare it against the baseline version, and promote the winner. Setup K8s cluster and local environment Get Helm 3.4+ Setup K8s cluster Get Linkerd Install Iter8 in K8s cluster Get iter8ctl Get the Iter8 Helm repo","title":"A/B Experiment"},{"location":"tutorials/linkerd/abexperiment-helmex/#1-create-baseline-version","text":"Deploy the baseline version of the hello world application using Helm. helm install my-app iter8/linkerd \\ --set baseline.dynamic.tag = 1 .0 \\ --set candidate = null Verify that baseline version is 1.0.0 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 1.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 1.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"1. Create baseline version"},{"location":"tutorials/linkerd/abexperiment-helmex/#2-create-candidate-version","text":"Deploy the candidate version of the hello world application using Helm. helm upgrade my-app iter8/linkerd \\ --set baseline.dynamic.tag = 1 .0 \\ --set baseline.weight = 50 \\ --set candidate.dynamic.tag = 2 .0 \\ --set candidate.weight = 50 \\ --install The above command creates an Iter8 experiment alongside the candidate deployment of the hello world application. The experiment will collect latency and error rate metrics for the candidate, and verify that it satisfies the mean latency (50 msec), error rate (0.0), 95 th percentile tail latency SLO (100 msec) SLOs. View application and experiment resources Use the command below to view your application and Iter8 experiment resources. helm get manifest my-app Verify that candidate version is 2.0.0 # do this in a separate terminal kubectl port-forward svc/hello-candidate 8081 :8080 curl localhost:8081 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"2. Create candidate version"},{"location":"tutorials/linkerd/abexperiment-helmex/#3-observe-experiment","text":"Describe the results of the Iter8 experiment. Wait 20 seconds before trying the following command. If the output is not as expected, try again after a few more seconds. iter8ctl describe Experiment results will look similar to this ... ****** Overview ****** Experiment name: hello-experiment-57a46 Experiment namespace: test Target: hello Testing pattern: A/B Deployment pattern: FixedSplit ****** Progress Summary ****** Experiment stage: Completed Number of completed iterations: 10 ****** Winner Assessment ****** App versions in this experiment: [ hello hello-candidate ] Winning version: hello-candidate Version recommended for promotion: hello-candidate ****** Reward Assessment ****** > Identifies values of reward metrics for each version. The best version is marked with a '*' . +--------------------------------+-------+-----------------+ | REWARD | HELLO | HELLO-CANDIDATE | +--------------------------------+-------+-----------------+ | test/user-engagement ( higher | 5 .204 | 9 .442 * | | better ) | | | +--------------------------------+-------+-----------------+ ****** Objective Assessment ****** > Identifies whether or not the experiment objectives are satisfied by the most recently observed metrics values for each version. +------------------------------+-------+-----------------+ | OBJECTIVE | HELLO | HELLO-CANDIDATE | +------------------------------+-------+-----------------+ | test/mean-latency < = 300 .000 | true | true | +------------------------------+-------+-----------------+ | test/error-rate < = 0 .010 | true | true | +------------------------------+-------+-----------------+ ****** Metrics Assessment ****** > Most recently read values of experiment metrics for each version. +--------------------------------+--------+-----------------+ | METRIC | HELLO | HELLO-CANDIDATE | +--------------------------------+--------+-----------------+ | test/mean-latency | 1 .000 | 0 .556 | | ( milliseconds ) | | | +--------------------------------+--------+-----------------+ | request-count | 10 .213 | 10 .213 | +--------------------------------+--------+-----------------+ | test/error-rate | 0 .000 | 0 .000 | +--------------------------------+--------+-----------------+ | test/request-count | 10 .213 | 10 .211 | +--------------------------------+--------+-----------------+ | test/user-engagement | 5 .204 | 9 .442 | +--------------------------------+--------+-----------------+","title":"3. Observe experiment"},{"location":"tutorials/linkerd/abexperiment-helmex/#4-promote-winner","text":"Assert that the experiment completed and found a winning version. If the conditions are not satisfied, try again after a few more seconds. iter8ctl assert -c completed -c winnerFound Promote the winner as follows. helm upgrade my-app iter8/linkerd \\ --install \\ --set baseline.dynamic.tag = 2 .0 \\ --set candidate = null Verify that baseline version is 2.0.0 # kill the port-forward commands from steps 1 and 2 # do this in a separate terminal kubectl port-forward svc/hello 8080 :8080 curl localhost:8080 # output will be similar to the following (notice 2.0.0 version tag) # hostname will be different in your environment Hello, world! Version: 2.0.0 Hostname: hello-bc95d9b56-xp9kv","title":"4. Promote winner"},{"location":"tutorials/linkerd/abexperiment-helmex/#5-cleanup","text":"helm uninstall my-app Next Steps Use in production The Helm chart source for this application is located in $ITER8/helm/linkerd . Modify the chart, including the experiment template, as needed by your application for production usage.","title":"5. Cleanup"},{"location":"tutorials/linkerd/platform-setup/","text":"Platform Setup for Linkerd \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Linkerd and Iter8 \u00b6 Setup Linkerd, Iter8, and the Linkerd Viz extension. $ITER8 /samples/linkerd/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/linkerd/platform-setup/#platform-setup-for-linkerd","text":"","title":"Platform Setup for Linkerd"},{"location":"tutorials/linkerd/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/linkerd/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/linkerd/platform-setup/#3-install-linkerd-and-iter8","text":"Setup Linkerd, Iter8, and the Linkerd Viz extension. $ITER8 /samples/linkerd/quickstart/platformsetup.sh","title":"3. Install Linkerd and Iter8"},{"location":"tutorials/linkerd/setup-for-tutorials/","text":"Setup For Tutorials \u00b6 Install Linkerd \u00b6 For production installation of Knative, refer to the official Linkerd instructions . For exercising Iter8 tutorials, install Linkerd as follows. Clone Iter8 repo git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) Install Linkerd $ITER8 /samples/linkerd/quickstart/platformsetup.sh Enable Linkerd in the default namespace kubectl annotate namespace default linkerd.io/inject = enabled","title":"Setup For Tutorials"},{"location":"tutorials/linkerd/setup-for-tutorials/#setup-for-tutorials","text":"","title":"Setup For Tutorials"},{"location":"tutorials/linkerd/setup-for-tutorials/#install-linkerd","text":"For production installation of Knative, refer to the official Linkerd instructions . For exercising Iter8 tutorials, install Linkerd as follows. Clone Iter8 repo git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) Install Linkerd $ITER8 /samples/linkerd/quickstart/platformsetup.sh Enable Linkerd in the default namespace kubectl annotate namespace default linkerd.io/inject = enabled","title":"Install Linkerd"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/","text":"A/B Testing \u00b6 Scenario: A/B testing and progressive traffic shift A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Setup K8s cluster Get Linkerd Install Iter8 in K8s cluster Get iter8ctl 1. Create application versions \u00b6 Create a new namespace, enable Linkerd proxy injection, deploy two Hello World applications, and create a traffic split. kubectl create deployment web --image = gcr.io/google-samples/hello-app:1.0 kubectl expose deployment web --type = NodePort --port = 8080 kubectl create deployment web2 --image = gcr.io/google-samples/hello-app:2.0 kubectl expose deployment web2 --type = NodePort --port = 8080 kubectl wait --for = condition = Ready pods --all kubectl apply -f $ITER8 /samples/linkerd/quickstart/traffic-split.yaml Look inside traffic-split.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit metadata : name : web-traffic-split spec : service : web backends : - service : web weight : 100 - service : web2 weight : 0 2. Generate requests \u00b6 Generate requests to your app using Fortio as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/fortio.yaml Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-allow-initial-errors\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : web.test:8080 volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock a number of metrics as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : user-engagement spec : description : Number of error responses type : Gauge mock : - name : web level : 5 - name : web2 level : 10 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le5ms-latency-percentile spec : description : Less than 5 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_bucket{le='5',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(response_latency_ms_bucket{le='+Inf',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus sampleSize : iter8-linkerd/request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_sum{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query 4. Launch experiment \u00b6 Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/linkerd/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : test/web-traffic-split strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n test apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/linkerd/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n test apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/linkerd/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : default/user-engagement preferredDirection : High objectives : # used for validating versions - metric : default/mean-latency upperLimit : 300 - metric : default/error-rate upperLimit : \"0.01\" requestCount : default/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : web variables : - name : namespace # used by final action if this version is the winner value : test weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit name : web-traffic-split fieldPath : .spec.backends[0].weight candidates : - name : web2 variables : - name : namespace # used by final action if this version is the winner value : test weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit name : web-traffic-split fieldPath : .spec.backends[1].weight 3. Observe experiment \u00b6 Follow these steps to observe your experiment. 4. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/linkerd/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/linkerd/quickstart/experiment.yaml","title":"A/B experiment"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#ab-testing","text":"Scenario: A/B testing and progressive traffic shift A/B testing enables you to compare two versions of an ML model, and select a winner based on a (business) reward metric. In this tutorial, you will: Perform A/B testing. Specify user-engagement as the reward metric. This metric will be mocked by Iter8 in this tutorial. Combine A/B testing with progressive traffic shifting . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Setup K8s cluster Get Linkerd Install Iter8 in K8s cluster Get iter8ctl","title":"A/B Testing"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#1-create-application-versions","text":"Create a new namespace, enable Linkerd proxy injection, deploy two Hello World applications, and create a traffic split. kubectl create deployment web --image = gcr.io/google-samples/hello-app:1.0 kubectl expose deployment web --type = NodePort --port = 8080 kubectl create deployment web2 --image = gcr.io/google-samples/hello-app:2.0 kubectl expose deployment web2 --type = NodePort --port = 8080 kubectl wait --for = condition = Ready pods --all kubectl apply -f $ITER8 /samples/linkerd/quickstart/traffic-split.yaml Look inside traffic-split.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit metadata : name : web-traffic-split spec : service : web backends : - service : web weight : 100 - service : web2 weight : 0","title":"1. Create application versions"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#2-generate-requests","text":"Generate requests to your app using Fortio as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/fortio.yaml Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : batch/v1 kind : Job metadata : name : fortio spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ \"fortio\" , \"load\" , \"-allow-initial-errors\" , \"-t\" , \"6000s\" , \"-qps\" , \"16\" , \"-json\" , \"/shared/fortiooutput.json\" , $(URL) ] env : - name : URL value : web.test:8080 volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 600' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. For the purpose of this tutorial, you will mock a number of metrics as follows. kubectl apply -f $ITER8 /samples/linkerd/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : user-engagement spec : description : Number of error responses type : Gauge mock : - name : web level : 5 - name : web2 level : 10 --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-count spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : error-rate spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_total{status_code=~'5..',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus sampleSize : request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : le5ms-latency-percentile spec : description : Less than 5 ms latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_bucket{le='5',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / sum(increase(response_latency_ms_bucket{le='+Inf',deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0) provider : prometheus sampleSize : iter8-linkerd/request-count type : Gauge urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : mean-latency spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(response_latency_ms_sum{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : request-count type : Gauge units : milliseconds urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(request_total{deployment='$name',namespace='$namespace',direction='inbound',tls='true'}[${elapsedTime}s])) provider : prometheus type : Counter urlTemplate : http://prometheus.linkerd-viz:9090/api/v1/query","title":"3. Define metrics"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#4-launch-experiment","text":"Launch the A/B testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/linkerd/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : # target identifies the service under experimentation using its fully qualified name target : test/web-traffic-split strategy : # this experiment will perform an A/B test testingPattern : A/B # this experiment will progressively shift traffic to the winning version deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : kubectl -n test apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/linkerd/quickstart/vs-for-v2.yaml - if : not CandidateWon() run : kubectl -n test apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/linkerd/quickstart/vs-for-v1.yaml criteria : rewards : # (business) reward metric to optimize in this experiment - metric : default/user-engagement preferredDirection : High objectives : # used for validating versions - metric : default/mean-latency upperLimit : 300 - metric : default/error-rate upperLimit : \"0.01\" requestCount : default/request-count duration : # product of fields determines length of the experiment intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about the app versions used in this experiment baseline : name : web variables : - name : namespace # used by final action if this version is the winner value : test weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit name : web-traffic-split fieldPath : .spec.backends[0].weight candidates : - name : web2 variables : - name : namespace # used by final action if this version is the winner value : test weightObjRef : apiVersion : split.smi-spec.io/v1alpha2 kind : TrafficSplit name : web-traffic-split fieldPath : .spec.backends[1].weight","title":"4. Launch experiment"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#3-observe-experiment","text":"Follow these steps to observe your experiment.","title":"3. Observe experiment"},{"location":"tutorials/linkerd/testing-strategies/abexperiment/#4-cleanup","text":"kubectl delete -f $ITER8 /samples/linkerd/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/linkerd/quickstart/experiment.yaml","title":"4. Cleanup"},{"location":"tutorials/openshift/openshift/","text":"CI/CD GitOps on Openshift \u00b6 Openshift Openshift shares many core components with Kubernetes and provides additional features that are catering specifically for enterprise usage. In such an environment, CI/CD pipelines are often automated to allow developers to quickly deliver new code to the production environment with certain level of assurance. In this tutorial, we assume the reader is already using Openshift, managing CI/CD pipelines with Openshift Pipeline (Tekton) and Openshift GitOps (ArgoCD), and has a GitOps setup to allow newly built images to be deployed into the cluster. We will describe how such users could integrate Iter8 into their existing pipeline in a minimally intrusive way to allow their applications to be progressively rolled out. This tutorial assumes a basic understanding of Iter8. See, for example, the Istio quick start tutorial . 1. Install Iter8 \u00b6 Installing Iter8 on Openshift is slightly different from installing it on K8s . After Iter8 is installed, an extra step one needs to perform is: oc adm policy add-scc-to-group anyuid system:serviceaccounts:istio-system 2. Openshift Metrics \u00b6 Openshift comes with a built-in Prometheus server, which is authenticated with a sidecar proxy. To allow Iter8 to retrieve metrics from it, one needs to provide a mean to authenticate. Iter8 currently supports basic authentication and bearer token, described in Iter8 metrics . An example request count metric looks like: apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : default spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(haproxy_server_http_responses_total{exported_service='$name',exported_namespace='$namespace'}[${elapsedTime}s])) or vector(0) provider : prometheus type : Counter urlTemplate : https://prometheus-operated.openshift-monitoring:9091/api/v1/query authType : Bearer secret : default/promsecret headerTemplates : - name : Authorization value : Bearer ${token} 3. Augment the CI Pipeline with Iter8 \u00b6 The CI pipeline gets triggered when new code is merged, and it is responsible for testing and building the new code. The end product is usually a newly build image pushed to an image repository. When one wants to leverage Iter8, one should create a Candidate Deployment (using the new image) and an Iter8 Experiment CR. Subsequently, Iter8 will observe metrics collected from the Candidate version and check if all success criteria have passed. If so, the candidate version will be promoted. Here is an example Tekton task that creates these resources in Git. apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : start-experiment annotations : tekton.dev/displayName : \"Start an Iter8 experiment\" spec : description : >- This task create a candidate and Iter8 experiment resource in the Env repo and make a PR from the changes params : - name : USER description : Github username - name : REPO description : Github repo name - name : BRANCH description : Base branch PR is opened against - name : GITHUB-TOKEN-SECRET description : Holds Github token steps : - name : start-experiment image : alpine/git:latest script : | #!/usr/bin/env sh apk add curl jq make git config --global user.email 'iter8@iter8.tools' git config --global user.name 'Iter8' git clone https://$(params.USER):${GITHUB_TOKEN}@github.com/$(params.USER)/$(params.REPO) --branch=$(params.BRANCH) [create a Deployment Candidate yaml] [create an Iter8 Experiment yaml] git add -A git commit -a -m 'start Iter8 experiment' git push -f origin $(params.BRANCH) env : - name : GITHUB_TOKEN valueFrom : secretKeyRef : name : $(params.GITHUB-TOKEN-SECRET) key : token In a non-GitOps environment, the git commands will be replaced with oc commands to directly deploy into the cluster. An example Tekton Pipeline where the above Task can be added to is: - name : start-experiment taskRef : name : start-experiment params : - name : USER value : [ YOUR GIT USERNAME ] - name : REPO value : [ YOUR GIT REPO ] - name : BRANCH value : master - name : GITHUB-TOKEN-SECRET value : github-token","title":"CI/CD integration"},{"location":"tutorials/openshift/openshift/#cicd-gitops-on-openshift","text":"Openshift Openshift shares many core components with Kubernetes and provides additional features that are catering specifically for enterprise usage. In such an environment, CI/CD pipelines are often automated to allow developers to quickly deliver new code to the production environment with certain level of assurance. In this tutorial, we assume the reader is already using Openshift, managing CI/CD pipelines with Openshift Pipeline (Tekton) and Openshift GitOps (ArgoCD), and has a GitOps setup to allow newly built images to be deployed into the cluster. We will describe how such users could integrate Iter8 into their existing pipeline in a minimally intrusive way to allow their applications to be progressively rolled out. This tutorial assumes a basic understanding of Iter8. See, for example, the Istio quick start tutorial .","title":"CI/CD GitOps on Openshift"},{"location":"tutorials/openshift/openshift/#1-install-iter8","text":"Installing Iter8 on Openshift is slightly different from installing it on K8s . After Iter8 is installed, an extra step one needs to perform is: oc adm policy add-scc-to-group anyuid system:serviceaccounts:istio-system","title":"1. Install Iter8"},{"location":"tutorials/openshift/openshift/#2-openshift-metrics","text":"Openshift comes with a built-in Prometheus server, which is authenticated with a sidecar proxy. To allow Iter8 to retrieve metrics from it, one needs to provide a mean to authenticate. Iter8 currently supports basic authentication and bearer token, described in Iter8 metrics . An example request count metric looks like: apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : labels : creator : iter8 name : request-count namespace : default spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(haproxy_server_http_responses_total{exported_service='$name',exported_namespace='$namespace'}[${elapsedTime}s])) or vector(0) provider : prometheus type : Counter urlTemplate : https://prometheus-operated.openshift-monitoring:9091/api/v1/query authType : Bearer secret : default/promsecret headerTemplates : - name : Authorization value : Bearer ${token}","title":"2. Openshift Metrics"},{"location":"tutorials/openshift/openshift/#3-augment-the-ci-pipeline-with-iter8","text":"The CI pipeline gets triggered when new code is merged, and it is responsible for testing and building the new code. The end product is usually a newly build image pushed to an image repository. When one wants to leverage Iter8, one should create a Candidate Deployment (using the new image) and an Iter8 Experiment CR. Subsequently, Iter8 will observe metrics collected from the Candidate version and check if all success criteria have passed. If so, the candidate version will be promoted. Here is an example Tekton task that creates these resources in Git. apiVersion : tekton.dev/v1beta1 kind : Task metadata : name : start-experiment annotations : tekton.dev/displayName : \"Start an Iter8 experiment\" spec : description : >- This task create a candidate and Iter8 experiment resource in the Env repo and make a PR from the changes params : - name : USER description : Github username - name : REPO description : Github repo name - name : BRANCH description : Base branch PR is opened against - name : GITHUB-TOKEN-SECRET description : Holds Github token steps : - name : start-experiment image : alpine/git:latest script : | #!/usr/bin/env sh apk add curl jq make git config --global user.email 'iter8@iter8.tools' git config --global user.name 'Iter8' git clone https://$(params.USER):${GITHUB_TOKEN}@github.com/$(params.USER)/$(params.REPO) --branch=$(params.BRANCH) [create a Deployment Candidate yaml] [create an Iter8 Experiment yaml] git add -A git commit -a -m 'start Iter8 experiment' git push -f origin $(params.BRANCH) env : - name : GITHUB_TOKEN valueFrom : secretKeyRef : name : $(params.GITHUB-TOKEN-SECRET) key : token In a non-GitOps environment, the git commands will be replaced with oc commands to directly deploy into the cluster. An example Tekton Pipeline where the above Task can be added to is: - name : start-experiment taskRef : name : start-experiment params : - name : USER value : [ YOUR GIT USERNAME ] - name : REPO value : [ YOUR GIT REPO ] - name : BRANCH value : master - name : GITHUB-TOKEN-SECRET value : github-token","title":"3. Augment the CI Pipeline with Iter8"},{"location":"tutorials/seldon/platform-setup/","text":"Platform Setup for Seldon \u00b6 1. Create Kubernetes cluster \u00b6 Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288 2. Clone Iter8 repo \u00b6 git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd ) 3. Install Seldon and Iter8 \u00b6 Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh","title":"Platform setup"},{"location":"tutorials/seldon/platform-setup/#platform-setup-for-seldon","text":"","title":"Platform Setup for Seldon"},{"location":"tutorials/seldon/platform-setup/#1-create-kubernetes-cluster","text":"Create a local cluster using Kind or Minikube as follows, or use a managed Kubernetes cluster. Ensure that the cluster has sufficient resources, for example, 8 CPUs and 12GB of memory. Kind kind create cluster --wait 5m kubectl cluster-info --context kind-kind Ensuring your Kind cluster has sufficient resources Your Kind cluster inherits the CPU and memory resources of its host. If you are using Docker Desktop, you can set its resources as shown below. Minikube minikube start --cpus 8 --memory 12288","title":"1. Create Kubernetes cluster"},{"location":"tutorials/seldon/platform-setup/#2-clone-iter8-repo","text":"git clone https://github.com/iter8-tools/iter8.git cd iter8 export ITER8 = $( pwd )","title":"2. Clone Iter8 repo"},{"location":"tutorials/seldon/platform-setup/#3-install-seldon-and-iter8","text":"Setup Seldon Core, Seldon Analytics and Iter8 within your cluster. $ITER8 /samples/seldon/quickstart/platformsetup.sh","title":"3. Install Seldon and Iter8"},{"location":"tutorials/seldon/quick-start/","text":"Hybrid (A/B + SLOs) testing \u00b6 Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of Seldon models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric; data for this metric will be provided by Prometheus. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Seldon and Iter8 in your K8s cluster. 1. Create ML model versions \u00b6 Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0 2. Generate requests \u00b6 Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":(6.8, 2.8, 4.8, 1.4)}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never 3. Define metrics \u00b6 Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here . 4. Launch experiment \u00b6 Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml\" criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris 5. Observe experiment \u00b6 Follow these steps to observe your experiment. 6. Cleanup \u00b6 kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"Quick start"},{"location":"tutorials/seldon/quick-start/#hybrid-ab-slos-testing","text":"Scenario: Hybrid (A/B + SLOs) testing and progressive traffic shift of Seldon models Hybrid (A/B + SLOs) testing enables you to combine A/B or A/B/n testing with a reward metric on the one hand with SLO validation using objectives on the other. Among the versions that satisfy objectives, the version which performs best in terms of the reward metric is the winner. In this tutorial, you will: Perform hybrid (A/B + SLOs) testing. Specify user-engagement as the reward metric; data for this metric will be provided by Prometheus. Specify latency and error-rate based objectives; data for these metrics will be provided by Prometheus. Combine hybrid (A/B + SLOs) testing with progressive traffic shift . Iter8 will progressively shift traffic towards the winner and promote it at the end as depicted below. Platform setup Follow these steps to install Seldon and Iter8 in your K8s cluster.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/seldon/quick-start/#1-create-ml-model-versions","text":"Deploy two Seldon Deployments corresponding to two versions of an Iris classification model, along with an Istio virtual service to split traffic between them. kubectl apply -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/candidate.yaml kubectl apply -f $ITER8 /samples/seldon/quickstart/routing-rule.yaml kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-baseline kubectl wait --for = condition = Ready --timeout = 600s pods --all -n ns-candidate Look inside baseline.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-baseline --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-baseline spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/sklearn/iris implementation : SKLEARN_SERVER Look inside candidate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : Namespace metadata : name : ns-candidate --- apiVersion : machinelearning.seldon.io/v1 kind : SeldonDeployment metadata : name : iris namespace : ns-candidate spec : predictors : - name : default graph : name : classifier modelUri : gs://seldon-models/xgboost/iris implementation : XGBOOST_SERVER Look inside routing-rule.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : routing-rule namespace : default spec : gateways : - istio-system/seldon-gateway hosts : - iris.example.com http : - route : - destination : host : iris-default.ns-baseline.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v1 weight : 100 - destination : host : iris-default.ns-candidate.svc.cluster.local port : number : 8000 headers : response : set : version : iris-v2 weight : 0","title":"1. Create ML model versions"},{"location":"tutorials/seldon/quick-start/#2-generate-requests","text":"Generate requests using Fortio as follows. URL_VALUE = \"http:// $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.clusterIP}' ) :80\" sed \"s+URL_VALUE+ ${ URL_VALUE } +g\" $ITER8 /samples/seldon/quickstart/fortio.yaml | sed \"s/6000s/600s/g\" | kubectl apply -f - Look inside fortio.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 apiVersion : batch/v1 kind : Job metadata : name : fortio-requests spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"5\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Host: iris.example.com' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"data\": {\"ndarray\":(6.8, 2.8, 4.8, 1.4)}}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/api/v1.0/predictions volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv1-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"0.7\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-baseline/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never --- apiVersion : batch/v1 kind : Job metadata : name : fortio-irisv2-rewards spec : template : spec : volumes : - name : shared emptyDir : {} containers : - name : fortio image : fortio/fortio command : [ 'fortio' , 'load' , '-t' , '6000s' , '-qps' , \"1\" , '-json' , '/shared/fortiooutput.json' , '-H' , 'Content-Type: application/json' , '-payload' , '{\"reward\": 1}' , \"$(URL)\" ] env : - name : URL value : URL_VALUE/seldon/ns-candidate/iris/api/v1.0/feedback volumeMounts : - name : shared mountPath : /shared - name : busybox image : busybox:1.28 command : [ 'sh' , '-c' , 'echo busybox is running! && sleep 6000' ] volumeMounts : - name : shared mountPath : /shared restartPolicy : Never","title":"2. Generate requests"},{"location":"tutorials/seldon/quick-start/#3-define-metrics","text":"Iter8 defines a custom K8s resource called Metric that makes it easy to use metrics from RESTful metric providers like Prometheus, New Relic, Sysdig and Elastic during experiments. Define the Iter8 metrics used in this experiment as follows. kubectl apply -f $ITER8 /samples/seldon/quickstart/metrics.yaml Look inside metrics.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 apiVersion : v1 kind : Namespace metadata : name : iter8-seldon --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : 95th-percentile-tail-latency namespace : iter8-seldon spec : description : 95th percentile tail latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | histogram_quantile(0.95, sum(rate(seldon_api_executor_client_requests_seconds_bucket{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) by (le)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-count namespace : iter8-seldon spec : description : Number of error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : error-rate namespace : iter8-seldon spec : description : Fraction of requests with error responses jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_server_requests_seconds_count{code!='200',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_server_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : mean-latency namespace : iter8-seldon spec : description : Mean latency jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | (sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) / (sum(increase(seldon_api_executor_client_requests_seconds_count{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0)) provider : prometheus sampleSize : iter8-seldon/request-count type : Gauge units : milliseconds urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : request-count namespace : iter8-seldon spec : description : Number of requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_client_requests_seconds_sum{seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Counter urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query --- apiVersion : iter8.tools/v2alpha2 kind : Metric metadata : name : user-engagement namespace : iter8-seldon spec : description : Number of feedback requests jqExpression : .data.result[0].value[1] | tonumber params : - name : query value : | sum(increase(seldon_api_executor_server_requests_seconds_count{service='feedback',seldon_deployment_id='$sid',kubernetes_namespace='$ns'}[${elapsedTime}s])) or on() vector(0) provider : prometheus type : Gauge urlTemplate : http://seldon-core-analytics-prometheus-seldon.seldon-system/api/v1/query Metrics in your environment You can define and use custom metrics from any database in Iter8 experiments. For your application, replace the mocked user-engagement metric used in this tutorial with any custom metric you wish to optimize in the hybrid (A/B + SLOs) test. Documentation on defining custom metrics is here .","title":"3. Define metrics"},{"location":"tutorials/seldon/quick-start/#4-launch-experiment","text":"Launch the hybrid (A/B + SLOs) testing & progressive traffic shift experiment as follows. This experiment also promotes the winning version of the model at the end. kubectl apply -f $ITER8 /samples/seldon/quickstart/experiment.yaml Look inside experiment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion : iter8.tools/v2alpha2 kind : Experiment metadata : name : quickstart-exp spec : target : iris strategy : testingPattern : A/B deploymentPattern : Progressive actions : # when the experiment completes, promote the winning version using kubectl apply finish : - if : CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v2.yaml\" - if : not CandidateWon() run : \"kubectl apply -f https://raw.githubusercontent.com/iter8-tools/iter8/master/samples/seldon/quickstart/promote-v1.yaml\" criteria : requestCount : iter8-seldon/request-count rewards : # Business rewards - metric : iter8-seldon/user-engagement preferredDirection : High # maximize user engagement objectives : - metric : iter8-seldon/mean-latency upperLimit : 2000 - metric : iter8-seldon/95th-percentile-tail-latency upperLimit : 5000 - metric : iter8-seldon/error-rate upperLimit : \"0.01\" duration : intervalSeconds : 10 iterationsPerLoop : 5 versionInfo : # information about model versions used in this experiment baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight variables : - name : ns value : ns-baseline - name : sid value : iris candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight variables : - name : ns value : ns-candidate - name : sid value : iris","title":"4. Launch experiment"},{"location":"tutorials/seldon/quick-start/#5-observe-experiment","text":"Follow these steps to observe your experiment.","title":"5. Observe experiment"},{"location":"tutorials/seldon/quick-start/#6-cleanup","text":"kubectl delete -f $ITER8 /samples/seldon/quickstart/fortio.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/experiment.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/baseline.yaml kubectl delete -f $ITER8 /samples/seldon/quickstart/candidate.yaml","title":"6. Cleanup"},{"location":"tutorials/seldon/rollout-strategies/progressive/","text":"Progressive Traffic Shift \u00b6 Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below. Tutorials with progressive traffic shift \u00b6 The hybrid (A/B + SLOs) testing tutorial demonstrates progressive traffic shift. Specifying weightObjRef \u00b6 Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight Traffic controls \u00b6 You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Progressive traffic shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#progressive-traffic-shift","text":"Scenario: Progressive traffic shift Progressive traffic shift is a type of canary rollout strategy. It enables you to incrementally shift traffic towards the winning version over multiple iterations of an experiment as shown below.","title":"Progressive Traffic Shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#tutorials-with-progressive-traffic-shift","text":"The hybrid (A/B + SLOs) testing tutorial demonstrates progressive traffic shift.","title":"Tutorials with progressive traffic shift"},{"location":"tutorials/seldon/rollout-strategies/progressive/#specifying-weightobjref","text":"Iter8 uses the weightObjRef field in the experiment resource to get the current traffic split between versions and/or modify the traffic split. Ensure that this field is specified correctly for each version. The following example demonstrates how to specify weightObjRef in experiments. Example The hybrid (A/B + SLOs) testing tutorial uses an Istio virtual service for traffic shifting. Hence, the experiment manifest specifies the weightObjRef field for each version by referencing this Istio virtual service and the traffic fields within the Istio virtual service corresponding to the versions. versionInfo : baseline : name : iris-v1 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[0].weight candidates : - name : iris-v2 weightObjRef : apiVersion : networking.istio.io/v1alpha3 kind : VirtualService name : routing-rule namespace : default fieldPath : .spec.http[0].route[1].weight","title":"Specifying weightObjRef"},{"location":"tutorials/seldon/rollout-strategies/progressive/#traffic-controls","text":"You can specify the maximum traffic percentage that is allowed for a candidate version during the experiment. You can also specify the maximum increase in traffic percentage that is allowed for a candidate version during a single iteration of the experiment. You can specify these two controls in the strategy section of an experiment as follows. strategy : weights : # additional traffic controls to be used during an experiment # candidate weight will not exceed 75 in any iteration maxCandidateWeight : 75 # candidate weight will not increase by more than 20 in a single iteration maxCandidateWeightIncrement : 20","title":"Traffic controls"},{"location":"tutorials/seldon/testing-strategies/hybrid/","text":"Hybrid (A/B + SLOs) testing \u00b6 The quick start tutorial for Seldon demonstrates hybrid (A/B + SLOs) testing.","title":"Hybrid (A/B + SLOs) testing"},{"location":"tutorials/seldon/testing-strategies/hybrid/#hybrid-ab-slos-testing","text":"The quick start tutorial for Seldon demonstrates hybrid (A/B + SLOs) testing.","title":"Hybrid (A/B + SLOs) testing"}]}